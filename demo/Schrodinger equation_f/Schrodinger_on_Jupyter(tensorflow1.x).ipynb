{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries, set the gpu and random seed\n",
    "\n",
    "#下面这行代码，是为了把自己编写的代码文件当作一共模块导入，这里是把Utilities文件夹中的plotting.py文件当作python的模块导入，对应的是下面的from plotting import newfig, savefig。路径要随着不同设备的系统做相应的修改\n",
    "import sys #导入sys模块。sys模块提供了一些变量和函数，用于与 Python解释器进行交互和访问。例如，sys.path 是一个 Python 在导入模块时会查找的路径列表，sys.argv 是一个包含命令行参数的列表，sys.exit() 函数可以用于退出 Python 程序。导入 sys 模块后，你就可以在你的程序中使用这些变量和函数了。\n",
    "sys.path.insert(0, 'C:/Users/cheny/Documents/GitHub/PINNs/Utilities/') #在 Python的sys.path列表中插入一个新的路径。sys.path是一个 Python 在导入模块时会查找的路径列表。新的路径'../../Utilities/'相对于当前脚本的路径。当你尝试导入一个模块时，Python 会在 sys.path 列表中的路径下查找这个模块。通过在列表开始位置插入一个路径，你可以让 Python 优先在这个路径下查找模块。这在你需要导入自定义模块或者不在 Python 标准库中的模块时非常有用。\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#下面的`scipy`是一个用于科学计算和技术计算的Python库，提供了许多高级的数学函数和便利的操作，包括数值积分、插值、优化、图像处理、统计等。\n",
    "import scipy.io #导入了scipy库中的io模块。scipy.io模块包含了一些用于文件输入/输出的函数，例如读取和写入.mat文件（MATLAB格式）。\n",
    "from scipy.interpolate import griddata#`scipy.interpolate`是`scipy`库中的一个模块，提供了许多插值工具，用于在给定的离散数据点之间进行插值和拟合。`griddata`是这个模块中的一个函数，用于在无规则的数据点上进行插值。它使用方法如下：\n",
    "#griddata(points, values, xi, method='linear', fill_value=nan, rescale=False)；\n",
    "   # `points`： ndarray of floats, shape (n, D)。表示数据点的坐标。`values`： ndarray of float or complex, shape (n,)。表示数据点的值。`xi`： ndarray of float, shape (M, D)。表示插值点的坐标。`method`： 插值方法，可选'linear'、'nearest'、'cubic'。默认为'linear'。\n",
    "   #`fill_value`： 在插值范围外的点的值。默认为nan。`rescale`： 是否对坐标点进行重标定，以提高数值稳定性。默认为False。\n",
    "   #返回值：ndarray，shape (M,) or (M, 1)。插值点的值。这个函数可以用于从散列的数据点创建一个连续的函数，这对于处理实际数据非常有用，因为实际数据通常是不规则或者不完整的。\n",
    "from pyDOE import lhs #`pyDOE`是一个Python库，用于设计实验。它提供了一些函数来生成各种设计，如因子设计、拉丁超立方设计等。`lhs`是库中的一个函数，全名为\"Latin Hypercube Sampling\"，拉丁超立方采样。这是一种统计方法，用于生成一个近似均匀分布的多维样本点集。它在参数空间中生成一个非常均匀的样本，这对于高维数值优化问题非常有用，因为它可以更好地覆盖参数空间。\n",
    "#`lhs`函数的基本用法如下：lhs(n, samples=1000):其中，`n`是参数的数量，`samples`是想生成的样本点的数量。这个函数会返回一个形状为(samples, n)的数组，每一行都是一个n维的样本点，所有的样本点都在[0, 1]范围内。\n",
    "from plotting import newfig, savefig #从自定义的plotting.py文件中导入了newfig和savefig函数。这两个函数用于创建和保存图形。这两个函数的定义在plotting.py文件中\n",
    "from mpl_toolkits.mplot3d import Axes3D #`mpl_toolkits.mplot3d`是`matplotlib`库的一个模块，用于创建三维图形。`Axes3D`是`mpl_toolkits.mplot3d`模块中的一个类，用于创建一个三维的坐标轴。可以在这个坐标轴上绘制三维的图形，如曲线、曲面等。\n",
    "import time #一个内置模块，用于处理时间相关的操作。\n",
    "import matplotlib.gridspec as gridspec #是`matplotlib`库的一个模块，用于创建一个网格布局来放置子图。在`matplotlib`中可以创建一个或多个子图（subplot），每个子图都有自己的坐标轴，并可以在其中绘制图形。`gridspec`模块提供了一个灵活的方式来创建和放置子图。\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #`mpl_toolkits.axes_grid1`是`matplotlib`库的一个模块，提供了一些高级的工具来控制matplotlib图形中的坐标轴和颜色条。`make_axes_locatable`是模块中的一个函数，用于创建一个可分割的坐标轴。可以在这个坐标轴的四个方向（上、下、左、右）添加新的坐标轴或颜色条。\n",
    "\n",
    "#下面两行是指定索引为哪一块的gpu进行训练，这里使用索引为1的，即第二块gpu\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1，0\"\n",
    "\n",
    "\n",
    "#NumPy和TensorFlow都有自己的随机数生成器，它们是独立的，互不影响。也就是说，设置NumPy的随机数种子不会影响TensorFlow的随机数生成，反之亦然\n",
    "np.random.seed(1234) #设置了NumPy的随机数生成器的种子。设置随机数生成器的种子可以确保每次运行程序时，NumPy生成的随机数序列都是一样的。\n",
    "tf.set_random_seed(1234) #设置了TensorFlow的随机数生成器的种子。设置随机数生成器的种子可以确保每次运行程序时，TensorFlow生成的随机数序列都是一样的\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the class of PINN\n",
    "\n",
    "#定义了一个名为`PhysicsInformedNN'的类，用于实现基于物理的神经网络。\n",
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub): #这个类包含的第一个方法__init__，这是一个特殊的方法，也就是这个类的构造函数，用于初始化新创建的对象，接受了几个参数\n",
    "        \n",
    "        \n",
    "        #`numpy.concatenate`是一个用于数组拼接的函数。它可以将多个数组沿指定的轴拼接在一起，形成一个新的数组：numpy.concatenate((a1,a2, ...), axis=0)其中，`a1,a2, ...`是需要拼接的数组（只能接受数组或序列类型的参数，且参数形状必须相同），可以是多个。`axis`参数用于指定拼接的轴向，`axis=0`表示沿着第一个轴（即行）进行拼接，不指定`axis`参数默认值是0。\n",
    "        X0 = np.concatenate((x0,0*x0), 1) # [x0, 0],将x0和0*x0两个数组在第二个维度（即列）上进行了合并。0*x0会生成一个与x0形状相同，但所有元素都为0的数组。因此，X0的结果是一个新的二维数组，其中第一列是x0的值，第二列全为0\n",
    "        X_lb = np.concatenate((0*tb+lb[0],tb), 1) # [lb[0], tb],将0*tb+lb[0]和tb两个数组在第二个维度（即列）上进行了合并。0*tb+lb[0]会生成一个与tb形状相同，但所有元素都为lb[0]的数组。因此，X_lb的结果是一个新的二维数组，其中第一列全为lb[0]的值，第二列是tb的值。\n",
    "        X_ub = np.concatenate((0*tb+ub[0],tb), 1) # [ub[0], tb],同上生成一个与tb形状相同，但所有元素都为ub[0]的数组。因此，X_ub的结果是一个新的二维数组，其中第一列全为ub[0]的值，第二列是tb的值\n",
    "        \n",
    "        #Python使用self关键字来表示类的实例。当在类的方法中定义一个变量时，例如lb和ub，这些变量只在该方法内部可见，也就是说它们的作用域仅限于该方法。当方法执行完毕后，这些变量就会被销毁，无法在其他方法中访问它们。但如果希望在类的其他方法中也能访问这些变量就需要将它们保存为类的实例属性。这就是self.lb和self.ub的作用。\n",
    "            #通过将lb和ub赋值给self.lb和self.ub，就可以在类的其他方法中通过self.lb和self.ub来访问这些值。总的来说，self.lb和self.ub是类的实例属性，它们的作用域是整个类，而不仅仅是定义它们的方法。\n",
    "        self.lb = lb #将传入的lb和ub参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.lb和self.ub来访问这些值。\n",
    "        self.ub = ub\n",
    "               \n",
    "        self.x0 = X0[:,0:1] #将X0的第一列赋值给self.x0（:表示取所有行,0：1实际上表示取第一列，因为python是左闭右开的）,将X0的第二列赋值给self.t0。这样可以在类的其他方法中通过self.x0和self.t0来访问这些值。\n",
    "        self.t0 = X0[:,1:2] #将x0的第二列赋值给self.t0\n",
    "\n",
    "        self.x_lb = X_lb[:,0:1] #将X_lb的第一列赋值给self.x_lb\n",
    "        self.t_lb = X_lb[:,1:2] #将X_lb的第二列赋值给self.t_lb\n",
    "\n",
    "        self.x_ub = X_ub[:,0:1] #将X_ub的第一列赋值给self.x_ub\n",
    "        self.t_ub = X_ub[:,1:2] #将X_ub的第二列赋值给self.t_ub\n",
    "        \n",
    "        self.x_f = X_f[:,0:1] #将X_f的第一列赋值给self.x_f\n",
    "        self.t_f = X_f[:,1:2] #将X_f的第二列赋值给self.t_f\n",
    "        \n",
    "        self.u0 = u0 #将传入的u0和v0参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.u0和self.v0来访问这些值。\n",
    "        self.v0 = v0\n",
    "        \n",
    "        # Initialize NNs \n",
    "        self.layers = layers #将传入的layers参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.layers来访问这些值。\n",
    "        self.weights, self.biases = self.initialize_NN(layers) #调用了initialize_NN方法，用于初始化神经网络的权重和偏置。这个方法接受一个参数layers，它是一个列表，包含了神经网络的层数和每一层的神经元数量。例如，layers=[2, 100, 100, 100, 100, 2]表示神经网络有5个隐藏层，每个隐藏层有100个神经元，输入层和输出层分别有2个神经元。这个方法返回了神经网络的权重和偏置（具体见下面），分别存储在self.weights和self.biases中。\n",
    "        \n",
    "        #使用TensorFlow库创建占位符，用于存储输入和输出数据。占位符是TensorFlow中的一个特殊对象，允许在运行时将数据传递给TensorFlow计算图。可以将占位符看作是一个变量，但不需要提供初始值。相反只需要在运行计算图时提供一个值。使用feed_dict参数来为占位符提供值。\n",
    "            #例如，如果x是一个占位符，可以使用feed_dict={x: 1.0}来为它提供值。这个值可以是一个单独的数字，也可以是一个数组。        \n",
    "        #每个占位符都使用了tf.placeholder函数进行创建。该函数接受两个参数,第一个是数据类型,这里都是tf.float32,表示占位符的数据类型是浮点数。第二个参数是形状,这里都是[None,self.x0.shape[1]]这样的形式，其中None表示这一维的长度可以是任意的，self.x0.shape[1]表示这一维的长度是self.x0的列数。\n",
    "        self.x0_tf = tf.placeholder(tf.float32, shape=[None, self.x0.shape[1]])\n",
    "        self.t0_tf = tf.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
    "        \n",
    "        self.u0_tf = tf.placeholder(tf.float32, shape=[None, self.u0.shape[1]])\n",
    "        self.v0_tf = tf.placeholder(tf.float32, shape=[None, self.v0.shape[1]])\n",
    "        \n",
    "        self.x_lb_tf = tf.placeholder(tf.float32, shape=[None, self.x_lb.shape[1]])\n",
    "        self.t_lb_tf = tf.placeholder(tf.float32, shape=[None, self.t_lb.shape[1]])\n",
    "        \n",
    "        self.x_ub_tf = tf.placeholder(tf.float32, shape=[None, self.x_ub.shape[1]])\n",
    "        self.t_ub_tf = tf.placeholder(tf.float32, shape=[None, self.t_ub.shape[1]])\n",
    "        \n",
    "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
    "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
    "\n",
    "\n",
    "\n",
    "        # tf Graphs，这里是使用TensorFlow库进行神经网络前向传播的部分。\n",
    "        self.u0_pred, self.v0_pred, _ , _ = self.net_uv(self.x0_tf, self.t0_tf) #是调用net_uv函数,将self.x0_tf和self.t0_tf作为参数传入,然后将返回的前两个结果赋值给self.u0_pred和self.v0_pred。后两个_是Python惯用法，表示不关心net_uv函数返回的后两个结果。\n",
    "        self.u_lb_pred, self.v_lb_pred, self.u_x_lb_pred, self.v_x_lb_pred = self.net_uv(self.x_lb_tf, self.t_lb_tf) #同上，不过这里函数返回的后两个结果会赋值给self.u_x_lb_pred和self.v_x_lb_pred。\n",
    "        self.u_ub_pred, self.v_ub_pred, self.u_x_ub_pred, self.v_x_ub_pred = self.net_uv(self.x_ub_tf, self.t_ub_tf) #同上\n",
    "        self.f_u_pred, self.f_v_pred = self.net_f_uv(self.x_f_tf, self.t_f_tf) #调用net_f_uv函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_u_pred和self.f_v_pred。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Loss，这里是使用TensorFlow库计算损失函数的部分，训练目标是最小化损失函数，这里的损失函数由八部分组成，分别是初始条件、边界条件、微分方程两边的残差。每一部分都是预测值与真实值之间的差的平方的均值（均方误差）\n",
    "        #tf.reduce_mean是TensorFlow库中的一个函数，用于计算张量的均值。它接受一个参数，即张量，可以是一个一维数组，也可以是一个多维数组。它会返回一个标量，即这个张量的均值。\n",
    "        #tf.square是TensorFlow库中的一个函数，用于计算张量的平方。它接受一个参数，即张量，可以是一个一维数组，也可以是一个多维数组。它会返回一个与输入张量形状相同的张量，其中每个元素都是输入张量对应元素的平方。\n",
    "        #这里的+ \\表示将两行代码连接成一行，这是Python中的行连接符，用于将一行代码分成多行书写。\n",
    "        self.loss = tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.v0_tf - self.v0_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.u_lb_pred - self.u_ub_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.v_lb_pred - self.v_ub_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.u_x_lb_pred - self.u_x_ub_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.v_x_lb_pred - self.v_x_ub_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_u_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_v_pred))\n",
    "\n",
    "\n",
    "\n",
    "        # Optimizers，这里是使用TensorFlow库进行优化的部分，使用了两种优化器，分别是L-BFGS-B和Adam。L-BFGS-B是一种基于梯度的优化方法，它使用了拟牛顿法来寻找损失函数的最小值。Adam是一种基于梯度的优化方法，它使用了自适应学习率来寻找损失函数的最小值\n",
    "\n",
    "        #首先用tf.contrib.opt.ScipyOptimizerInterface函数创建了一个优化器self.optimizer，它使用了L-BFGS-B方法，最大迭代次数为50000次，最大函数调用次数为50000次，最大相关矩阵大小为50，最大线搜索次数为50，终止条件为1.0 * np.finfo(float).eps。\n",
    "              #tf.contrib.opt.ScipyOptimizerInterface是TensorFlow中的函数，提供了一个接口，可以使用优化算法来最小化TensorFlow的损失函数，接受三个参数，第一个参数是损失函数（这里是self.loss）,第二个参数是优化方法，这里是L-BFGS-B，第三个参数options是一个字典，用于指定优化器的参数。\n",
    "              #这里maxiter表示最大迭代次数，maxfun表示最大函数调用次数，maxcor表示每次迭代中使用的最大修正因子数量，maxls表示每次迭代中最大线搜索次数，ftol表示终止条件，这里是1.0*np.finfo(float).eps，其中np.finfo(float).eps表示浮点数的精度，1.0 * np.finfo(float).eps表示浮点数的精度乘以1.0。\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps}) #np.finfo()是numpy库中的一个函数，用于获取浮点数的精度。np.finfo(float).eps返回的是浮点数的及其精度，也就是1.0和比1.0大的最小浮点数之间的差(大多数机器上，对于双精度浮点数，如python中的float，这个值大约是2.22e-16)\n",
    "        #使用函数tf.train.AdamOptimizer创建了一个优化器self.optimizer_Adam，它使用了Adam方法\n",
    "            #tf.train.AdamOptimizer函数语法：tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')，第一个参数表示学习率；第二个参数表示一阶矩估计的指数衰减率；第三个参数表示二阶矩估计的指数衰减率；第四个参数表示防止除0错误的小常数；第五个参数默认False，若为True则在更新操作中对变量加锁；最后一个是操作的名称，默认为'Adam'\n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        #使用函数创建了一个名为self.train_op_Adam的训练操作，使用Adam优化器最小化损失函数（这里是之前定义的self.loss）\n",
    "            #minimize函数语法：minimize(loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)，参数loss表示需要最小化的损失函数；第二个参数可选，若提供则每次优化操作后该值增加1；第三个参数是需要优化的变量列表（不提供默认所有的可训练变量）；其他参数大多数情况下不需要设置\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        # tf session，使用TensorFlow库创建了一个会话self.sess，是用于执行计算图的环境。\n",
    "    \n",
    "        #使用tf.Session创建了一个名为tf.sess的会话。接受一个config参数，这是一个tf.ConfigProto对象，用于设置会话的配置选项。这里设置了两个选项：\n",
    "              #allow_soft_placement：如果设置为True，那么当某些操作无法在GPU上执行时，TensorFlow会自动将它们放在CPU上执行；\n",
    "              #log_device_placement：如果设置为True，那么在日志中会记录每个节点被安排在哪个设备上执行\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        #使用tf.global_variables_initializer函数创建了一个初始化所有全局变量的操作（Tensorflow中所有变量在使用之前都需要进行初始化）\n",
    "        init = tf.global_variables_initializer()\n",
    "        #sess.run是会话的一个方法，用于执行图中的操作或计算张量的值。这里是执行初始化\n",
    "        self.sess.run(init)\n",
    "\n",
    "    #定义了一个名为`initialize_NN'的函数/方法，用于初始化神经网络的权重和偏置，最后返回权重和偏置。          \n",
    "    def initialize_NN(self, layers): #接受一个参数layers\n",
    "        #定义了两个空列表weights和biases，用于存储神经网络的权重和偏置。        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)  #获取神经网络的层数（由输入参数给出）并将其赋值给num_layers\n",
    "        for l in range(0,num_layers-1):  #使用循环遍历神经网络的每一层（除了输出层）\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]]) #初始化该层的权重，使用了xavier_init函数(该函数定义在下面)进行Xaiver初始化，权重的形状是layers[l]*layers[l+1]，其中layers[l]是上一层的神经元数量，layers[l+1]是当前层的神经元数量。\n",
    "              #tf.Variable(initial_value, dtype=None, name=None)函数用于创建一个变量，接受三个参数，第一个参数initial_value是变量的初始值，第二个参数dtype是变量的数据类型，第三个参数name是变量的名称。\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32) #初始化该层的偏置b，使用了tf.Variable函数，将偏置b初始化为一个形状为[1,layers[l+1]]的全0数组，其中layers[l+1]是当前层的神经元数量\n",
    "              #list.append(element)是Python中的一个方法，用于向列表中添加元素（添加到列表末尾）。list是要添加元素的列表，element是要添加的元素（然和类型的参数均可）\n",
    "            weights.append(W) #将初始化的权重和偏置添加到weights和biases列表中\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    #定义了一个名为xavier_init的函数/方法，用于初始化神经网络的权重(在神经网络参数初始化中实现，见上面)。这个函数使用了Xavier初始化方法，这是一种常用的权重初始化方法，可以帮助我们在训练深度神经网络时保持每一层的激活值的分布相对稳定。\n",
    "    def xavier_init(self, size):   #接受一个参数size\n",
    "        in_dim = size[0]  #输入维度是size的第一个数\n",
    "        out_dim = size[1]     #输出维度是size的第二个数\n",
    "        xavier_stddev = np.sqrt(2/(in_dim+out_dim))   #计算标准差\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32) #返回一个变量，类型为32位浮点，初始值为截断正态分布，标准差为xavier_stddev，形状为[in_dim, out_dim]，其中in_dim和out_dim分别是输入维度和输出维度\n",
    "    \n",
    "    #定义了一个名为neural_net的函数/方法，用于实现神经网络的输出。这个方法接受三个参数，分别是X，weights和biases，其中X是输入数据，weights和biases是神经网络的权重和偏置。\n",
    "    def neural_net(self,X,weights,biases):\n",
    "        num_layers=len(weights)+1 #计算神经网络的层数并返回到num_layers，其值位权重矩阵的长度（行数）加1\n",
    "        \n",
    "        H=2.0*(X-self.lb)/(self.ub-self.lb)-1.0 #这里H是X经过归一化处理后的结果，将X映射到了[-1,1]区间内\n",
    "        for l in range(0,num_layers-2): #使用循环遍历神经网络的每一层（除了输出层）\n",
    "            W=weights[l] #获取当前层的权重\n",
    "            b=biases[l] #获取当前层的偏置\n",
    "            H=tf.tanh(tf.add(tf.matmul(H,W),b)) #计算当前层的输出，使用了tf.matmul函数计算矩阵乘法，tf.add函数计算矩阵加法，tf.tanh函数计算双曲正切函数\n",
    "        W=weights[-1] #获取输出层的权重,这里的weights[-1]表示列表weights的最后一个元素\n",
    "        b=biases[-1] #获取输出层的偏置\n",
    "        Y=tf.add(tf.matmul(H,W),b)  #计算输出层的输出H*W+b\n",
    "        return Y #返回输出层的输出Y\n",
    "    \n",
    "\n",
    "\n",
    "    #定义了一个名为net_uv的函数/方法，用于计算神经网络的输出以及输出关于输入x的梯度。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回神经网络的两个输出以及输出它们关于输入x的梯度。\n",
    "    def net_uv(self,x,t):\n",
    "            #tf.concat(values, axis)，用于将多个张量在指定的维度上进行拼接，接受两个参数，第一个参数values是一个列表，表示需要拼接的张量；第二个参数axis是一个整数，表示拼接的维度。\n",
    "        X=tf.concat([x,t],1)   #将输入的两个参数x和t在第二个维度（列）上进行拼接，形成一个新的张量X\n",
    "        #调用之前定义的neural_net函数，根据两个参数权重和偏置，以及上一步得到的张量X，计算神经网络的输出uv\n",
    "        uv=self.neural_net(X,self.weights,self.biases)\n",
    "        #将uv（是一个二维张量）的第一列赋值给u，第二列赋值给v\n",
    "        u=uv[:,0:1]\n",
    "        v=uv[:,1:2]\n",
    "        #分别计算u和v关于x的梯度，使用了tf.gradients函数，它接受两个参数，第一个参数ys是一个张量，表示需要求导的张量；第二个参数xs是一个张量或张量列表，表示需要对哪些张量求导。\n",
    "            #tf.gradients(ys, xs, grad_ys=None, name=\"gradients\", colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)：第一个参数ys是一个张量，表示需要求导的张量；第二个参数xs是一个张量或张量列表，表示需要对哪些张量求导；\n",
    "            #第三个参数表示一个与ys相同长度的张量列表，每个张量都是ys中对应张量的梯度，若为None则假设每个ys的梯度为1；其他参数用于控制梯度计算的细节，通常不需要修改\n",
    "            #最后的输出是一个列表，列表中的每个元素都是一个张量，表示`ys`中对应元素关于`xs`中对应元素的梯度：若`ys`和`xs`都是长度为`n`的张量列表，那么`tf.gradients(ys,xs)`的输出就是一个长度为`n`的张量列表，其中第`i`个张量是`ys[i]`关于`xs[i]`的梯度；若`ys`是一个张量，`xs`是一个张量列表，那么`tf.gradients(ys, xs)`的输出就是一个长度与`xs`相同的张量列表，其中第`i`个张量是`ys`关于`xs[i]`的梯度。\n",
    "        u_x=tf.gradients(u,x)[0]  #计算u关于x的梯度\n",
    "        v_x=tf.gradients(v,x)[0]  #计算v关于x的梯度\n",
    "        return u,v,u_x,v_x\n",
    "    \n",
    "\n",
    "\n",
    "    #定义了一个名为net_f_uv的函数/方法，用于计算f_u和f_v。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回计算得到的f_u和f_v。\n",
    "    def net_f_uv(self, x, t):\n",
    "        u,v,u_x,v_x=self.net_uv(x,t) #调用上面的函数/方法，计算神经网络的输出（两个）以及输出关于输入x的梯度（两个）\n",
    "        u_t=tf.gradients(u,t)[0] #计算u关于t的梯度\n",
    "        u_xx=tf.gradients(u_x,x)[0] #计算u_x关于x的梯度，也就是u关于x的二阶导数\n",
    "        v_t=tf.gradients(v,t)[0] #计算v关于t的梯度\n",
    "        v_xx=tf.gradients(v_x,x)[0] #计算v_x关于x的梯度，也就是v关于x的二阶导数\n",
    "        \n",
    "        f_u=u_t+0.5*v_xx+(u**2+v**2)*v    #计算f_u,定义见论文\n",
    "        f_v=v_t-0.5*u_xx-(u**2+v**2)*u   #计算f_v,定义见论文\n",
    "        \n",
    "        return f_u,f_v\n",
    "    \n",
    "    def callback(self,loss):  #定义了一个名为callback的函数/方法，打印损失值\n",
    "        print('Loss:',loss)\n",
    "\n",
    "\n",
    "    \n",
    "    #定义了一个名为train的函数/方法，用于训练神经网络。这个方法接受一个参数nIter，表示训练的迭代次数。\n",
    "    def train(self, nIter):\n",
    "        #创建一个名为tf_dict的字典，该字典将TensorFlow占位符映射到它们对应的数据。创建tf_dict的目的是为了在运行TensorFlow的计算图时，能够将数据传递给占位符。例如，当运行self.sess.run(self.train_op_Adam, tf_dict)时，tf_dict中的数据就会被传递给对应的占位符，然后在计算图中使用\n",
    "            #字典语法：dict = {key1: value1, key2: value2, ...}：key1、key2等是字典的键，value1、value2等是对应的值（这里键是占位符，值是对应的数据）。\n",
    "        tf_dict = {self.x0_tf:self.x0,self.t0_tf:self.t0,\n",
    "                   self.u0_tf:self.u0,self.v0_tf:self.v0,\n",
    "                   self.x_lb_tf:self.x_lb,self.t_lb_tf:self.t_lb,\n",
    "                   self.x_ub_tf:self.x_ub,self.t_ub_tf:self.t_ub,\n",
    "                   self.x_f_tf:self.x_f,self.t_f_tf:self.t_f}\n",
    "        #time.time()函数用于获取当前时间并赋值给start_time\n",
    "\n",
    "        #writer=tf.summary.FileWriter('logs/',sess.graph) #tensorboard可视化\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):  #进行nIter次训练迭代\n",
    "            #每次迭代时传入输入数据，并先执行Adam优化器的训练操作\n",
    "                #sess.run(fetches, feed_dict=None, options=None, run_metadata=None)：该函数用于执行TensorFlow操作或评估Tensor对象。第一个参数表示需要执行的操作或需要评估的Tensor；第二个参数feed_dict是一个字典，用于将数据传递给占位符；其他参数用于控制会话的细节，通常不需要修改\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "            # Print，如果迭代次数是10的倍数，那么就计算并打印出当前的迭代次数、损失值和训练时间\n",
    "            if it % 100 == 0:\n",
    "                elapsed = time.time() - start_time  #每十次迭代计算下运行时间\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)  #每十次迭代计算下损失值\n",
    "                print('It: %d, Loss: %.3e, Time: %.2f' %   \n",
    "                      (it, loss_value, elapsed))  #打印迭代次数、损失值和训练时间\n",
    "                start_time = time.time()  #更新当前时间，为计算下一个10次迭代的运行时间做准备\n",
    "        #进一步使用L-BFGS-B优化器来最小化损失函数，第一个参数表示TensorFlow会话，第二个参数表示将数据传递给占位符的字典，fetches表示要获取的结果（这里只获取了损失函数的值），最后一个参数表示每次优化迭代后调用的回调参数，这里是之前定义的，用来打印损失函数                                                  \n",
    "        self.optimizer.minimize(self.sess, \n",
    "                                feed_dict = tf_dict,         \n",
    "                                fetches = [self.loss],    \n",
    "                                loss_callback = self.callback)        \n",
    "                                    \n",
    "    #定义了一个名为predict的函数/方法，用于预测神经网络的输出。这个方法接受一个参数X_star，表示输入数据。最后返回预测的两个输出和两个输出的梯度。\n",
    "    def predict(self, X_star):\n",
    "        #创建一个字典，其中包含了两个键值对，键是 TensorFlow 的占位符（self.x0_tf 和 self.t0_tf），值是 X_star中的对应列。这个字典将被用于在下面的TensorFlow 会话中给placeholder赋值，用以运行计算图。\n",
    "        tf_dict = {self.x0_tf: X_star[:,0:1], self.t0_tf: X_star[:,1:2]}\n",
    "        #使用 self.sess.run方法来给placeholder赋值并运行计算图，并获取 self.u0_pred(神经网络的输出)和self.v0_pred 的计算结果。这两个结果被保存在 u_star 和 v_star 中\n",
    "        u_star = self.sess.run(self.u0_pred, tf_dict)  \n",
    "        v_star = self.sess.run(self.v0_pred, tf_dict)  \n",
    "        #创建了另一个字典tf_dict，其中包含了两个键值对，键是TensorFlow的占位符（self.x_f_tf和self.t_f_tf）值是 X_star中的对应列。这个字典将被用于在 TensorFlow 会话中运行计算图\n",
    "        tf_dict = {self.x_f_tf: X_star[:,0:1], self.t_f_tf: X_star[:,1:2]}\n",
    "        #使用 self.sess.run方法来运行计算图给placeholder赋值并运行计算图，并获取 self.f_u_pred(神经网络的输出)和self.f_v_pred 的计算结果。这两个结果被保存在 f_u_star 和 f_v_star 中\n",
    "        f_u_star = self.sess.run(self.f_u_pred, tf_dict)\n",
    "        f_v_star = self.sess.run(self.f_v_pred, tf_dict)\n",
    "               \n",
    "        return u_star, v_star, f_u_star, f_v_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:53:00.0, compute capability: 7.5\n",
      "\n",
      "It: 0, Loss: 7.937e-01, Time: 1.44\n",
      "It: 100, Loss: 2.403e-01, Time: 2.99\n",
      "It: 200, Loss: 1.154e-01, Time: 2.94\n",
      "It: 300, Loss: 7.387e-02, Time: 2.92\n",
      "It: 400, Loss: 6.083e-02, Time: 2.97\n",
      "It: 500, Loss: 5.405e-02, Time: 2.97\n",
      "It: 600, Loss: 4.760e-02, Time: 2.97\n",
      "It: 700, Loss: 4.777e-02, Time: 2.94\n",
      "It: 800, Loss: 4.846e-02, Time: 2.96\n",
      "It: 900, Loss: 3.978e-02, Time: 2.96\n",
      "It: 1000, Loss: 3.630e-02, Time: 2.95\n",
      "It: 1100, Loss: 3.479e-02, Time: 2.96\n",
      "It: 1200, Loss: 3.464e-02, Time: 2.92\n",
      "It: 1300, Loss: 3.275e-02, Time: 2.93\n",
      "It: 1400, Loss: 2.877e-02, Time: 2.90\n",
      "It: 1500, Loss: 2.809e-02, Time: 2.98\n",
      "It: 1600, Loss: 3.043e-02, Time: 2.93\n",
      "It: 1700, Loss: 2.534e-02, Time: 2.97\n",
      "It: 1800, Loss: 2.397e-02, Time: 2.92\n",
      "It: 1900, Loss: 2.131e-02, Time: 2.94\n",
      "It: 2000, Loss: 2.043e-02, Time: 2.94\n",
      "It: 2100, Loss: 1.921e-02, Time: 2.94\n",
      "It: 2200, Loss: 1.832e-02, Time: 2.93\n",
      "It: 2300, Loss: 1.739e-02, Time: 2.96\n",
      "It: 2400, Loss: 1.690e-02, Time: 2.96\n",
      "It: 2500, Loss: 1.557e-02, Time: 2.96\n",
      "It: 2600, Loss: 3.585e-02, Time: 2.91\n",
      "It: 2700, Loss: 4.534e-02, Time: 2.95\n",
      "It: 2800, Loss: 1.565e-02, Time: 2.97\n",
      "It: 2900, Loss: 1.265e-02, Time: 2.94\n",
      "It: 3000, Loss: 1.195e-02, Time: 2.96\n",
      "It: 3100, Loss: 1.740e-02, Time: 2.97\n",
      "It: 3200, Loss: 2.421e-02, Time: 2.94\n",
      "It: 3300, Loss: 1.229e-02, Time: 2.98\n",
      "It: 3400, Loss: 9.443e-03, Time: 2.96\n",
      "It: 3500, Loss: 8.868e-03, Time: 2.98\n",
      "It: 3600, Loss: 8.569e-03, Time: 3.01\n",
      "It: 3700, Loss: 8.050e-03, Time: 2.98\n",
      "It: 3800, Loss: 5.195e-02, Time: 2.95\n",
      "It: 3900, Loss: 7.035e-03, Time: 2.97\n",
      "It: 4000, Loss: 6.682e-03, Time: 2.92\n",
      "It: 4100, Loss: 6.471e-03, Time: 2.92\n",
      "It: 4200, Loss: 1.669e-02, Time: 2.93\n",
      "It: 4300, Loss: 5.753e-03, Time: 2.95\n",
      "It: 4400, Loss: 5.552e-03, Time: 2.93\n",
      "It: 4500, Loss: 1.200e-02, Time: 2.95\n",
      "It: 4600, Loss: 6.631e-03, Time: 2.96\n",
      "It: 4700, Loss: 4.705e-03, Time: 2.95\n",
      "It: 4800, Loss: 4.498e-03, Time: 2.98\n",
      "It: 4900, Loss: 4.306e-03, Time: 2.92\n",
      "It: 5000, Loss: 5.171e-03, Time: 2.95\n",
      "It: 5100, Loss: 5.461e-03, Time: 2.92\n",
      "It: 5200, Loss: 3.755e-03, Time: 2.98\n",
      "It: 5300, Loss: 7.352e-02, Time: 2.97\n",
      "It: 5400, Loss: 3.412e-03, Time: 2.96\n",
      "It: 5500, Loss: 4.691e-03, Time: 2.91\n",
      "It: 5600, Loss: 3.082e-03, Time: 2.94\n",
      "It: 5700, Loss: 3.022e-03, Time: 2.95\n",
      "It: 5800, Loss: 9.585e-03, Time: 2.92\n",
      "It: 5900, Loss: 2.758e-03, Time: 2.93\n",
      "It: 6000, Loss: 2.583e-03, Time: 2.92\n",
      "It: 6100, Loss: 2.523e-03, Time: 2.95\n",
      "It: 6200, Loss: 8.387e-02, Time: 2.92\n",
      "It: 6300, Loss: 2.334e-03, Time: 2.98\n",
      "It: 6400, Loss: 2.187e-03, Time: 2.95\n",
      "It: 6500, Loss: 2.134e-03, Time: 2.96\n",
      "It: 6600, Loss: 2.626e-03, Time: 2.96\n",
      "It: 6700, Loss: 3.264e-03, Time: 2.96\n",
      "It: 6800, Loss: 1.849e-03, Time: 2.95\n",
      "It: 6900, Loss: 1.855e-03, Time: 2.94\n",
      "It: 7000, Loss: 1.724e-03, Time: 2.94\n",
      "It: 7100, Loss: 2.247e-03, Time: 2.94\n",
      "It: 7200, Loss: 1.606e-03, Time: 2.94\n",
      "It: 7300, Loss: 1.619e-03, Time: 2.94\n",
      "It: 7400, Loss: 2.384e-03, Time: 2.96\n",
      "It: 7500, Loss: 4.333e-03, Time: 2.91\n",
      "It: 7600, Loss: 1.923e-03, Time: 2.95\n",
      "It: 7700, Loss: 3.622e-03, Time: 2.92\n",
      "It: 7800, Loss: 1.309e-03, Time: 2.93\n",
      "It: 7900, Loss: 2.698e-03, Time: 2.99\n",
      "It: 8000, Loss: 1.273e-03, Time: 2.99\n",
      "It: 8100, Loss: 4.868e-03, Time: 3.01\n",
      "It: 8200, Loss: 1.469e-02, Time: 3.00\n",
      "It: 8300, Loss: 1.644e-02, Time: 3.02\n",
      "It: 8400, Loss: 1.561e-03, Time: 3.01\n",
      "It: 8500, Loss: 1.067e-03, Time: 3.02\n",
      "It: 8600, Loss: 1.158e-03, Time: 3.02\n",
      "It: 8700, Loss: 1.023e-03, Time: 3.06\n",
      "It: 8800, Loss: 1.005e-03, Time: 3.17\n",
      "It: 8900, Loss: 1.208e-03, Time: 3.07\n",
      "It: 9000, Loss: 2.105e-03, Time: 3.16\n",
      "It: 9100, Loss: 9.024e-04, Time: 3.02\n",
      "It: 9200, Loss: 1.759e-03, Time: 3.01\n",
      "It: 9300, Loss: 3.206e-02, Time: 3.02\n",
      "It: 9400, Loss: 8.335e-04, Time: 3.01\n",
      "It: 9500, Loss: 1.029e-03, Time: 2.99\n",
      "It: 9600, Loss: 9.385e-04, Time: 3.02\n",
      "It: 9700, Loss: 9.397e-03, Time: 3.02\n",
      "It: 9800, Loss: 1.429e-03, Time: 3.01\n",
      "It: 9900, Loss: 1.235e-03, Time: 3.02\n",
      "It: 10000, Loss: 7.311e-04, Time: 3.01\n",
      "It: 10100, Loss: 7.770e-04, Time: 2.99\n",
      "It: 10200, Loss: 1.038e-03, Time: 3.01\n",
      "It: 10300, Loss: 2.572e-03, Time: 2.98\n",
      "It: 10400, Loss: 2.349e-02, Time: 2.97\n",
      "It: 10500, Loss: 6.501e-04, Time: 3.00\n",
      "It: 10600, Loss: 7.030e-04, Time: 3.01\n",
      "It: 10700, Loss: 1.145e-03, Time: 3.00\n",
      "It: 10800, Loss: 6.826e-04, Time: 3.00\n",
      "It: 10900, Loss: 2.184e-03, Time: 3.00\n",
      "It: 11000, Loss: 5.778e-04, Time: 2.98\n",
      "It: 11100, Loss: 4.575e-03, Time: 3.00\n",
      "It: 11200, Loss: 1.853e-03, Time: 3.01\n",
      "It: 11300, Loss: 6.288e-04, Time: 3.02\n",
      "It: 11400, Loss: 5.610e-04, Time: 3.01\n",
      "It: 11500, Loss: 3.806e-03, Time: 3.02\n",
      "It: 11600, Loss: 5.342e-04, Time: 3.01\n",
      "It: 11700, Loss: 6.290e-04, Time: 3.01\n",
      "It: 11800, Loss: 1.106e-03, Time: 3.02\n",
      "It: 11900, Loss: 5.115e-04, Time: 3.00\n",
      "It: 12000, Loss: 6.281e-04, Time: 3.03\n",
      "It: 12100, Loss: 5.297e-04, Time: 3.01\n",
      "It: 12200, Loss: 1.305e-03, Time: 3.09\n",
      "It: 12300, Loss: 5.267e-04, Time: 3.15\n",
      "It: 12400, Loss: 4.828e-04, Time: 3.21\n",
      "It: 12500, Loss: 7.706e-04, Time: 3.11\n",
      "It: 12600, Loss: 5.987e-04, Time: 3.14\n",
      "It: 12700, Loss: 9.351e-04, Time: 3.10\n",
      "It: 12800, Loss: 6.239e-04, Time: 3.05\n",
      "It: 12900, Loss: 4.460e-04, Time: 3.09\n",
      "It: 13000, Loss: 2.440e-03, Time: 3.02\n",
      "It: 13100, Loss: 1.256e-03, Time: 3.02\n",
      "It: 13200, Loss: 2.020e-03, Time: 3.00\n",
      "It: 13300, Loss: 1.781e-02, Time: 3.01\n",
      "It: 13400, Loss: 4.842e-04, Time: 3.00\n",
      "It: 13500, Loss: 7.145e-04, Time: 3.01\n",
      "It: 13600, Loss: 5.139e-04, Time: 2.99\n",
      "It: 13700, Loss: 4.418e-04, Time: 3.02\n",
      "It: 13800, Loss: 3.991e-04, Time: 2.98\n",
      "It: 13900, Loss: 3.937e-04, Time: 3.00\n",
      "It: 14000, Loss: 1.635e-03, Time: 3.10\n",
      "It: 14100, Loss: 3.152e-03, Time: 3.02\n",
      "It: 14200, Loss: 5.151e-04, Time: 3.02\n",
      "It: 14300, Loss: 5.305e-04, Time: 3.01\n",
      "It: 14400, Loss: 6.543e-04, Time: 3.01\n",
      "It: 14500, Loss: 3.703e-04, Time: 3.00\n",
      "It: 14600, Loss: 3.694e-04, Time: 3.01\n",
      "It: 14700, Loss: 1.107e-03, Time: 3.01\n",
      "It: 14800, Loss: 4.994e-03, Time: 3.01\n",
      "It: 14900, Loss: 1.036e-03, Time: 3.01\n",
      "It: 15000, Loss: 5.957e-04, Time: 3.01\n",
      "It: 15100, Loss: 8.619e-04, Time: 2.98\n",
      "It: 15200, Loss: 3.627e-04, Time: 3.01\n",
      "It: 15300, Loss: 6.640e-03, Time: 2.98\n",
      "It: 15400, Loss: 6.719e-04, Time: 2.96\n",
      "It: 15500, Loss: 4.523e-04, Time: 2.99\n",
      "It: 15600, Loss: 5.818e-04, Time: 3.00\n",
      "It: 15700, Loss: 4.351e-04, Time: 2.98\n",
      "It: 15800, Loss: 3.305e-03, Time: 2.99\n",
      "It: 15900, Loss: 2.132e-03, Time: 3.01\n",
      "It: 16000, Loss: 7.805e-03, Time: 3.01\n",
      "It: 16100, Loss: 3.266e-04, Time: 3.02\n",
      "It: 16200, Loss: 1.120e-03, Time: 3.02\n",
      "It: 16300, Loss: 4.766e-04, Time: 3.02\n",
      "It: 16400, Loss: 4.430e-03, Time: 3.01\n",
      "It: 16500, Loss: 5.621e-04, Time: 3.00\n",
      "It: 16600, Loss: 9.973e-04, Time: 3.01\n",
      "It: 16700, Loss: 3.918e-04, Time: 3.00\n",
      "It: 16800, Loss: 3.464e-04, Time: 3.00\n",
      "It: 16900, Loss: 2.975e-04, Time: 2.99\n",
      "It: 17000, Loss: 3.522e-04, Time: 3.04\n",
      "It: 17100, Loss: 4.744e-04, Time: 3.10\n",
      "It: 17200, Loss: 4.162e-03, Time: 3.07\n",
      "It: 17300, Loss: 5.920e-04, Time: 3.09\n",
      "It: 17400, Loss: 4.461e-04, Time: 3.07\n",
      "It: 17500, Loss: 1.158e-03, Time: 3.10\n",
      "It: 17600, Loss: 3.921e-04, Time: 3.12\n",
      "It: 17700, Loss: 6.957e-04, Time: 3.07\n",
      "It: 17800, Loss: 9.911e-03, Time: 3.08\n",
      "It: 17900, Loss: 6.415e-03, Time: 3.06\n",
      "It: 18000, Loss: 5.009e-03, Time: 3.07\n",
      "It: 18100, Loss: 4.085e-04, Time: 3.01\n",
      "It: 18200, Loss: 4.801e-04, Time: 3.03\n",
      "It: 18300, Loss: 9.402e-04, Time: 3.08\n",
      "It: 18400, Loss: 2.457e-03, Time: 3.01\n",
      "It: 18500, Loss: 4.040e-04, Time: 3.02\n",
      "It: 18600, Loss: 2.746e-04, Time: 3.00\n",
      "It: 18700, Loss: 3.094e-04, Time: 3.00\n",
      "It: 18800, Loss: 2.798e-04, Time: 3.03\n",
      "It: 18900, Loss: 2.873e-03, Time: 3.01\n",
      "It: 19000, Loss: 4.196e-04, Time: 2.99\n",
      "It: 19100, Loss: 2.747e-04, Time: 3.01\n",
      "It: 19200, Loss: 5.066e-04, Time: 3.02\n",
      "It: 19300, Loss: 3.221e-04, Time: 3.00\n",
      "It: 19400, Loss: 4.462e-04, Time: 3.03\n",
      "It: 19500, Loss: 4.093e-04, Time: 3.00\n",
      "It: 19600, Loss: 7.185e-03, Time: 2.99\n",
      "It: 19700, Loss: 1.085e-02, Time: 2.99\n",
      "It: 19800, Loss: 6.212e-04, Time: 2.99\n",
      "It: 19900, Loss: 2.395e-02, Time: 2.99\n",
      "It: 20000, Loss: 9.935e-03, Time: 3.05\n",
      "It: 20100, Loss: 1.005e-02, Time: 3.04\n",
      "It: 20200, Loss: 2.415e-03, Time: 3.06\n",
      "It: 20300, Loss: 3.911e-04, Time: 3.06\n",
      "It: 20400, Loss: 1.475e-02, Time: 3.03\n",
      "It: 20500, Loss: 6.334e-03, Time: 3.04\n",
      "It: 20600, Loss: 9.384e-03, Time: 3.03\n",
      "It: 20700, Loss: 5.154e-04, Time: 3.01\n",
      "It: 20800, Loss: 3.360e-03, Time: 3.03\n",
      "It: 20900, Loss: 3.305e-03, Time: 3.05\n",
      "It: 21000, Loss: 1.728e-03, Time: 3.07\n",
      "It: 21100, Loss: 9.381e-04, Time: 3.04\n",
      "It: 21200, Loss: 2.416e-04, Time: 3.05\n",
      "It: 21300, Loss: 2.903e-04, Time: 3.05\n",
      "It: 21400, Loss: 5.658e-04, Time: 3.05\n",
      "It: 21500, Loss: 3.230e-04, Time: 3.05\n",
      "It: 21600, Loss: 4.979e-04, Time: 3.03\n",
      "It: 21700, Loss: 2.507e-04, Time: 2.98\n",
      "It: 21800, Loss: 2.252e-04, Time: 3.02\n",
      "It: 21900, Loss: 2.514e-04, Time: 3.01\n",
      "It: 22000, Loss: 4.087e-04, Time: 3.04\n",
      "It: 22100, Loss: 5.611e-04, Time: 2.99\n",
      "It: 22200, Loss: 2.576e-04, Time: 3.00\n",
      "It: 22300, Loss: 3.027e-04, Time: 3.01\n",
      "It: 22400, Loss: 1.963e-04, Time: 3.00\n",
      "It: 22500, Loss: 2.066e-04, Time: 3.03\n",
      "It: 22600, Loss: 3.421e-04, Time: 3.02\n",
      "It: 22700, Loss: 8.749e-04, Time: 3.05\n",
      "It: 22800, Loss: 4.267e-04, Time: 3.06\n",
      "It: 22900, Loss: 8.774e-04, Time: 3.11\n",
      "It: 23000, Loss: 3.424e-04, Time: 3.09\n",
      "It: 23100, Loss: 4.379e-03, Time: 3.01\n",
      "It: 23200, Loss: 1.364e-03, Time: 3.05\n",
      "It: 23300, Loss: 2.545e-03, Time: 3.01\n",
      "It: 23400, Loss: 3.245e-04, Time: 3.01\n",
      "It: 23500, Loss: 1.999e-04, Time: 3.01\n",
      "It: 23600, Loss: 2.813e-04, Time: 3.02\n",
      "It: 23700, Loss: 3.081e-04, Time: 3.02\n",
      "It: 23800, Loss: 6.196e-04, Time: 3.00\n",
      "It: 23900, Loss: 1.931e-04, Time: 3.02\n",
      "It: 24000, Loss: 4.188e-04, Time: 3.01\n",
      "It: 24100, Loss: 2.272e-04, Time: 3.03\n",
      "It: 24200, Loss: 2.250e-04, Time: 3.04\n",
      "It: 24300, Loss: 1.072e-02, Time: 3.04\n",
      "It: 24400, Loss: 2.581e-04, Time: 3.00\n",
      "It: 24500, Loss: 2.090e-04, Time: 3.04\n",
      "It: 24600, Loss: 5.257e-04, Time: 3.02\n",
      "It: 24700, Loss: 4.358e-04, Time: 3.01\n",
      "It: 24800, Loss: 1.916e-04, Time: 3.00\n",
      "It: 24900, Loss: 9.010e-03, Time: 3.02\n",
      "It: 25000, Loss: 7.538e-04, Time: 2.99\n",
      "It: 25100, Loss: 4.074e-04, Time: 3.02\n",
      "It: 25200, Loss: 2.683e-03, Time: 3.01\n",
      "It: 25300, Loss: 1.875e-04, Time: 3.03\n",
      "It: 25400, Loss: 1.845e-04, Time: 3.02\n",
      "It: 25500, Loss: 1.654e-04, Time: 3.03\n",
      "It: 25600, Loss: 8.090e-03, Time: 3.03\n",
      "It: 25700, Loss: 4.187e-04, Time: 3.00\n",
      "It: 25800, Loss: 5.414e-04, Time: 3.02\n",
      "It: 25900, Loss: 5.869e-04, Time: 3.02\n",
      "It: 26000, Loss: 3.722e-03, Time: 3.03\n",
      "It: 26100, Loss: 2.275e-04, Time: 3.03\n",
      "It: 26200, Loss: 2.388e-04, Time: 3.03\n",
      "It: 26300, Loss: 1.445e-03, Time: 3.03\n",
      "It: 26400, Loss: 1.621e-03, Time: 3.03\n",
      "It: 26500, Loss: 2.764e-04, Time: 3.01\n",
      "It: 26600, Loss: 2.919e-04, Time: 3.02\n",
      "It: 26700, Loss: 5.220e-03, Time: 3.02\n",
      "It: 26800, Loss: 1.520e-03, Time: 3.00\n",
      "It: 26900, Loss: 6.171e-04, Time: 3.01\n",
      "It: 27000, Loss: 2.920e-04, Time: 3.02\n",
      "It: 27100, Loss: 2.319e-04, Time: 3.01\n",
      "It: 27200, Loss: 1.789e-04, Time: 3.03\n",
      "It: 27300, Loss: 1.557e-04, Time: 3.00\n",
      "It: 27400, Loss: 4.166e-03, Time: 2.99\n",
      "It: 27500, Loss: 1.564e-04, Time: 3.00\n",
      "It: 27600, Loss: 3.088e-03, Time: 3.01\n",
      "It: 27700, Loss: 3.107e-04, Time: 3.02\n",
      "It: 27800, Loss: 9.929e-03, Time: 3.02\n",
      "It: 27900, Loss: 1.114e-03, Time: 3.01\n",
      "It: 28000, Loss: 1.737e-04, Time: 3.03\n",
      "It: 28100, Loss: 5.123e-04, Time: 3.03\n",
      "It: 28200, Loss: 6.685e-04, Time: 3.00\n",
      "It: 28300, Loss: 1.172e-03, Time: 3.00\n",
      "It: 28400, Loss: 1.124e-03, Time: 3.02\n",
      "It: 28500, Loss: 2.885e-04, Time: 3.02\n",
      "It: 28600, Loss: 2.092e-04, Time: 3.02\n",
      "It: 28700, Loss: 1.479e-03, Time: 3.03\n",
      "It: 28800, Loss: 2.837e-03, Time: 3.03\n",
      "It: 28900, Loss: 4.816e-03, Time: 3.04\n",
      "It: 29000, Loss: 2.000e-04, Time: 3.02\n",
      "It: 29100, Loss: 9.391e-04, Time: 3.03\n",
      "It: 29200, Loss: 2.978e-03, Time: 3.00\n",
      "It: 29300, Loss: 2.356e-04, Time: 3.05\n",
      "It: 29400, Loss: 1.675e-04, Time: 3.01\n",
      "It: 29500, Loss: 3.569e-03, Time: 3.02\n",
      "It: 29600, Loss: 1.166e-03, Time: 2.99\n",
      "It: 29700, Loss: 1.113e-03, Time: 3.02\n",
      "It: 29800, Loss: 3.622e-04, Time: 3.02\n",
      "It: 29900, Loss: 1.137e-03, Time: 3.00\n",
      "It: 30000, Loss: 4.534e-04, Time: 3.01\n",
      "It: 30100, Loss: 7.213e-03, Time: 3.01\n",
      "It: 30200, Loss: 2.376e-04, Time: 3.01\n",
      "It: 30300, Loss: 1.660e-04, Time: 3.01\n",
      "It: 30400, Loss: 1.351e-03, Time: 3.06\n",
      "It: 30500, Loss: 8.172e-04, Time: 3.01\n",
      "It: 30600, Loss: 4.062e-04, Time: 3.05\n",
      "It: 30700, Loss: 8.166e-04, Time: 3.01\n",
      "It: 30800, Loss: 3.848e-03, Time: 3.04\n",
      "It: 30900, Loss: 5.337e-04, Time: 3.05\n",
      "It: 31000, Loss: 1.276e-04, Time: 3.04\n",
      "It: 31100, Loss: 2.200e-04, Time: 3.00\n",
      "It: 31200, Loss: 8.192e-04, Time: 3.04\n",
      "It: 31300, Loss: 4.611e-03, Time: 3.04\n",
      "It: 31400, Loss: 2.465e-03, Time: 3.03\n",
      "It: 31500, Loss: 2.407e-04, Time: 3.02\n",
      "It: 31600, Loss: 1.889e-03, Time: 3.02\n",
      "It: 31700, Loss: 4.990e-04, Time: 3.00\n",
      "It: 31800, Loss: 6.563e-03, Time: 3.05\n",
      "It: 31900, Loss: 3.954e-04, Time: 3.02\n",
      "It: 32000, Loss: 1.632e-03, Time: 3.02\n",
      "It: 32100, Loss: 1.538e-04, Time: 3.01\n",
      "It: 32200, Loss: 1.423e-04, Time: 3.00\n",
      "It: 32300, Loss: 3.709e-03, Time: 3.01\n",
      "It: 32400, Loss: 2.494e-04, Time: 3.04\n",
      "It: 32500, Loss: 8.044e-04, Time: 3.01\n",
      "It: 32600, Loss: 2.113e-04, Time: 3.01\n",
      "It: 32700, Loss: 1.184e-03, Time: 3.03\n",
      "It: 32800, Loss: 1.405e-03, Time: 2.99\n",
      "It: 32900, Loss: 4.160e-04, Time: 3.03\n",
      "It: 33000, Loss: 1.605e-04, Time: 3.02\n",
      "It: 33100, Loss: 1.258e-04, Time: 3.07\n",
      "It: 33200, Loss: 6.894e-04, Time: 3.00\n",
      "It: 33300, Loss: 2.023e-03, Time: 3.02\n",
      "It: 33400, Loss: 9.534e-04, Time: 3.03\n",
      "It: 33500, Loss: 1.436e-04, Time: 3.00\n",
      "It: 33600, Loss: 1.614e-03, Time: 3.02\n",
      "It: 33700, Loss: 1.015e-03, Time: 3.02\n",
      "It: 33800, Loss: 1.895e-03, Time: 3.00\n",
      "It: 33900, Loss: 7.036e-04, Time: 3.02\n",
      "It: 34000, Loss: 1.481e-03, Time: 3.01\n",
      "It: 34100, Loss: 8.037e-04, Time: 3.04\n",
      "It: 34200, Loss: 4.588e-03, Time: 3.03\n",
      "It: 34300, Loss: 2.863e-03, Time: 3.03\n",
      "It: 34400, Loss: 1.207e-04, Time: 3.02\n",
      "It: 34500, Loss: 1.354e-04, Time: 3.02\n",
      "It: 34600, Loss: 1.610e-03, Time: 3.02\n",
      "It: 34700, Loss: 3.747e-04, Time: 3.00\n",
      "It: 34800, Loss: 2.250e-03, Time: 3.01\n",
      "It: 34900, Loss: 2.644e-04, Time: 3.02\n",
      "It: 35000, Loss: 5.888e-04, Time: 3.04\n",
      "It: 35100, Loss: 3.249e-03, Time: 3.03\n",
      "It: 35200, Loss: 3.346e-03, Time: 3.00\n",
      "It: 35300, Loss: 8.925e-04, Time: 3.00\n",
      "It: 35400, Loss: 9.708e-04, Time: 3.00\n",
      "It: 35500, Loss: 4.471e-04, Time: 2.99\n",
      "It: 35600, Loss: 6.768e-04, Time: 3.00\n",
      "It: 35700, Loss: 7.536e-04, Time: 3.06\n",
      "It: 35800, Loss: 2.165e-04, Time: 3.02\n",
      "It: 35900, Loss: 4.524e-04, Time: 3.02\n",
      "It: 36000, Loss: 9.432e-04, Time: 3.02\n",
      "It: 36100, Loss: 6.041e-04, Time: 3.03\n",
      "It: 36200, Loss: 5.088e-03, Time: 3.04\n",
      "It: 36300, Loss: 1.657e-03, Time: 3.02\n",
      "It: 36400, Loss: 1.875e-04, Time: 3.04\n",
      "It: 36500, Loss: 1.497e-03, Time: 3.02\n",
      "It: 36600, Loss: 1.404e-04, Time: 3.03\n",
      "It: 36700, Loss: 4.232e-03, Time: 3.01\n",
      "It: 36800, Loss: 1.750e-04, Time: 3.03\n",
      "It: 36900, Loss: 2.044e-04, Time: 3.03\n",
      "It: 37000, Loss: 3.578e-03, Time: 3.03\n",
      "It: 37100, Loss: 3.338e-04, Time: 3.00\n",
      "It: 37200, Loss: 1.920e-04, Time: 2.97\n",
      "It: 37300, Loss: 2.058e-03, Time: 3.02\n",
      "It: 37400, Loss: 2.288e-04, Time: 3.00\n",
      "It: 37500, Loss: 2.992e-03, Time: 3.05\n",
      "It: 37600, Loss: 3.637e-04, Time: 3.03\n",
      "It: 37700, Loss: 4.082e-04, Time: 3.02\n",
      "It: 37800, Loss: 1.783e-03, Time: 3.06\n",
      "It: 37900, Loss: 1.961e-03, Time: 3.03\n",
      "It: 38000, Loss: 6.352e-04, Time: 3.02\n",
      "It: 38100, Loss: 4.211e-04, Time: 3.02\n",
      "It: 38200, Loss: 6.810e-04, Time: 3.00\n",
      "It: 38300, Loss: 1.168e-03, Time: 3.05\n",
      "It: 38400, Loss: 2.529e-04, Time: 3.03\n",
      "It: 38500, Loss: 1.713e-03, Time: 3.01\n",
      "It: 38600, Loss: 3.230e-04, Time: 3.06\n",
      "It: 38700, Loss: 2.961e-04, Time: 3.02\n",
      "It: 38800, Loss: 1.997e-04, Time: 3.03\n",
      "It: 38900, Loss: 2.295e-04, Time: 2.99\n",
      "It: 39000, Loss: 2.090e-04, Time: 3.01\n",
      "It: 39100, Loss: 6.803e-04, Time: 3.02\n",
      "It: 39200, Loss: 1.091e-03, Time: 3.01\n",
      "It: 39300, Loss: 8.226e-04, Time: 3.04\n",
      "It: 39400, Loss: 2.559e-03, Time: 3.03\n",
      "It: 39500, Loss: 3.502e-03, Time: 3.06\n",
      "It: 39600, Loss: 3.140e-04, Time: 3.03\n",
      "It: 39700, Loss: 1.618e-03, Time: 3.03\n",
      "It: 39800, Loss: 2.371e-04, Time: 3.03\n",
      "It: 39900, Loss: 1.611e-03, Time: 3.04\n",
      "It: 40000, Loss: 1.544e-03, Time: 3.02\n",
      "It: 40100, Loss: 5.663e-04, Time: 3.02\n",
      "It: 40200, Loss: 3.125e-04, Time: 3.02\n",
      "It: 40300, Loss: 7.464e-04, Time: 3.00\n",
      "It: 40400, Loss: 3.592e-03, Time: 3.01\n",
      "It: 40500, Loss: 1.973e-03, Time: 3.02\n",
      "It: 40600, Loss: 1.864e-04, Time: 3.02\n",
      "It: 40700, Loss: 6.410e-04, Time: 3.02\n",
      "It: 40800, Loss: 9.845e-04, Time: 3.05\n",
      "It: 40900, Loss: 4.445e-04, Time: 3.02\n",
      "It: 41000, Loss: 3.747e-04, Time: 3.05\n",
      "It: 41100, Loss: 4.472e-04, Time: 3.02\n",
      "It: 41200, Loss: 3.486e-04, Time: 3.02\n",
      "It: 41300, Loss: 2.451e-04, Time: 3.00\n",
      "It: 41400, Loss: 1.206e-04, Time: 3.01\n",
      "It: 41500, Loss: 2.795e-04, Time: 3.00\n",
      "It: 41600, Loss: 1.255e-04, Time: 3.03\n",
      "It: 41700, Loss: 1.439e-04, Time: 3.01\n",
      "It: 41800, Loss: 2.494e-03, Time: 3.02\n",
      "It: 41900, Loss: 1.271e-04, Time: 3.00\n",
      "It: 42000, Loss: 2.439e-04, Time: 2.99\n",
      "It: 42100, Loss: 5.088e-03, Time: 3.02\n",
      "It: 42200, Loss: 4.179e-03, Time: 2.99\n",
      "It: 42300, Loss: 3.123e-04, Time: 3.02\n",
      "It: 42400, Loss: 4.466e-04, Time: 3.01\n",
      "It: 42500, Loss: 7.740e-04, Time: 3.02\n",
      "It: 42600, Loss: 9.961e-04, Time: 3.02\n",
      "It: 42700, Loss: 1.602e-03, Time: 3.04\n",
      "It: 42800, Loss: 1.184e-04, Time: 3.03\n",
      "It: 42900, Loss: 3.010e-03, Time: 3.03\n",
      "It: 43000, Loss: 2.029e-04, Time: 3.01\n",
      "It: 43100, Loss: 3.128e-03, Time: 3.02\n",
      "It: 43200, Loss: 7.510e-04, Time: 3.03\n",
      "It: 43300, Loss: 1.162e-04, Time: 3.04\n",
      "It: 43400, Loss: 7.873e-03, Time: 2.99\n",
      "It: 43500, Loss: 3.198e-04, Time: 3.02\n",
      "It: 43600, Loss: 6.324e-04, Time: 3.02\n",
      "It: 43700, Loss: 5.588e-04, Time: 3.01\n",
      "It: 43800, Loss: 4.037e-03, Time: 3.01\n",
      "It: 43900, Loss: 3.509e-04, Time: 2.98\n",
      "It: 44000, Loss: 3.822e-03, Time: 3.02\n",
      "It: 44100, Loss: 1.039e-03, Time: 3.02\n",
      "It: 44200, Loss: 4.419e-04, Time: 3.04\n",
      "It: 44300, Loss: 4.221e-03, Time: 3.06\n",
      "It: 44400, Loss: 1.235e-04, Time: 3.04\n",
      "It: 44500, Loss: 8.409e-03, Time: 3.04\n",
      "It: 44600, Loss: 1.829e-04, Time: 3.03\n",
      "It: 44700, Loss: 1.430e-04, Time: 3.00\n",
      "It: 44800, Loss: 2.905e-04, Time: 3.01\n",
      "It: 44900, Loss: 2.226e-03, Time: 3.01\n",
      "It: 45000, Loss: 8.683e-04, Time: 3.00\n",
      "It: 45100, Loss: 3.248e-04, Time: 3.02\n",
      "It: 45200, Loss: 1.489e-03, Time: 3.00\n",
      "It: 45300, Loss: 1.566e-03, Time: 3.03\n",
      "It: 45400, Loss: 3.187e-03, Time: 3.03\n",
      "It: 45500, Loss: 1.660e-04, Time: 3.03\n",
      "It: 45600, Loss: 1.873e-03, Time: 3.05\n",
      "It: 45700, Loss: 2.702e-04, Time: 3.03\n",
      "It: 45800, Loss: 3.753e-03, Time: 3.03\n",
      "It: 45900, Loss: 4.900e-04, Time: 3.03\n",
      "It: 46000, Loss: 2.034e-03, Time: 3.03\n",
      "It: 46100, Loss: 9.057e-04, Time: 3.03\n",
      "It: 46200, Loss: 1.020e-04, Time: 3.02\n",
      "It: 46300, Loss: 3.220e-04, Time: 3.01\n",
      "It: 46400, Loss: 2.453e-04, Time: 3.03\n",
      "It: 46500, Loss: 1.109e-04, Time: 3.02\n",
      "It: 46600, Loss: 1.503e-03, Time: 3.02\n",
      "It: 46700, Loss: 2.157e-04, Time: 3.01\n",
      "It: 46800, Loss: 3.642e-03, Time: 2.99\n",
      "It: 46900, Loss: 8.770e-05, Time: 3.00\n",
      "It: 47000, Loss: 5.674e-04, Time: 2.99\n",
      "It: 47100, Loss: 2.158e-04, Time: 3.03\n",
      "It: 47200, Loss: 9.731e-04, Time: 3.04\n",
      "It: 47300, Loss: 3.411e-04, Time: 3.01\n",
      "It: 47400, Loss: 2.584e-03, Time: 3.19\n",
      "It: 47500, Loss: 2.222e-03, Time: 3.03\n",
      "It: 47600, Loss: 1.731e-04, Time: 3.01\n",
      "It: 47700, Loss: 1.838e-04, Time: 3.03\n",
      "It: 47800, Loss: 2.267e-04, Time: 3.01\n",
      "It: 47900, Loss: 3.457e-04, Time: 3.02\n",
      "It: 48000, Loss: 2.622e-04, Time: 3.00\n",
      "It: 48100, Loss: 1.423e-04, Time: 3.03\n",
      "It: 48200, Loss: 1.168e-04, Time: 3.05\n",
      "It: 48300, Loss: 9.965e-05, Time: 3.04\n",
      "It: 48400, Loss: 1.231e-04, Time: 3.01\n",
      "It: 48500, Loss: 2.604e-04, Time: 3.01\n",
      "It: 48600, Loss: 2.779e-04, Time: 3.00\n",
      "It: 48700, Loss: 3.158e-03, Time: 3.01\n",
      "It: 48800, Loss: 1.227e-04, Time: 3.01\n",
      "It: 48900, Loss: 1.223e-03, Time: 3.02\n",
      "It: 49000, Loss: 3.132e-03, Time: 3.03\n",
      "It: 49100, Loss: 3.215e-03, Time: 3.04\n",
      "It: 49200, Loss: 2.168e-04, Time: 3.04\n",
      "It: 49300, Loss: 1.623e-03, Time: 2.99\n",
      "It: 49400, Loss: 1.949e-03, Time: 3.03\n",
      "It: 49500, Loss: 3.307e-04, Time: 3.01\n",
      "It: 49600, Loss: 1.648e-03, Time: 3.03\n",
      "It: 49700, Loss: 1.917e-03, Time: 3.09\n",
      "It: 49800, Loss: 1.790e-04, Time: 3.01\n",
      "It: 49900, Loss: 1.050e-03, Time: 3.02\n",
      "Loss: 0.0039008087\n",
      "Loss: 9077.621\n",
      "Loss: 19.966948\n",
      "Loss: 0.001767904\n",
      "Loss: 0.0002348858\n",
      "Loss: 0.00018966189\n",
      "Loss: 0.0001799197\n",
      "Loss: 0.00014648137\n",
      "Loss: 0.00016096907\n",
      "Loss: 0.00014074729\n",
      "Loss: 0.0001349624\n",
      "Loss: 0.00011658685\n",
      "Loss: 0.00010764487\n",
      "Loss: 9.884005e-05\n",
      "Loss: 9.417339e-05\n",
      "Loss: 9.187487e-05\n",
      "Loss: 9.0099915e-05\n",
      "Loss: 8.828509e-05\n",
      "Loss: 8.75435e-05\n",
      "Loss: 8.596484e-05\n",
      "Loss: 8.49292e-05\n",
      "Loss: 8.4022424e-05\n",
      "Loss: 8.345349e-05\n",
      "Loss: 8.313344e-05\n",
      "Loss: 8.225764e-05\n",
      "Loss: 8.1630205e-05\n",
      "Loss: 8.137151e-05\n",
      "Loss: 8.104093e-05\n",
      "Loss: 8.05897e-05\n",
      "Loss: 7.997737e-05\n",
      "Loss: 7.9519166e-05\n",
      "Loss: 7.9271136e-05\n",
      "Loss: 7.9155405e-05\n",
      "Loss: 7.898669e-05\n",
      "Loss: 7.871525e-05\n",
      "Loss: 7.8342375e-05\n",
      "Loss: 7.8113735e-05\n",
      "Loss: 7.8010955e-05\n",
      "Loss: 7.7891906e-05\n",
      "Loss: 7.766819e-05\n",
      "Loss: 7.730702e-05\n",
      "Loss: 7.687962e-05\n",
      "Loss: 7.658689e-05\n",
      "Loss: 7.637551e-05\n",
      "Loss: 7.618523e-05\n",
      "Loss: 7.578695e-05\n",
      "Loss: 7.540961e-05\n",
      "Loss: 7.5121745e-05\n",
      "Loss: 7.47413e-05\n",
      "Loss: 7.42868e-05\n",
      "Loss: 7.373964e-05\n",
      "Loss: 7.310658e-05\n",
      "Loss: 7.279754e-05\n",
      "Loss: 7.2597584e-05\n",
      "Loss: 7.2261224e-05\n",
      "Loss: 7.173697e-05\n",
      "Loss: 7.143135e-05\n",
      "Loss: 7.122786e-05\n",
      "Loss: 7.112449e-05\n",
      "Loss: 7.1041584e-05\n",
      "Loss: 7.0935916e-05\n",
      "Loss: 7.131006e-05\n",
      "Loss: 7.0910246e-05\n",
      "Loss: 7.081784e-05\n",
      "Loss: 7.074909e-05\n",
      "Loss: 7.065227e-05\n",
      "Loss: 7.055728e-05\n",
      "Loss: 7.063421e-05\n",
      "Loss: 7.05077e-05\n",
      "Loss: 7.040516e-05\n",
      "Loss: 7.032415e-05\n",
      "Loss: 7.0184426e-05\n",
      "Loss: 7.003186e-05\n",
      "Loss: 7.013217e-05\n",
      "Loss: 6.995887e-05\n",
      "Loss: 6.9827875e-05\n",
      "Loss: 6.975639e-05\n",
      "Loss: 6.973902e-05\n",
      "Loss: 6.9623464e-05\n",
      "Loss: 6.957304e-05\n",
      "Loss: 6.9500595e-05\n",
      "Loss: 6.943292e-05\n",
      "Loss: 6.9582835e-05\n",
      "Loss: 6.939108e-05\n",
      "Loss: 6.9295675e-05\n",
      "Loss: 6.922777e-05\n",
      "Loss: 6.9142174e-05\n",
      "Loss: 6.927895e-05\n",
      "Loss: 6.910725e-05\n",
      "Loss: 6.903677e-05\n",
      "Loss: 6.895156e-05\n",
      "Loss: 6.887513e-05\n",
      "Loss: 6.8754685e-05\n",
      "Loss: 6.8534195e-05\n",
      "Loss: 6.905988e-05\n",
      "Loss: 6.846714e-05\n",
      "Loss: 6.8323556e-05\n",
      "Loss: 6.8221685e-05\n",
      "Loss: 6.811578e-05\n",
      "Loss: 6.803719e-05\n",
      "Loss: 6.797415e-05\n",
      "Loss: 6.7852954e-05\n",
      "Loss: 6.768468e-05\n",
      "Loss: 6.756162e-05\n",
      "Loss: 6.74663e-05\n",
      "Loss: 6.736744e-05\n",
      "Loss: 6.728614e-05\n",
      "Loss: 6.7076304e-05\n",
      "Loss: 6.715463e-05\n",
      "Loss: 6.701149e-05\n",
      "Loss: 6.694041e-05\n",
      "Loss: 6.686686e-05\n",
      "Loss: 6.679617e-05\n",
      "Loss: 6.664147e-05\n",
      "Loss: 6.653476e-05\n",
      "Loss: 6.6411696e-05\n",
      "Loss: 6.629931e-05\n",
      "Loss: 6.6259265e-05\n",
      "Loss: 6.621386e-05\n",
      "Loss: 6.6149994e-05\n",
      "Loss: 6.6741944e-05\n",
      "Loss: 6.6131135e-05\n",
      "Loss: 6.606768e-05\n",
      "Loss: 6.59848e-05\n",
      "Loss: 6.592199e-05\n",
      "Loss: 6.5840686e-05\n",
      "Loss: 6.6005756e-05\n",
      "Loss: 6.58146e-05\n",
      "Loss: 6.5742934e-05\n",
      "Loss: 6.5699074e-05\n",
      "Loss: 6.5635555e-05\n",
      "Loss: 6.555331e-05\n",
      "Loss: 6.61776e-05\n",
      "Loss: 6.552778e-05\n",
      "Loss: 6.545652e-05\n",
      "Loss: 6.540082e-05\n",
      "Loss: 6.532074e-05\n",
      "Loss: 6.5236025e-05\n",
      "Loss: 6.559321e-05\n",
      "Loss: 6.520446e-05\n",
      "Loss: 6.51477e-05\n",
      "Loss: 6.510081e-05\n",
      "Loss: 6.503886e-05\n",
      "Loss: 6.49438e-05\n",
      "Loss: 6.549681e-05\n",
      "Loss: 6.4927706e-05\n",
      "Loss: 6.486156e-05\n",
      "Loss: 6.4813474e-05\n",
      "Loss: 6.479342e-05\n",
      "Loss: 6.47251e-05\n",
      "Loss: 6.469744e-05\n",
      "Loss: 6.463942e-05\n",
      "Loss: 6.4547494e-05\n",
      "Loss: 6.4421714e-05\n",
      "Loss: 6.4326836e-05\n",
      "Loss: 6.423851e-05\n",
      "Loss: 6.41246e-05\n",
      "Loss: 6.403483e-05\n",
      "Loss: 6.3904634e-05\n",
      "Loss: 6.3811836e-05\n",
      "Loss: 6.3704356e-05\n",
      "Loss: 6.361225e-05\n",
      "Loss: 6.351038e-05\n",
      "Loss: 6.340403e-05\n",
      "Loss: 6.3278174e-05\n",
      "Loss: 6.312202e-05\n",
      "Loss: 6.297305e-05\n",
      "Loss: 6.283421e-05\n",
      "Loss: 6.2746505e-05\n",
      "Loss: 6.264686e-05\n",
      "Loss: 6.252307e-05\n",
      "Loss: 6.223418e-05\n",
      "Loss: 6.831868e-05\n",
      "Loss: 6.2209736e-05\n",
      "Loss: 6.2036335e-05\n",
      "Loss: 6.190922e-05\n",
      "Loss: 6.178495e-05\n",
      "Loss: 6.192946e-05\n",
      "Loss: 6.172506e-05\n",
      "Loss: 6.159172e-05\n",
      "Loss: 6.151609e-05\n",
      "Loss: 6.146384e-05\n",
      "Loss: 6.139401e-05\n",
      "Loss: 6.120694e-05\n",
      "Loss: 6.231369e-05\n",
      "Loss: 6.115927e-05\n",
      "Loss: 6.105753e-05\n",
      "Loss: 6.099874e-05\n",
      "Loss: 6.0940823e-05\n",
      "Loss: 6.0824565e-05\n",
      "Loss: 6.114498e-05\n",
      "Loss: 6.0780934e-05\n",
      "Loss: 6.068329e-05\n",
      "Loss: 6.059098e-05\n",
      "Loss: 6.051042e-05\n",
      "Loss: 6.040768e-05\n",
      "Loss: 6.0375656e-05\n",
      "Loss: 6.02901e-05\n",
      "Loss: 6.0259314e-05\n",
      "Loss: 6.0212707e-05\n",
      "Loss: 6.0095757e-05\n",
      "Loss: 6.0673992e-05\n",
      "Loss: 6.0045055e-05\n",
      "Loss: 5.9898786e-05\n",
      "Loss: 5.9814243e-05\n",
      "Loss: 5.9722654e-05\n",
      "Loss: 5.983442e-05\n",
      "Loss: 5.968716e-05\n",
      "Loss: 5.9617243e-05\n",
      "Loss: 5.9559417e-05\n",
      "Loss: 5.9480022e-05\n",
      "Loss: 5.9367543e-05\n",
      "Loss: 5.929698e-05\n",
      "Loss: 5.9118043e-05\n",
      "Loss: 5.9031292e-05\n",
      "Loss: 5.8889316e-05\n",
      "Loss: 5.8812504e-05\n",
      "Loss: 5.866602e-05\n",
      "Loss: 5.859589e-05\n",
      "Loss: 5.8529204e-05\n",
      "Loss: 5.8397593e-05\n",
      "Loss: 5.822104e-05\n",
      "Loss: 5.8037658e-05\n",
      "Loss: 5.7889763e-05\n",
      "Loss: 5.7803234e-05\n",
      "Loss: 5.767255e-05\n",
      "Loss: 5.74809e-05\n",
      "Loss: 5.7309706e-05\n",
      "Loss: 5.7176534e-05\n",
      "Loss: 5.707011e-05\n",
      "Loss: 5.720903e-05\n",
      "Loss: 5.7019715e-05\n",
      "Loss: 5.6945057e-05\n",
      "Loss: 5.6889978e-05\n",
      "Loss: 5.6784986e-05\n",
      "Loss: 5.715547e-05\n",
      "Loss: 5.6761484e-05\n",
      "Loss: 5.669605e-05\n",
      "Loss: 5.6638455e-05\n",
      "Loss: 5.6561912e-05\n",
      "Loss: 5.6441324e-05\n",
      "Loss: 5.6276564e-05\n",
      "Loss: 5.6287732e-05\n",
      "Loss: 5.6191246e-05\n",
      "Loss: 5.6141187e-05\n",
      "Loss: 5.6076933e-05\n",
      "Loss: 5.6033692e-05\n",
      "Loss: 5.5930912e-05\n",
      "Loss: 5.6426925e-05\n",
      "Loss: 5.5897763e-05\n",
      "Loss: 5.5813798e-05\n",
      "Loss: 5.574606e-05\n",
      "Loss: 5.5658515e-05\n",
      "Loss: 5.555513e-05\n",
      "Loss: 5.617566e-05\n",
      "Loss: 5.5520148e-05\n",
      "Loss: 5.543774e-05\n",
      "Loss: 5.5370663e-05\n",
      "Loss: 5.5303368e-05\n",
      "Loss: 5.522599e-05\n",
      "Loss: 5.5195967e-05\n",
      "Loss: 5.506757e-05\n",
      "Loss: 5.5019642e-05\n",
      "Loss: 5.4971668e-05\n",
      "Loss: 5.491445e-05\n",
      "Loss: 5.4748445e-05\n",
      "Loss: 5.6810637e-05\n",
      "Loss: 5.470483e-05\n",
      "Loss: 5.4624164e-05\n",
      "Loss: 5.4569282e-05\n",
      "Loss: 5.4510907e-05\n",
      "Loss: 5.439889e-05\n",
      "Loss: 5.4510085e-05\n",
      "Loss: 5.4335826e-05\n",
      "Loss: 5.421562e-05\n",
      "Loss: 5.412953e-05\n",
      "Loss: 5.4047465e-05\n",
      "Loss: 5.3957978e-05\n",
      "Loss: 5.3850417e-05\n",
      "Loss: 5.3758362e-05\n",
      "Loss: 5.3648073e-05\n",
      "Loss: 5.356795e-05\n",
      "Loss: 5.3470514e-05\n",
      "Loss: 5.358854e-05\n",
      "Loss: 5.342596e-05\n",
      "Loss: 5.3350006e-05\n",
      "Loss: 5.3265823e-05\n",
      "Loss: 5.31786e-05\n",
      "Loss: 5.305815e-05\n",
      "Loss: 5.307037e-05\n",
      "Loss: 5.2976422e-05\n",
      "Loss: 5.2900636e-05\n",
      "Loss: 5.2791045e-05\n",
      "Loss: 5.2732627e-05\n",
      "Loss: 5.268301e-05\n",
      "Loss: 5.262601e-05\n",
      "Loss: 5.2534506e-05\n",
      "Loss: 5.248033e-05\n",
      "Loss: 5.242382e-05\n",
      "Loss: 5.2347204e-05\n",
      "Loss: 5.2606258e-05\n",
      "Loss: 5.2307616e-05\n",
      "Loss: 5.2199015e-05\n",
      "Loss: 5.214555e-05\n",
      "Loss: 5.2041494e-05\n",
      "Loss: 5.192368e-05\n",
      "Loss: 5.1703406e-05\n",
      "Loss: 5.248665e-05\n",
      "Loss: 5.1668234e-05\n",
      "Loss: 5.1565243e-05\n",
      "Loss: 5.1482442e-05\n",
      "Loss: 5.1403207e-05\n",
      "Loss: 5.1299492e-05\n",
      "Loss: 5.1351497e-05\n",
      "Loss: 5.124133e-05\n",
      "Loss: 5.118039e-05\n",
      "Loss: 5.11237e-05\n",
      "Loss: 5.1049643e-05\n",
      "Loss: 5.091427e-05\n",
      "Loss: 5.0968196e-05\n",
      "Loss: 5.0866183e-05\n",
      "Loss: 5.0759492e-05\n",
      "Loss: 5.0682705e-05\n",
      "Loss: 5.0576342e-05\n",
      "Loss: 5.0465587e-05\n",
      "Loss: 5.0353105e-05\n",
      "Loss: 5.025987e-05\n",
      "Loss: 5.016398e-05\n",
      "Loss: 5.0111135e-05\n",
      "Loss: 4.998513e-05\n",
      "Loss: 4.991419e-05\n",
      "Loss: 4.98303e-05\n",
      "Loss: 4.9734852e-05\n",
      "Loss: 4.9596027e-05\n",
      "Loss: 4.941556e-05\n",
      "Loss: 4.9575414e-05\n",
      "Loss: 4.9319056e-05\n",
      "Loss: 4.9198265e-05\n",
      "Loss: 4.9115493e-05\n",
      "Loss: 4.9053706e-05\n",
      "Loss: 4.891431e-05\n",
      "Loss: 4.9129863e-05\n",
      "Loss: 4.884609e-05\n",
      "Loss: 4.873192e-05\n",
      "Loss: 4.8636717e-05\n",
      "Loss: 4.8560192e-05\n",
      "Loss: 4.8490077e-05\n",
      "Loss: 4.836318e-05\n",
      "Loss: 4.8271515e-05\n",
      "Loss: 4.8877304e-05\n",
      "Loss: 4.8236776e-05\n",
      "Loss: 4.8156966e-05\n",
      "Loss: 4.808155e-05\n",
      "Loss: 4.8006787e-05\n",
      "Loss: 4.7976115e-05\n",
      "Loss: 4.790411e-05\n",
      "Loss: 4.7867794e-05\n",
      "Loss: 4.78274e-05\n",
      "Loss: 4.7781446e-05\n",
      "Loss: 4.776903e-05\n",
      "Loss: 4.7681307e-05\n",
      "Loss: 4.7658767e-05\n",
      "Loss: 4.7613736e-05\n",
      "Loss: 4.7549365e-05\n",
      "Loss: 4.7438312e-05\n",
      "Loss: 4.852704e-05\n",
      "Loss: 4.7420806e-05\n",
      "Loss: 4.7352787e-05\n",
      "Loss: 4.7277266e-05\n",
      "Loss: 4.7173206e-05\n",
      "Loss: 4.7198468e-05\n",
      "Loss: 4.7136462e-05\n",
      "Loss: 4.7065914e-05\n",
      "Loss: 4.7009566e-05\n",
      "Loss: 4.6950845e-05\n",
      "Loss: 4.687871e-05\n",
      "Loss: 4.676662e-05\n",
      "Loss: 4.6925423e-05\n",
      "Loss: 4.6712143e-05\n",
      "Loss: 4.661453e-05\n",
      "Loss: 4.65556e-05\n",
      "Loss: 4.6478788e-05\n",
      "Loss: 4.635646e-05\n",
      "Loss: 4.624513e-05\n",
      "Loss: 4.6172365e-05\n",
      "Loss: 4.6099267e-05\n",
      "Loss: 4.6028657e-05\n",
      "Loss: 4.5969122e-05\n",
      "Loss: 4.590523e-05\n",
      "Loss: 4.586107e-05\n",
      "Loss: 4.5815606e-05\n",
      "Loss: 4.5717938e-05\n",
      "Loss: 4.5574212e-05\n",
      "Loss: 4.557831e-05\n",
      "Loss: 4.549092e-05\n",
      "Loss: 4.5340104e-05\n",
      "Loss: 4.5248504e-05\n",
      "Loss: 4.514632e-05\n",
      "Loss: 4.5053363e-05\n",
      "Loss: 4.493063e-05\n",
      "Loss: 4.4820634e-05\n",
      "Loss: 4.469874e-05\n",
      "Loss: 4.4584507e-05\n",
      "Loss: 4.4454115e-05\n",
      "Loss: 4.435057e-05\n",
      "Loss: 4.4272645e-05\n",
      "Loss: 4.426272e-05\n",
      "Loss: 4.4209766e-05\n",
      "Loss: 4.414945e-05\n",
      "Loss: 4.409026e-05\n",
      "Loss: 4.4027896e-05\n",
      "Loss: 4.393916e-05\n",
      "Loss: 4.3886917e-05\n",
      "Loss: 4.3805747e-05\n",
      "Loss: 4.3769527e-05\n",
      "Loss: 4.3720982e-05\n",
      "Loss: 4.3626493e-05\n",
      "Loss: 4.363637e-05\n",
      "Loss: 4.3576725e-05\n",
      "Loss: 4.352587e-05\n",
      "Loss: 4.346933e-05\n",
      "Loss: 4.3407832e-05\n",
      "Loss: 4.3276592e-05\n",
      "Loss: 4.335835e-05\n",
      "Loss: 4.323924e-05\n",
      "Loss: 4.3146247e-05\n",
      "Loss: 4.3085918e-05\n",
      "Loss: 4.3013257e-05\n",
      "Loss: 4.293418e-05\n",
      "Loss: 4.2887874e-05\n",
      "Loss: 4.281498e-05\n",
      "Loss: 4.278766e-05\n",
      "Loss: 4.2747473e-05\n",
      "Loss: 4.267191e-05\n",
      "Loss: 4.2583597e-05\n",
      "Loss: 4.2796048e-05\n",
      "Loss: 4.2554297e-05\n",
      "Loss: 4.247351e-05\n",
      "Loss: 4.2417414e-05\n",
      "Loss: 4.2351727e-05\n",
      "Loss: 4.2278683e-05\n",
      "Loss: 4.2521024e-05\n",
      "Loss: 4.223979e-05\n",
      "Loss: 4.2158255e-05\n",
      "Loss: 4.2065672e-05\n",
      "Loss: 4.1912994e-05\n",
      "Loss: 4.198029e-05\n",
      "Loss: 4.1876672e-05\n",
      "Loss: 4.1803614e-05\n",
      "Loss: 4.1733532e-05\n",
      "Loss: 4.164155e-05\n",
      "Loss: 4.154245e-05\n",
      "Loss: 4.148108e-05\n",
      "Loss: 4.1342893e-05\n",
      "Loss: 4.1276962e-05\n",
      "Loss: 4.1230094e-05\n",
      "Loss: 4.1147483e-05\n",
      "Loss: 4.102553e-05\n",
      "Loss: 4.0919873e-05\n",
      "Loss: 4.0855597e-05\n",
      "Loss: 4.0787658e-05\n",
      "Loss: 4.0740808e-05\n",
      "Loss: 4.059512e-05\n",
      "Loss: 4.0860614e-05\n",
      "Loss: 4.0541083e-05\n",
      "Loss: 4.045453e-05\n",
      "Loss: 4.0381223e-05\n",
      "Loss: 4.0339175e-05\n",
      "Loss: 4.0277475e-05\n",
      "Loss: 4.021582e-05\n",
      "Loss: 4.0187682e-05\n",
      "Loss: 4.012679e-05\n",
      "Loss: 4.0102237e-05\n",
      "Loss: 4.0078812e-05\n",
      "Loss: 4.0021107e-05\n",
      "Loss: 4.0006125e-05\n",
      "Loss: 3.995135e-05\n",
      "Loss: 3.9936443e-05\n",
      "Loss: 3.9902025e-05\n",
      "Loss: 3.9839633e-05\n",
      "Loss: 3.997177e-05\n",
      "Loss: 3.9810206e-05\n",
      "Loss: 3.9753933e-05\n",
      "Loss: 3.9713457e-05\n",
      "Loss: 3.9653278e-05\n",
      "Loss: 3.9551538e-05\n",
      "Loss: 4.00713e-05\n",
      "Loss: 3.9528448e-05\n",
      "Loss: 3.94624e-05\n",
      "Loss: 3.941971e-05\n",
      "Loss: 3.9335624e-05\n",
      "Loss: 3.9205464e-05\n",
      "Loss: 3.9058054e-05\n",
      "Loss: 3.8953927e-05\n",
      "Loss: 3.89054e-05\n",
      "Loss: 3.8862836e-05\n",
      "Loss: 3.8832528e-05\n",
      "Loss: 3.8777493e-05\n",
      "Loss: 3.8697366e-05\n",
      "Loss: 3.8679267e-05\n",
      "Loss: 3.854507e-05\n",
      "Loss: 3.8501435e-05\n",
      "Loss: 3.8456586e-05\n",
      "Loss: 3.841603e-05\n",
      "Loss: 3.834814e-05\n",
      "Loss: 3.8290113e-05\n",
      "Loss: 3.8222337e-05\n",
      "Loss: 3.8165832e-05\n",
      "Loss: 3.8230755e-05\n",
      "Loss: 3.8107133e-05\n",
      "Loss: 3.8003287e-05\n",
      "Loss: 3.793852e-05\n",
      "Loss: 3.7881804e-05\n",
      "Loss: 3.781001e-05\n",
      "Loss: 3.785312e-05\n",
      "Loss: 3.776112e-05\n",
      "Loss: 3.7669397e-05\n",
      "Loss: 3.759539e-05\n",
      "Loss: 3.7538477e-05\n",
      "Loss: 3.7473543e-05\n",
      "Loss: 3.7366306e-05\n",
      "Loss: 3.748052e-05\n",
      "Loss: 3.7320355e-05\n",
      "Loss: 3.72609e-05\n",
      "Loss: 3.7220092e-05\n",
      "Loss: 3.7170168e-05\n",
      "Loss: 3.7074235e-05\n",
      "Loss: 3.7559766e-05\n",
      "Loss: 3.7049515e-05\n",
      "Loss: 3.6969956e-05\n",
      "Loss: 3.6916485e-05\n",
      "Loss: 3.6854464e-05\n",
      "Loss: 3.6786794e-05\n",
      "Loss: 3.6934827e-05\n",
      "Loss: 3.6758378e-05\n",
      "Loss: 3.6707417e-05\n",
      "Loss: 3.6672645e-05\n",
      "Loss: 3.66253e-05\n",
      "Loss: 3.6568086e-05\n",
      "Loss: 3.653873e-05\n",
      "Loss: 3.6487847e-05\n",
      "Loss: 3.6461075e-05\n",
      "Loss: 3.6434398e-05\n",
      "Loss: 3.6374648e-05\n",
      "Loss: 3.6310583e-05\n",
      "Loss: 3.6312118e-05\n",
      "Loss: 3.627346e-05\n",
      "Loss: 3.6238467e-05\n",
      "Loss: 3.620852e-05\n",
      "Loss: 3.617702e-05\n",
      "Loss: 3.6112502e-05\n",
      "Loss: 3.60404e-05\n",
      "Loss: 3.603973e-05\n",
      "Loss: 3.599171e-05\n",
      "Loss: 3.595775e-05\n",
      "Loss: 3.5924717e-05\n",
      "Loss: 3.5887788e-05\n",
      "Loss: 3.578422e-05\n",
      "Loss: 3.572512e-05\n",
      "Loss: 3.5684614e-05\n",
      "Loss: 3.5648613e-05\n",
      "Loss: 3.55835e-05\n",
      "Loss: 3.5635323e-05\n",
      "Loss: 3.5552133e-05\n",
      "Loss: 3.5494624e-05\n",
      "Loss: 3.544212e-05\n",
      "Loss: 3.540881e-05\n",
      "Loss: 3.534786e-05\n",
      "Loss: 3.5269473e-05\n",
      "Loss: 3.5310004e-05\n",
      "Loss: 3.5221805e-05\n",
      "Loss: 3.515389e-05\n",
      "Loss: 3.5107063e-05\n",
      "Loss: 3.50584e-05\n",
      "Loss: 3.4968e-05\n",
      "Loss: 3.499026e-05\n",
      "Loss: 3.4904268e-05\n",
      "Loss: 3.4837743e-05\n",
      "Loss: 3.478179e-05\n",
      "Loss: 3.470031e-05\n",
      "Loss: 3.4625322e-05\n",
      "Loss: 3.4590004e-05\n",
      "Loss: 3.4524528e-05\n",
      "Loss: 3.447762e-05\n",
      "Loss: 3.4429057e-05\n",
      "Loss: 3.4325192e-05\n",
      "Loss: 3.4719305e-05\n",
      "Loss: 3.4294993e-05\n",
      "Loss: 3.4227123e-05\n",
      "Loss: 3.4175937e-05\n",
      "Loss: 3.4111355e-05\n",
      "Loss: 3.401232e-05\n",
      "Loss: 3.4374243e-05\n",
      "Loss: 3.3988945e-05\n",
      "Loss: 3.3911594e-05\n",
      "Loss: 3.3840326e-05\n",
      "Loss: 3.3763168e-05\n",
      "Loss: 3.404747e-05\n",
      "Loss: 3.373758e-05\n",
      "Loss: 3.370511e-05\n",
      "Loss: 3.3624303e-05\n",
      "Loss: 3.35734e-05\n",
      "Loss: 3.3518354e-05\n",
      "Loss: 3.345882e-05\n",
      "Loss: 3.3408876e-05\n",
      "Loss: 3.3400524e-05\n",
      "Loss: 3.3346863e-05\n",
      "Loss: 3.332818e-05\n",
      "Loss: 3.329529e-05\n",
      "Loss: 3.3238426e-05\n",
      "Loss: 3.317168e-05\n",
      "Loss: 3.3125307e-05\n",
      "Loss: 3.306301e-05\n",
      "Loss: 3.3034907e-05\n",
      "Loss: 3.299712e-05\n",
      "Loss: 3.2960976e-05\n",
      "Loss: 3.292616e-05\n",
      "Loss: 3.28909e-05\n",
      "Loss: 3.28476e-05\n",
      "Loss: 3.277383e-05\n",
      "Loss: 3.2762386e-05\n",
      "Loss: 3.269984e-05\n",
      "Loss: 3.2682045e-05\n",
      "Loss: 3.264885e-05\n",
      "Loss: 3.259787e-05\n",
      "Loss: 3.2541036e-05\n",
      "Loss: 3.2532487e-05\n",
      "Loss: 3.24811e-05\n",
      "Loss: 3.246167e-05\n",
      "Loss: 3.2420117e-05\n",
      "Loss: 3.235795e-05\n",
      "Loss: 3.2282733e-05\n",
      "Loss: 3.220982e-05\n",
      "Loss: 3.2166696e-05\n",
      "Loss: 3.2130847e-05\n",
      "Loss: 3.2092357e-05\n",
      "Loss: 3.2200944e-05\n",
      "Loss: 3.206008e-05\n",
      "Loss: 3.1987285e-05\n",
      "Loss: 3.1964773e-05\n",
      "Loss: 3.192752e-05\n",
      "Loss: 3.1881107e-05\n",
      "Loss: 3.205768e-05\n",
      "Loss: 3.186139e-05\n",
      "Loss: 3.181566e-05\n",
      "Loss: 3.1780557e-05\n",
      "Loss: 3.1744414e-05\n",
      "Loss: 3.170359e-05\n",
      "Loss: 3.166866e-05\n",
      "Loss: 3.1620933e-05\n",
      "Loss: 3.1598494e-05\n",
      "Loss: 3.1563242e-05\n",
      "Loss: 3.1544307e-05\n",
      "Loss: 3.151466e-05\n",
      "Loss: 3.1485673e-05\n",
      "Loss: 3.1451687e-05\n",
      "Loss: 3.1395484e-05\n",
      "Loss: 3.1363954e-05\n",
      "Loss: 3.1321055e-05\n",
      "Loss: 3.1296284e-05\n",
      "Loss: 3.1347587e-05\n",
      "Loss: 3.128533e-05\n",
      "Loss: 3.1267526e-05\n",
      "Loss: 3.1220585e-05\n",
      "Loss: 3.117503e-05\n",
      "Loss: 3.111289e-05\n",
      "Loss: 3.1008734e-05\n",
      "Loss: 3.1259384e-05\n",
      "Loss: 3.098071e-05\n",
      "Loss: 3.0904957e-05\n",
      "Loss: 3.085967e-05\n",
      "Loss: 3.082598e-05\n",
      "Loss: 3.078374e-05\n",
      "Loss: 3.075622e-05\n",
      "Loss: 3.0725467e-05\n",
      "Loss: 3.066783e-05\n",
      "Loss: 3.067831e-05\n",
      "Loss: 3.063478e-05\n",
      "Loss: 3.0569558e-05\n",
      "Loss: 3.0530005e-05\n",
      "Loss: 3.0486986e-05\n",
      "Loss: 3.0440762e-05\n",
      "Loss: 3.0373827e-05\n",
      "Loss: 3.0479114e-05\n",
      "Loss: 3.0351308e-05\n",
      "Loss: 3.0302355e-05\n",
      "Loss: 3.0269002e-05\n",
      "Loss: 3.0233652e-05\n",
      "Loss: 3.018977e-05\n",
      "Loss: 3.0126612e-05\n",
      "Loss: 3.0098028e-05\n",
      "Loss: 3.0027077e-05\n",
      "Loss: 3.0001935e-05\n",
      "Loss: 2.9973748e-05\n",
      "Loss: 2.9906972e-05\n",
      "Loss: 3.0162546e-05\n",
      "Loss: 2.987992e-05\n",
      "Loss: 2.9824398e-05\n",
      "Loss: 2.9797426e-05\n",
      "Loss: 2.9758765e-05\n",
      "Loss: 2.9732415e-05\n",
      "Loss: 2.9695924e-05\n",
      "Loss: 2.967295e-05\n",
      "Loss: 2.9634539e-05\n",
      "Loss: 2.9603932e-05\n",
      "Loss: 2.955992e-05\n",
      "Loss: 2.9527357e-05\n",
      "Loss: 2.9495948e-05\n",
      "Loss: 2.9473631e-05\n",
      "Loss: 2.9452622e-05\n",
      "Loss: 2.9395673e-05\n",
      "Loss: 2.9440554e-05\n",
      "Loss: 2.9365798e-05\n",
      "Loss: 2.9334422e-05\n",
      "Loss: 2.9306293e-05\n",
      "Loss: 2.928089e-05\n",
      "Loss: 2.9219551e-05\n",
      "Loss: 2.9268205e-05\n",
      "Loss: 2.9196115e-05\n",
      "Loss: 2.9145403e-05\n",
      "Loss: 2.9109504e-05\n",
      "Loss: 2.9066938e-05\n",
      "Loss: 2.90112e-05\n",
      "Loss: 2.8952047e-05\n",
      "Loss: 2.890734e-05\n",
      "Loss: 2.8847087e-05\n",
      "Loss: 2.8816783e-05\n",
      "Loss: 2.878043e-05\n",
      "Loss: 2.874606e-05\n",
      "Loss: 2.8719212e-05\n",
      "Loss: 2.8682065e-05\n",
      "Loss: 2.8654107e-05\n",
      "Loss: 2.8630782e-05\n",
      "Loss: 2.8587869e-05\n",
      "Loss: 2.8602517e-05\n",
      "Loss: 2.8563249e-05\n",
      "Loss: 2.8523167e-05\n",
      "Loss: 2.8489769e-05\n",
      "Loss: 2.8455182e-05\n",
      "Loss: 2.8418748e-05\n",
      "Loss: 2.8376615e-05\n",
      "Loss: 2.8350518e-05\n",
      "Loss: 2.8301682e-05\n",
      "Loss: 2.8277842e-05\n",
      "Loss: 2.8231825e-05\n",
      "Loss: 2.8159659e-05\n",
      "Loss: 2.8125078e-05\n",
      "Loss: 2.8056089e-05\n",
      "Loss: 2.8031463e-05\n",
      "Loss: 2.797526e-05\n",
      "Loss: 2.7915085e-05\n",
      "Loss: 2.7891994e-05\n",
      "Loss: 2.7847072e-05\n",
      "Loss: 2.7833583e-05\n",
      "Loss: 2.7819002e-05\n",
      "Loss: 2.7795628e-05\n",
      "Loss: 2.7852144e-05\n",
      "Loss: 2.7781694e-05\n",
      "Loss: 2.775392e-05\n",
      "Loss: 2.7728529e-05\n",
      "Loss: 2.7698761e-05\n",
      "Loss: 2.7654238e-05\n",
      "Loss: 2.7796112e-05\n",
      "Loss: 2.7637885e-05\n",
      "Loss: 2.7602468e-05\n",
      "Loss: 2.7581107e-05\n",
      "Loss: 2.7556121e-05\n",
      "Loss: 2.7521262e-05\n",
      "Loss: 2.7469476e-05\n",
      "Loss: 2.7420265e-05\n",
      "Loss: 2.7386433e-05\n",
      "Loss: 2.736007e-05\n",
      "Loss: 2.732984e-05\n",
      "Loss: 2.7241696e-05\n",
      "Loss: 2.7623148e-05\n",
      "Loss: 2.7227718e-05\n",
      "Loss: 2.7185984e-05\n",
      "Loss: 2.71599e-05\n",
      "Loss: 2.7115044e-05\n",
      "Loss: 2.7062018e-05\n",
      "Loss: 2.7018239e-05\n",
      "Loss: 2.6960119e-05\n",
      "Loss: 2.6923852e-05\n",
      "Loss: 2.6880985e-05\n",
      "Loss: 2.6824491e-05\n",
      "Loss: 2.6790603e-05\n",
      "Loss: 2.676202e-05\n",
      "Loss: 2.673742e-05\n",
      "Loss: 2.6714133e-05\n",
      "Loss: 2.6652677e-05\n",
      "Loss: 2.6660673e-05\n",
      "Loss: 2.6626822e-05\n",
      "Loss: 2.6577996e-05\n",
      "Loss: 2.654269e-05\n",
      "Loss: 2.6509137e-05\n",
      "Loss: 2.6467791e-05\n",
      "Loss: 2.6403515e-05\n",
      "Loss: 2.6390266e-05\n",
      "Loss: 2.6329275e-05\n",
      "Loss: 2.631551e-05\n",
      "Loss: 2.6294276e-05\n",
      "Loss: 2.6234855e-05\n",
      "Loss: 2.634404e-05\n",
      "Loss: 2.6207419e-05\n",
      "Loss: 2.6136237e-05\n",
      "Loss: 2.6091919e-05\n",
      "Loss: 2.604513e-05\n",
      "Loss: 2.5997237e-05\n",
      "Loss: 2.6135069e-05\n",
      "Loss: 2.5979163e-05\n",
      "Loss: 2.5925212e-05\n",
      "Loss: 2.5881498e-05\n",
      "Loss: 2.5792633e-05\n",
      "Loss: 2.573493e-05\n",
      "Loss: 2.5676245e-05\n",
      "Loss: 2.5641544e-05\n",
      "Loss: 2.5614907e-05\n",
      "Loss: 2.5571788e-05\n",
      "Loss: 2.5510046e-05\n",
      "Loss: 2.5621352e-05\n",
      "Loss: 2.5487476e-05\n",
      "Loss: 2.5461452e-05\n",
      "Loss: 2.5446436e-05\n",
      "Loss: 2.5418327e-05\n",
      "Loss: 2.5366335e-05\n",
      "Loss: 2.5332925e-05\n",
      "Loss: 2.5259227e-05\n",
      "Loss: 2.5227735e-05\n",
      "Loss: 2.5197405e-05\n",
      "Loss: 2.5161826e-05\n",
      "Loss: 2.5067468e-05\n",
      "Loss: 2.5538578e-05\n",
      "Loss: 2.5046927e-05\n",
      "Loss: 2.4990117e-05\n",
      "Loss: 2.4962359e-05\n",
      "Loss: 2.4934106e-05\n",
      "Loss: 2.489164e-05\n",
      "Loss: 2.4873647e-05\n",
      "Loss: 2.4804722e-05\n",
      "Loss: 2.4782054e-05\n",
      "Loss: 2.4731611e-05\n",
      "Loss: 2.467433e-05\n",
      "Loss: 2.464413e-05\n",
      "Loss: 2.4606885e-05\n",
      "Loss: 2.4593084e-05\n",
      "Loss: 2.4570298e-05\n",
      "Loss: 2.4530558e-05\n",
      "Loss: 2.4523531e-05\n",
      "Loss: 2.4459305e-05\n",
      "Loss: 2.4435538e-05\n",
      "Loss: 2.4402161e-05\n",
      "Loss: 2.4369217e-05\n",
      "Loss: 2.4321944e-05\n",
      "Loss: 2.4331914e-05\n",
      "Loss: 2.428696e-05\n",
      "Loss: 2.4258887e-05\n",
      "Loss: 2.422878e-05\n",
      "Loss: 2.4182393e-05\n",
      "Loss: 2.4106103e-05\n",
      "Loss: 2.4041317e-05\n",
      "Loss: 2.4001809e-05\n",
      "Loss: 2.3941528e-05\n",
      "Loss: 2.3879593e-05\n",
      "Loss: 2.3856966e-05\n",
      "Loss: 2.3795754e-05\n",
      "Loss: 2.3773313e-05\n",
      "Loss: 2.375093e-05\n",
      "Loss: 2.3702127e-05\n",
      "Loss: 2.3691595e-05\n",
      "Loss: 2.3606786e-05\n",
      "Loss: 2.3566237e-05\n",
      "Loss: 2.3536924e-05\n",
      "Loss: 2.3486595e-05\n",
      "Loss: 2.3435554e-05\n",
      "Loss: 2.3379063e-05\n",
      "Loss: 2.331618e-05\n",
      "Loss: 2.328075e-05\n",
      "Loss: 2.3258708e-05\n",
      "Loss: 2.3226774e-05\n",
      "Loss: 2.3174383e-05\n",
      "Loss: 2.3162196e-05\n",
      "Loss: 2.3111597e-05\n",
      "Loss: 2.3096842e-05\n",
      "Loss: 2.307259e-05\n",
      "Loss: 2.301679e-05\n",
      "Loss: 2.3128214e-05\n",
      "Loss: 2.2998596e-05\n",
      "Loss: 2.2955297e-05\n",
      "Loss: 2.2929706e-05\n",
      "Loss: 2.2884582e-05\n",
      "Loss: 2.2840613e-05\n",
      "Loss: 2.2789554e-05\n",
      "Loss: 2.2743741e-05\n",
      "Loss: 2.2705277e-05\n",
      "Loss: 2.266109e-05\n",
      "Loss: 2.2625329e-05\n",
      "Loss: 2.2608481e-05\n",
      "Loss: 2.256034e-05\n",
      "Loss: 2.2541688e-05\n",
      "Loss: 2.2516037e-05\n",
      "Loss: 2.2471097e-05\n",
      "Loss: 2.2403647e-05\n",
      "Loss: 2.2523258e-05\n",
      "Loss: 2.2387441e-05\n",
      "Loss: 2.2358277e-05\n",
      "Loss: 2.2341399e-05\n",
      "Loss: 2.2317166e-05\n",
      "Loss: 2.2276245e-05\n",
      "Loss: 2.227437e-05\n",
      "Loss: 2.2250293e-05\n",
      "Loss: 2.2202585e-05\n",
      "Loss: 2.217213e-05\n",
      "Loss: 2.2130145e-05\n",
      "Loss: 2.2095914e-05\n",
      "Loss: 2.2075415e-05\n",
      "Loss: 2.203578e-05\n",
      "Loss: 2.2008651e-05\n",
      "Loss: 2.1963013e-05\n",
      "Loss: 2.1930973e-05\n",
      "Loss: 2.193241e-05\n",
      "Loss: 2.1912274e-05\n",
      "Loss: 2.1894493e-05\n",
      "Loss: 2.1881322e-05\n",
      "Loss: 2.18592e-05\n",
      "Loss: 2.1826738e-05\n",
      "Loss: 2.1762946e-05\n",
      "Loss: 2.1784144e-05\n",
      "Loss: 2.1729691e-05\n",
      "Loss: 2.167088e-05\n",
      "Loss: 2.163829e-05\n",
      "Loss: 2.160752e-05\n",
      "Loss: 2.1583463e-05\n",
      "Loss: 2.1556549e-05\n",
      "Loss: 2.1537895e-05\n",
      "Loss: 2.1505475e-05\n",
      "Loss: 2.146391e-05\n",
      "Loss: 2.1470682e-05\n",
      "Loss: 2.143515e-05\n",
      "Loss: 2.1406831e-05\n",
      "Loss: 2.1387465e-05\n",
      "Loss: 2.1374006e-05\n",
      "Loss: 2.135586e-05\n",
      "Loss: 2.1309708e-05\n",
      "Loss: 2.1290107e-05\n",
      "Loss: 2.1378644e-05\n",
      "Loss: 2.127534e-05\n",
      "Loss: 2.1241038e-05\n",
      "Loss: 2.1214739e-05\n",
      "Loss: 2.1170286e-05\n",
      "Loss: 2.1131229e-05\n",
      "Loss: 2.1134583e-05\n",
      "Loss: 2.1099982e-05\n",
      "Loss: 2.1068088e-05\n",
      "Loss: 2.103902e-05\n",
      "Loss: 2.1003721e-05\n",
      "Loss: 2.0958109e-05\n",
      "Loss: 2.0940204e-05\n",
      "Loss: 2.0903673e-05\n",
      "Loss: 2.0889262e-05\n",
      "Loss: 2.0870795e-05\n",
      "Loss: 2.0836966e-05\n",
      "Loss: 2.0787902e-05\n",
      "Loss: 2.0774072e-05\n",
      "Loss: 2.0732965e-05\n",
      "Loss: 2.0721462e-05\n",
      "Loss: 2.0704863e-05\n",
      "Loss: 2.0711275e-05\n",
      "Loss: 2.0690892e-05\n",
      "Loss: 2.0661799e-05\n",
      "Loss: 2.0627434e-05\n",
      "Loss: 2.0596539e-05\n",
      "Loss: 2.0576705e-05\n",
      "Loss: 2.0558175e-05\n",
      "Loss: 2.0538537e-05\n",
      "Loss: 2.0502344e-05\n",
      "Loss: 2.0460902e-05\n",
      "Loss: 2.0442381e-05\n",
      "Loss: 2.0425883e-05\n",
      "Loss: 2.0414354e-05\n",
      "Loss: 2.0404423e-05\n",
      "Loss: 2.0384747e-05\n",
      "Loss: 2.0360469e-05\n",
      "Loss: 2.036925e-05\n",
      "Loss: 2.0343763e-05\n",
      "Loss: 2.0312415e-05\n",
      "Loss: 2.0296175e-05\n",
      "Loss: 2.0272706e-05\n",
      "Loss: 2.0251926e-05\n",
      "Loss: 2.0229458e-05\n",
      "Loss: 2.0203774e-05\n",
      "Loss: 2.0192865e-05\n",
      "Loss: 2.0162215e-05\n",
      "Loss: 2.0129817e-05\n",
      "Loss: 2.010048e-05\n",
      "Loss: 2.008189e-05\n",
      "Loss: 2.0064806e-05\n",
      "Loss: 2.0053132e-05\n",
      "Loss: 2.0028467e-05\n",
      "Loss: 2.0008534e-05\n",
      "Loss: 1.9973875e-05\n",
      "Loss: 1.995482e-05\n",
      "Loss: 1.9938021e-05\n",
      "Loss: 1.9925334e-05\n",
      "Loss: 1.9910212e-05\n",
      "Loss: 1.9879648e-05\n",
      "Loss: 1.9857016e-05\n",
      "Loss: 1.9839536e-05\n",
      "Loss: 1.9824862e-05\n",
      "Loss: 1.980516e-05\n",
      "Loss: 1.97718e-05\n",
      "Loss: 1.980719e-05\n",
      "Loss: 1.9757705e-05\n",
      "Loss: 1.973748e-05\n",
      "Loss: 1.9718682e-05\n",
      "Loss: 1.969274e-05\n",
      "Loss: 1.9655648e-05\n",
      "Loss: 1.9612446e-05\n",
      "Loss: 1.9562765e-05\n",
      "Loss: 1.9630139e-05\n",
      "Loss: 1.9552645e-05\n",
      "Loss: 1.9532032e-05\n",
      "Loss: 1.950622e-05\n",
      "Loss: 1.947564e-05\n",
      "Loss: 1.9467974e-05\n",
      "Loss: 1.9442465e-05\n",
      "Loss: 1.9429817e-05\n",
      "Loss: 1.9413827e-05\n",
      "Loss: 1.9396277e-05\n",
      "Loss: 1.9390622e-05\n",
      "Loss: 1.9357645e-05\n",
      "Loss: 1.9345298e-05\n",
      "Loss: 1.9324854e-05\n",
      "Loss: 1.9348285e-05\n",
      "Loss: 1.9315381e-05\n",
      "Loss: 1.9302945e-05\n",
      "Loss: 1.9283289e-05\n",
      "Loss: 1.9267456e-05\n",
      "Loss: 1.9246325e-05\n",
      "Loss: 1.9216493e-05\n",
      "Loss: 1.9201572e-05\n",
      "Loss: 1.9170138e-05\n",
      "Loss: 1.9155588e-05\n",
      "Loss: 1.9138142e-05\n",
      "Loss: 1.9125306e-05\n",
      "Loss: 1.9103894e-05\n",
      "Loss: 1.9088595e-05\n",
      "Loss: 1.9074163e-05\n",
      "Loss: 1.9055944e-05\n",
      "Loss: 1.9030955e-05\n",
      "Loss: 1.9016796e-05\n",
      "Loss: 1.8989916e-05\n",
      "Loss: 1.8976601e-05\n",
      "Loss: 1.8963065e-05\n",
      "Loss: 1.8928215e-05\n",
      "Loss: 1.89296e-05\n",
      "Loss: 1.8907169e-05\n",
      "Loss: 1.8879826e-05\n",
      "Loss: 1.8864917e-05\n",
      "Loss: 1.8854626e-05\n",
      "Loss: 1.8837214e-05\n",
      "Loss: 1.8807612e-05\n",
      "Loss: 1.8774641e-05\n",
      "Loss: 1.8733863e-05\n",
      "Loss: 1.8710816e-05\n",
      "Loss: 1.8674356e-05\n",
      "Loss: 1.8758223e-05\n",
      "Loss: 1.8662695e-05\n",
      "Loss: 1.8636081e-05\n",
      "Loss: 1.8604376e-05\n",
      "Loss: 1.8580447e-05\n",
      "Loss: 1.8563773e-05\n",
      "Loss: 1.853798e-05\n",
      "Loss: 1.8512845e-05\n",
      "Loss: 1.8478802e-05\n",
      "Loss: 1.8452156e-05\n",
      "Loss: 1.8552708e-05\n",
      "Loss: 1.8441719e-05\n",
      "Loss: 1.8413224e-05\n",
      "Loss: 1.8397724e-05\n",
      "Loss: 1.83749e-05\n",
      "Loss: 1.8352643e-05\n",
      "Loss: 1.8349405e-05\n",
      "Loss: 1.833601e-05\n",
      "Loss: 1.8318371e-05\n",
      "Loss: 1.8296329e-05\n",
      "Loss: 1.8275492e-05\n",
      "Loss: 1.8349712e-05\n",
      "Loss: 1.8267638e-05\n",
      "Loss: 1.8248778e-05\n",
      "Loss: 1.8225022e-05\n",
      "Loss: 1.8287039e-05\n",
      "Loss: 1.8218361e-05\n",
      "Loss: 1.820547e-05\n",
      "Loss: 1.819353e-05\n",
      "Loss: 1.8175644e-05\n",
      "Loss: 1.8155028e-05\n",
      "Loss: 1.8128467e-05\n",
      "Loss: 1.810527e-05\n",
      "Loss: 1.8084986e-05\n",
      "Loss: 1.806438e-05\n",
      "Loss: 1.8043367e-05\n",
      "Loss: 1.8030187e-05\n",
      "Loss: 1.800209e-05\n",
      "Loss: 1.800554e-05\n",
      "Loss: 1.7987983e-05\n",
      "Loss: 1.7974198e-05\n",
      "Loss: 1.7946724e-05\n",
      "Loss: 1.7932758e-05\n",
      "Loss: 1.7921942e-05\n",
      "Loss: 1.789038e-05\n",
      "Loss: 1.7880524e-05\n",
      "Loss: 1.7868711e-05\n",
      "Loss: 1.7854105e-05\n",
      "Loss: 1.7814436e-05\n",
      "Loss: 1.7838329e-05\n",
      "Loss: 1.7795304e-05\n",
      "Loss: 1.7774422e-05\n",
      "Loss: 1.7755308e-05\n",
      "Loss: 1.7734015e-05\n",
      "Loss: 1.7688999e-05\n",
      "Loss: 1.8277753e-05\n",
      "Loss: 1.7676142e-05\n",
      "Loss: 1.7649421e-05\n",
      "Loss: 1.7633352e-05\n",
      "Loss: 1.761748e-05\n",
      "Loss: 1.7601453e-05\n",
      "Loss: 1.7571436e-05\n",
      "Loss: 1.7552145e-05\n",
      "Loss: 1.7530678e-05\n",
      "Loss: 1.7487404e-05\n",
      "Loss: 1.7458886e-05\n",
      "Loss: 1.7443086e-05\n",
      "Loss: 1.743004e-05\n",
      "Loss: 1.7401771e-05\n",
      "Loss: 1.7355484e-05\n",
      "Loss: 1.7371713e-05\n",
      "Loss: 1.7334602e-05\n",
      "Loss: 1.731945e-05\n",
      "Loss: 1.730089e-05\n",
      "Loss: 1.7279759e-05\n",
      "Loss: 1.7426453e-05\n",
      "Loss: 1.727007e-05\n",
      "Loss: 1.723954e-05\n",
      "Loss: 1.7217033e-05\n",
      "Loss: 1.7181783e-05\n",
      "Loss: 1.71795e-05\n",
      "Loss: 1.714835e-05\n",
      "Loss: 1.7136386e-05\n",
      "Loss: 1.7122817e-05\n",
      "Loss: 1.7117196e-05\n",
      "Loss: 1.7105871e-05\n",
      "Loss: 1.7093906e-05\n",
      "Loss: 1.708473e-05\n",
      "Loss: 1.706789e-05\n",
      "Loss: 1.702112e-05\n",
      "Loss: 1.7232964e-05\n",
      "Loss: 1.7007837e-05\n",
      "Loss: 1.6977883e-05\n",
      "Loss: 1.6955932e-05\n",
      "Loss: 1.6930677e-05\n",
      "Loss: 1.6903216e-05\n",
      "Loss: 1.6894235e-05\n",
      "Loss: 1.6858954e-05\n",
      "Loss: 1.6846823e-05\n",
      "Loss: 1.6827675e-05\n",
      "Loss: 1.6797449e-05\n",
      "Loss: 1.6896185e-05\n",
      "Loss: 1.6784757e-05\n",
      "Loss: 1.6765589e-05\n",
      "Loss: 1.675308e-05\n",
      "Loss: 1.6734492e-05\n",
      "Loss: 1.6701086e-05\n",
      "Loss: 1.664838e-05\n",
      "Loss: 1.6956044e-05\n",
      "Loss: 1.6639782e-05\n",
      "Loss: 1.660538e-05\n",
      "Loss: 1.6581087e-05\n",
      "Loss: 1.6536993e-05\n",
      "Loss: 1.650244e-05\n",
      "Loss: 1.6505803e-05\n",
      "Loss: 1.6478336e-05\n",
      "Loss: 1.6458906e-05\n",
      "Loss: 1.64433e-05\n",
      "Loss: 1.6421218e-05\n",
      "Loss: 1.6549777e-05\n",
      "Loss: 1.6414597e-05\n",
      "Loss: 1.6393731e-05\n",
      "Loss: 1.6378555e-05\n",
      "Loss: 1.635926e-05\n",
      "Loss: 1.6337563e-05\n",
      "Loss: 1.6345388e-05\n",
      "Loss: 1.6321434e-05\n",
      "Loss: 1.630315e-05\n",
      "Loss: 1.628929e-05\n",
      "Loss: 1.627607e-05\n",
      "Loss: 1.624809e-05\n",
      "Loss: 1.6253613e-05\n",
      "Loss: 1.6232963e-05\n",
      "Loss: 1.6220649e-05\n",
      "Loss: 1.6208698e-05\n",
      "Loss: 1.6197051e-05\n",
      "Loss: 1.6161199e-05\n",
      "Loss: 1.6171905e-05\n",
      "Loss: 1.6144793e-05\n",
      "Loss: 1.6108883e-05\n",
      "Loss: 1.6080905e-05\n",
      "Loss: 1.605901e-05\n",
      "Loss: 1.6042108e-05\n",
      "Loss: 1.6023958e-05\n",
      "Loss: 1.600572e-05\n",
      "Loss: 1.5973244e-05\n",
      "Loss: 1.595243e-05\n",
      "Loss: 1.5939117e-05\n",
      "Loss: 1.5930662e-05\n",
      "Loss: 1.5916077e-05\n",
      "Loss: 1.590465e-05\n",
      "Loss: 1.5885757e-05\n",
      "Loss: 1.5860789e-05\n",
      "Loss: 1.6059923e-05\n",
      "Loss: 1.585277e-05\n",
      "Loss: 1.5829402e-05\n",
      "Loss: 1.5817399e-05\n",
      "Loss: 1.5803533e-05\n",
      "Loss: 1.5793947e-05\n",
      "Loss: 1.5781774e-05\n",
      "Loss: 1.5762036e-05\n",
      "Loss: 1.5752623e-05\n",
      "Loss: 1.5722806e-05\n",
      "Loss: 1.5747193e-05\n",
      "Loss: 1.5705686e-05\n",
      "Loss: 1.5685804e-05\n",
      "Loss: 1.5662219e-05\n",
      "Loss: 1.5641197e-05\n",
      "Loss: 1.5674417e-05\n",
      "Loss: 1.5629816e-05\n",
      "Loss: 1.560619e-05\n",
      "Loss: 1.5587808e-05\n",
      "Loss: 1.5556569e-05\n",
      "Loss: 1.5524212e-05\n",
      "Loss: 1.5593478e-05\n",
      "Loss: 1.5513873e-05\n",
      "Loss: 1.5490159e-05\n",
      "Loss: 1.547161e-05\n",
      "Loss: 1.543788e-05\n",
      "Loss: 1.5417347e-05\n",
      "Loss: 1.5389847e-05\n",
      "Loss: 1.5374815e-05\n",
      "Loss: 1.5354235e-05\n",
      "Loss: 1.5326317e-05\n",
      "Loss: 1.538805e-05\n",
      "Loss: 1.5313823e-05\n",
      "Loss: 1.5294452e-05\n",
      "Loss: 1.5282885e-05\n",
      "Loss: 1.5266018e-05\n",
      "Loss: 1.523691e-05\n",
      "Loss: 1.5326108e-05\n",
      "Loss: 1.5226957e-05\n",
      "Loss: 1.5206585e-05\n",
      "Loss: 1.5188891e-05\n",
      "Loss: 1.5165487e-05\n",
      "Loss: 1.5127262e-05\n",
      "Loss: 1.51794375e-05\n",
      "Loss: 1.5107491e-05\n",
      "Loss: 1.5059895e-05\n",
      "Loss: 1.5024894e-05\n",
      "Loss: 1.5000358e-05\n",
      "Loss: 1.4985529e-05\n",
      "Loss: 1.4968067e-05\n",
      "Loss: 1.4929685e-05\n",
      "Loss: 1.4915384e-05\n",
      "Loss: 1.4966021e-05\n",
      "Loss: 1.49108155e-05\n",
      "Loss: 1.4901256e-05\n",
      "Loss: 1.4880464e-05\n",
      "Loss: 1.4870668e-05\n",
      "Loss: 1.4848086e-05\n",
      "Loss: 1.4815395e-05\n",
      "Loss: 1.4783207e-05\n",
      "Loss: 1.4766865e-05\n",
      "Loss: 1.4756848e-05\n",
      "Loss: 1.4743933e-05\n",
      "Loss: 1.4726199e-05\n",
      "Loss: 1.47000155e-05\n",
      "Loss: 1.4745996e-05\n",
      "Loss: 1.4688894e-05\n",
      "Loss: 1.4662557e-05\n",
      "Loss: 1.4640043e-05\n",
      "Loss: 1.4618236e-05\n",
      "Loss: 1.4601566e-05\n",
      "Loss: 1.45822205e-05\n",
      "Loss: 1.4565557e-05\n",
      "Loss: 1.4546114e-05\n",
      "Loss: 1.45533395e-05\n",
      "Loss: 1.4540972e-05\n",
      "Loss: 1.4532566e-05\n",
      "Loss: 1.4517685e-05\n",
      "Loss: 1.4499441e-05\n",
      "Loss: 1.4478946e-05\n",
      "Loss: 1.4440389e-05\n",
      "Loss: 1.4459626e-05\n",
      "Loss: 1.4417963e-05\n",
      "Loss: 1.4393156e-05\n",
      "Loss: 1.4381545e-05\n",
      "Loss: 1.4367953e-05\n",
      "Loss: 1.4365111e-05\n",
      "Loss: 1.4334804e-05\n",
      "Loss: 1.4325878e-05\n",
      "Loss: 1.4315074e-05\n",
      "Loss: 1.4297233e-05\n",
      "Loss: 1.4281124e-05\n",
      "Loss: 1.4252156e-05\n",
      "Loss: 1.42448425e-05\n",
      "Loss: 1.4233492e-05\n",
      "Loss: 1.42189e-05\n",
      "Loss: 1.420572e-05\n",
      "Loss: 1.4189412e-05\n",
      "Loss: 1.4179571e-05\n",
      "Loss: 1.41455885e-05\n",
      "Loss: 1.4124881e-05\n",
      "Loss: 1.410195e-05\n",
      "Loss: 1.4082867e-05\n",
      "Loss: 1.40666425e-05\n",
      "Loss: 1.4032046e-05\n",
      "Loss: 1.4158331e-05\n",
      "Loss: 1.4020239e-05\n",
      "Loss: 1.3988157e-05\n",
      "Loss: 1.3977794e-05\n",
      "Loss: 1.3960684e-05\n",
      "Loss: 1.3950483e-05\n",
      "Loss: 1.3938037e-05\n",
      "Loss: 1.3919758e-05\n",
      "Loss: 1.3898951e-05\n",
      "Loss: 1.3883735e-05\n",
      "Loss: 1.3864803e-05\n",
      "Loss: 1.3855005e-05\n",
      "Loss: 1.3841445e-05\n",
      "Loss: 1.3823905e-05\n",
      "Loss: 1.381065e-05\n",
      "Loss: 1.3797509e-05\n",
      "Loss: 1.3780373e-05\n",
      "Loss: 1.3765737e-05\n",
      "Loss: 1.37540665e-05\n",
      "Loss: 1.3745592e-05\n",
      "Loss: 1.3725271e-05\n",
      "Loss: 1.37025745e-05\n",
      "Loss: 1.3718885e-05\n",
      "Loss: 1.3694595e-05\n",
      "Loss: 1.3677753e-05\n",
      "Loss: 1.3664014e-05\n",
      "Loss: 1.365193e-05\n",
      "Loss: 1.3641649e-05\n",
      "Loss: 1.3623087e-05\n",
      "Loss: 1.3609333e-05\n",
      "Loss: 1.3580238e-05\n",
      "Loss: 1.3646979e-05\n",
      "Loss: 1.35702285e-05\n",
      "Loss: 1.355317e-05\n",
      "Loss: 1.35384835e-05\n",
      "Loss: 1.3523591e-05\n",
      "Loss: 1.35075115e-05\n",
      "Loss: 1.3521689e-05\n",
      "Loss: 1.3501967e-05\n",
      "Loss: 1.3492923e-05\n",
      "Loss: 1.3483841e-05\n",
      "Loss: 1.3470467e-05\n",
      "Loss: 1.3450543e-05\n",
      "Loss: 1.3453331e-05\n",
      "Loss: 1.3435735e-05\n",
      "Loss: 1.34236025e-05\n",
      "Loss: 1.341047e-05\n",
      "Loss: 1.3398769e-05\n",
      "Loss: 1.3375737e-05\n",
      "Loss: 1.3376173e-05\n",
      "Loss: 1.3362097e-05\n",
      "Loss: 1.3351284e-05\n",
      "Loss: 1.3336785e-05\n",
      "Loss: 1.3321069e-05\n",
      "Loss: 1.3292068e-05\n",
      "Loss: 1.3296321e-05\n",
      "Loss: 1.3280491e-05\n",
      "Loss: 1.326613e-05\n",
      "Loss: 1.32439345e-05\n",
      "Loss: 1.3225879e-05\n",
      "Loss: 1.3232677e-05\n",
      "Loss: 1.3215858e-05\n",
      "Loss: 1.3199076e-05\n",
      "Loss: 1.3184366e-05\n",
      "Loss: 1.3160384e-05\n",
      "Loss: 1.3139199e-05\n",
      "Loss: 1.312524e-05\n",
      "Loss: 1.3102698e-05\n",
      "Loss: 1.3107363e-05\n",
      "Loss: 1.30916615e-05\n",
      "Loss: 1.3074921e-05\n",
      "Loss: 1.3059349e-05\n",
      "Loss: 1.3045159e-05\n",
      "Loss: 1.30291955e-05\n",
      "Loss: 1.3018198e-05\n",
      "Loss: 1.3001489e-05\n",
      "Loss: 1.2985612e-05\n",
      "Loss: 1.2986711e-05\n",
      "Loss: 1.2970526e-05\n",
      "Loss: 1.2948926e-05\n",
      "Loss: 1.294271e-05\n",
      "Loss: 1.2919492e-05\n",
      "Loss: 1.2896939e-05\n",
      "Loss: 1.2885739e-05\n",
      "Loss: 1.286866e-05\n",
      "Loss: 1.2861084e-05\n",
      "Loss: 1.2853304e-05\n",
      "Loss: 1.2843619e-05\n",
      "Loss: 1.2826497e-05\n",
      "Loss: 1.2808981e-05\n",
      "Loss: 1.279754e-05\n",
      "Loss: 1.2780084e-05\n",
      "Loss: 1.275742e-05\n",
      "Loss: 1.2740546e-05\n",
      "Loss: 1.2723276e-05\n",
      "Loss: 1.2716359e-05\n",
      "Loss: 1.2708109e-05\n",
      "Loss: 1.2696124e-05\n",
      "Loss: 1.268081e-05\n",
      "Loss: 1.2664421e-05\n",
      "Loss: 1.2649396e-05\n",
      "Loss: 1.263344e-05\n",
      "Loss: 1.262086e-05\n",
      "Loss: 1.2603037e-05\n",
      "Loss: 1.2590225e-05\n",
      "Loss: 1.2600387e-05\n",
      "Loss: 1.2585051e-05\n",
      "Loss: 1.2577895e-05\n",
      "Loss: 1.2563916e-05\n",
      "Loss: 1.2550624e-05\n",
      "Loss: 1.2525645e-05\n",
      "Loss: 1.2488506e-05\n",
      "Loss: 1.2467434e-05\n",
      "Loss: 1.2439831e-05\n",
      "Loss: 1.2431612e-05\n",
      "Loss: 1.24163535e-05\n",
      "Loss: 1.2415068e-05\n",
      "Loss: 1.24075505e-05\n",
      "Loss: 1.2397049e-05\n",
      "Loss: 1.2388844e-05\n",
      "Loss: 1.2375631e-05\n",
      "Loss: 1.2357071e-05\n",
      "Loss: 1.2342747e-05\n",
      "Loss: 1.2328275e-05\n",
      "Loss: 1.2313213e-05\n",
      "Loss: 1.2298675e-05\n",
      "Loss: 1.2283794e-05\n",
      "Loss: 1.2273304e-05\n",
      "Loss: 1.2256685e-05\n",
      "Loss: 1.2249078e-05\n",
      "Loss: 1.2232495e-05\n",
      "Loss: 1.2212326e-05\n",
      "Loss: 1.2291719e-05\n",
      "Loss: 1.2206432e-05\n",
      "Loss: 1.2191975e-05\n",
      "Loss: 1.2183347e-05\n",
      "Loss: 1.2164299e-05\n",
      "Loss: 1.2146298e-05\n",
      "Loss: 1.2132823e-05\n",
      "Loss: 1.2109436e-05\n",
      "Loss: 1.2101203e-05\n",
      "Loss: 1.2086739e-05\n",
      "Loss: 1.2086519e-05\n",
      "Loss: 1.2076645e-05\n",
      "Loss: 1.20610885e-05\n",
      "Loss: 1.20470595e-05\n",
      "Loss: 1.2034745e-05\n",
      "Loss: 1.2017181e-05\n",
      "Loss: 1.2007676e-05\n",
      "Loss: 1.1986849e-05\n",
      "Loss: 1.1974882e-05\n",
      "Loss: 1.1960094e-05\n",
      "Loss: 1.194363e-05\n",
      "Loss: 1.192373e-05\n",
      "Loss: 1.19083925e-05\n",
      "Loss: 1.1890106e-05\n",
      "Loss: 1.1875787e-05\n",
      "Loss: 1.1862191e-05\n",
      "Loss: 1.18521e-05\n",
      "Loss: 1.1847153e-05\n",
      "Loss: 1.1834598e-05\n",
      "Loss: 1.183723e-05\n",
      "Loss: 1.1827636e-05\n",
      "Loss: 1.181443e-05\n",
      "Loss: 1.18068165e-05\n",
      "Loss: 1.17987e-05\n",
      "Loss: 1.1786425e-05\n",
      "Loss: 1.1764626e-05\n",
      "Loss: 1.17710015e-05\n",
      "Loss: 1.1754837e-05\n",
      "Loss: 1.1741539e-05\n",
      "Loss: 1.17331165e-05\n",
      "Loss: 1.1722962e-05\n",
      "Loss: 1.1713279e-05\n",
      "Loss: 1.1702829e-05\n",
      "Loss: 1.1695258e-05\n",
      "Loss: 1.1684668e-05\n",
      "Loss: 1.1676693e-05\n",
      "Loss: 1.1661736e-05\n",
      "Loss: 1.1648488e-05\n",
      "Loss: 1.16403335e-05\n",
      "Loss: 1.1622998e-05\n",
      "Loss: 1.1611419e-05\n",
      "Loss: 1.1602063e-05\n",
      "Loss: 1.1592638e-05\n",
      "Loss: 1.1583108e-05\n",
      "Loss: 1.156694e-05\n",
      "Loss: 1.1550288e-05\n",
      "Loss: 1.1562022e-05\n",
      "Loss: 1.1543233e-05\n",
      "Loss: 1.1534859e-05\n",
      "Loss: 1.1528686e-05\n",
      "Loss: 1.1515291e-05\n",
      "Loss: 1.149183e-05\n",
      "Loss: 1.1554486e-05\n",
      "Loss: 1.14862105e-05\n",
      "Loss: 1.147036e-05\n",
      "Loss: 1.1459848e-05\n",
      "Loss: 1.1445669e-05\n",
      "Loss: 1.1429645e-05\n",
      "Loss: 1.1424635e-05\n",
      "Loss: 1.14085215e-05\n",
      "Loss: 1.1403043e-05\n",
      "Loss: 1.1395503e-05\n",
      "Loss: 1.1379107e-05\n",
      "Loss: 1.141824e-05\n",
      "Loss: 1.1371871e-05\n",
      "Loss: 1.1356382e-05\n",
      "Loss: 1.1348584e-05\n",
      "Loss: 1.1342103e-05\n",
      "Loss: 1.133317e-05\n",
      "Loss: 1.1323258e-05\n",
      "Loss: 1.1307517e-05\n",
      "Loss: 1.1289389e-05\n",
      "Loss: 1.12601265e-05\n",
      "Loss: 1.1251995e-05\n",
      "Loss: 1.123708e-05\n",
      "Loss: 1.1234311e-05\n",
      "Loss: 1.12272055e-05\n",
      "Loss: 1.1230619e-05\n",
      "Loss: 1.1222003e-05\n",
      "Loss: 1.1213222e-05\n",
      "Loss: 1.1204187e-05\n",
      "Loss: 1.1192536e-05\n",
      "Loss: 1.1175729e-05\n",
      "Loss: 1.1171392e-05\n",
      "Loss: 1.1151363e-05\n",
      "Loss: 1.1144664e-05\n",
      "Loss: 1.1134454e-05\n",
      "Loss: 1.1121794e-05\n",
      "Loss: 1.1111445e-05\n",
      "Loss: 1.11031895e-05\n",
      "Loss: 1.1095637e-05\n",
      "Loss: 1.1088325e-05\n",
      "Loss: 1.1068472e-05\n",
      "Loss: 1.1078331e-05\n",
      "Loss: 1.10604815e-05\n",
      "Loss: 1.1052857e-05\n",
      "Loss: 1.1045651e-05\n",
      "Loss: 1.1036757e-05\n",
      "Loss: 1.10248875e-05\n",
      "Loss: 1.1012853e-05\n",
      "Loss: 1.1003593e-05\n",
      "Loss: 1.0995374e-05\n",
      "Loss: 1.0988877e-05\n",
      "Loss: 1.0973319e-05\n",
      "Loss: 1.0972167e-05\n",
      "Loss: 1.0964089e-05\n",
      "Loss: 1.0956852e-05\n",
      "Loss: 1.0946251e-05\n",
      "Loss: 1.0935852e-05\n",
      "Loss: 1.0961964e-05\n",
      "Loss: 1.0931134e-05\n",
      "Loss: 1.091839e-05\n",
      "Loss: 1.0910067e-05\n",
      "Loss: 1.0899999e-05\n",
      "Loss: 1.08894965e-05\n",
      "Loss: 1.0871219e-05\n",
      "Loss: 1.0948972e-05\n",
      "Loss: 1.0867451e-05\n",
      "Loss: 1.0858092e-05\n",
      "Loss: 1.0849482e-05\n",
      "Loss: 1.0834393e-05\n",
      "Loss: 1.0825006e-05\n",
      "Loss: 1.0816462e-05\n",
      "Loss: 1.0806747e-05\n",
      "Loss: 1.0798657e-05\n",
      "Loss: 1.07866645e-05\n",
      "Loss: 1.0769243e-05\n",
      "Loss: 1.07534925e-05\n",
      "Loss: 1.0743124e-05\n",
      "Loss: 1.0733011e-05\n",
      "Loss: 1.0777503e-05\n",
      "Loss: 1.0728079e-05\n",
      "Loss: 1.0713755e-05\n",
      "Loss: 1.0706435e-05\n",
      "Loss: 1.0696887e-05\n",
      "Loss: 1.0686864e-05\n",
      "Loss: 1.0677069e-05\n",
      "Loss: 1.0668848e-05\n",
      "Loss: 1.0662669e-05\n",
      "Loss: 1.0655901e-05\n",
      "Loss: 1.0639518e-05\n",
      "Loss: 1.0682277e-05\n",
      "Loss: 1.063501e-05\n",
      "Loss: 1.0626105e-05\n",
      "Loss: 1.0619293e-05\n",
      "Loss: 1.0611448e-05\n",
      "Loss: 1.0603787e-05\n",
      "Loss: 1.0592377e-05\n",
      "Loss: 1.0587858e-05\n",
      "Loss: 1.0579455e-05\n",
      "Loss: 1.0569178e-05\n",
      "Loss: 1.0560223e-05\n",
      "Loss: 1.054472e-05\n",
      "Loss: 1.0537074e-05\n",
      "Loss: 1.05225745e-05\n",
      "Loss: 1.050219e-05\n",
      "Loss: 1.0510426e-05\n",
      "Loss: 1.0491078e-05\n",
      "Loss: 1.0479267e-05\n",
      "Loss: 1.0469016e-05\n",
      "Loss: 1.0457094e-05\n",
      "Loss: 1.0500709e-05\n",
      "Loss: 1.0452852e-05\n",
      "Loss: 1.0439899e-05\n",
      "Loss: 1.04313895e-05\n",
      "Loss: 1.0420501e-05\n",
      "Loss: 1.0410304e-05\n",
      "Loss: 1.0410204e-05\n",
      "Loss: 1.0402735e-05\n",
      "Loss: 1.0389622e-05\n",
      "Loss: 1.03818675e-05\n",
      "Loss: 1.0366738e-05\n",
      "Loss: 1.0352143e-05\n",
      "Loss: 1.0349011e-05\n",
      "Loss: 1.0332473e-05\n",
      "Loss: 1.0326266e-05\n",
      "Loss: 1.0320364e-05\n",
      "Loss: 1.0307819e-05\n",
      "Loss: 1.0312001e-05\n",
      "Loss: 1.0299531e-05\n",
      "Loss: 1.0289696e-05\n",
      "Loss: 1.028348e-05\n",
      "Loss: 1.0273021e-05\n",
      "Loss: 1.0262784e-05\n",
      "Loss: 1.0256146e-05\n",
      "Loss: 1.0250291e-05\n",
      "Loss: 1.023856e-05\n",
      "Loss: 1.0221544e-05\n",
      "Loss: 1.02277445e-05\n",
      "Loss: 1.021312e-05\n",
      "Loss: 1.0206467e-05\n",
      "Loss: 1.0200827e-05\n",
      "Loss: 1.01940805e-05\n",
      "Loss: 1.01806545e-05\n",
      "Loss: 1.0384613e-05\n",
      "Loss: 1.0177628e-05\n",
      "Loss: 1.016866e-05\n",
      "Loss: 1.0155336e-05\n",
      "Loss: 1.0141113e-05\n",
      "Loss: 1.0125782e-05\n",
      "Loss: 1.010764e-05\n",
      "Loss: 1.0101129e-05\n",
      "Loss: 1.0085279e-05\n",
      "Loss: 1.007973e-05\n",
      "Loss: 1.0071382e-05\n",
      "Loss: 1.00605685e-05\n",
      "Loss: 1.0052281e-05\n",
      "Loss: 1.0042172e-05\n",
      "Loss: 1.003278e-05\n",
      "Loss: 1.0014456e-05\n",
      "Loss: 1.0117854e-05\n",
      "Loss: 1.0012003e-05\n",
      "Loss: 1.0005634e-05\n",
      "Loss: 9.995333e-06\n",
      "Loss: 9.977162e-06\n",
      "Loss: 1.0092654e-05\n",
      "Loss: 9.973978e-06\n",
      "Loss: 9.9671615e-06\n",
      "Loss: 9.959918e-06\n",
      "Loss: 9.951116e-06\n",
      "Loss: 9.9585795e-06\n",
      "Loss: 9.947777e-06\n",
      "Loss: 9.94056e-06\n",
      "Loss: 9.9336885e-06\n",
      "Loss: 9.925412e-06\n",
      "Loss: 9.915926e-06\n",
      "Loss: 9.9005165e-06\n",
      "Loss: 9.964872e-06\n",
      "Loss: 9.895639e-06\n",
      "Loss: 9.882517e-06\n",
      "Loss: 9.877057e-06\n",
      "Loss: 9.867047e-06\n",
      "Loss: 9.854824e-06\n",
      "Loss: 9.914413e-06\n",
      "Loss: 9.850902e-06\n",
      "Loss: 9.841468e-06\n",
      "Loss: 9.832618e-06\n",
      "Loss: 9.826501e-06\n",
      "Loss: 9.817647e-06\n",
      "Loss: 9.8075825e-06\n",
      "Loss: 9.800142e-06\n",
      "Loss: 9.793759e-06\n",
      "Loss: 9.789061e-06\n",
      "Loss: 9.783513e-06\n",
      "Loss: 9.778119e-06\n",
      "Loss: 9.774501e-06\n",
      "Loss: 9.763946e-06\n",
      "Loss: 9.762055e-06\n",
      "Loss: 9.752579e-06\n",
      "Loss: 9.749532e-06\n",
      "Loss: 9.745315e-06\n",
      "Loss: 9.73563e-06\n",
      "Loss: 9.757448e-06\n",
      "Loss: 9.731562e-06\n",
      "Loss: 9.7215125e-06\n",
      "Loss: 9.714116e-06\n",
      "Loss: 9.704221e-06\n",
      "Loss: 9.695525e-06\n",
      "Loss: 9.676727e-06\n",
      "Loss: 9.679705e-06\n",
      "Loss: 9.665604e-06\n",
      "Loss: 9.65279e-06\n",
      "Loss: 9.645675e-06\n",
      "Loss: 9.664596e-06\n",
      "Loss: 9.643074e-06\n",
      "Loss: 9.637831e-06\n",
      "Loss: 9.622338e-06\n",
      "Loss: 9.6106005e-06\n",
      "Loss: 9.601494e-06\n",
      "Loss: 9.593157e-06\n",
      "Loss: 9.582795e-06\n",
      "Loss: 9.576148e-06\n",
      "Loss: 9.567022e-06\n",
      "Loss: 9.552306e-06\n",
      "Loss: 9.588812e-06\n",
      "Loss: 9.5463365e-06\n",
      "Loss: 9.535534e-06\n",
      "Loss: 9.528657e-06\n",
      "Loss: 9.518262e-06\n",
      "Loss: 9.515175e-06\n",
      "Loss: 9.49916e-06\n",
      "Loss: 9.492411e-06\n",
      "Loss: 9.484653e-06\n",
      "Loss: 9.47055e-06\n",
      "Loss: 9.453274e-06\n",
      "Loss: 9.448755e-06\n",
      "Loss: 9.441757e-06\n",
      "Loss: 9.4389325e-06\n",
      "Loss: 9.4328725e-06\n",
      "Loss: 9.42251e-06\n",
      "Loss: 9.4015995e-06\n",
      "Loss: 9.55613e-06\n",
      "Loss: 9.397725e-06\n",
      "Loss: 9.384865e-06\n",
      "Loss: 9.373107e-06\n",
      "Loss: 9.357354e-06\n",
      "Loss: 9.359579e-06\n",
      "Loss: 9.350865e-06\n",
      "Loss: 9.341204e-06\n",
      "Loss: 9.3321505e-06\n",
      "Loss: 9.329852e-06\n",
      "Loss: 9.3164335e-06\n",
      "Loss: 9.310839e-06\n",
      "Loss: 9.303632e-06\n",
      "Loss: 9.295736e-06\n",
      "Loss: 9.282027e-06\n",
      "Loss: 9.295452e-06\n",
      "Loss: 9.275146e-06\n",
      "Loss: 9.269099e-06\n",
      "Loss: 9.263467e-06\n",
      "Loss: 9.257297e-06\n",
      "Loss: 9.256963e-06\n",
      "Loss: 9.250447e-06\n",
      "Loss: 9.235986e-06\n",
      "Loss: 9.227006e-06\n",
      "Loss: 9.218339e-06\n",
      "Loss: 9.211243e-06\n",
      "Loss: 9.193673e-06\n",
      "Loss: 9.247258e-06\n",
      "Loss: 9.188674e-06\n",
      "Loss: 9.17589e-06\n",
      "Loss: 9.1675865e-06\n",
      "Loss: 9.161116e-06\n",
      "Loss: 9.151914e-06\n",
      "Loss: 9.146548e-06\n",
      "Loss: 9.1405145e-06\n",
      "Loss: 9.131752e-06\n",
      "Loss: 9.116262e-06\n",
      "Loss: 9.12425e-06\n",
      "Loss: 9.10854e-06\n",
      "Loss: 9.094409e-06\n",
      "Loss: 9.100092e-06\n",
      "Loss: 9.086954e-06\n",
      "Loss: 9.077581e-06\n",
      "Loss: 9.072775e-06\n",
      "Loss: 9.066536e-06\n",
      "Loss: 9.061696e-06\n",
      "Loss: 9.051391e-06\n",
      "Loss: 9.038993e-06\n",
      "Loss: 9.048166e-06\n",
      "Loss: 9.032446e-06\n",
      "Loss: 9.025756e-06\n",
      "Loss: 9.019146e-06\n",
      "Loss: 9.0139465e-06\n",
      "Loss: 9.001493e-06\n",
      "Loss: 8.999497e-06\n",
      "Loss: 8.991834e-06\n",
      "Loss: 8.98898e-06\n",
      "Loss: 8.984205e-06\n",
      "Loss: 8.97575e-06\n",
      "Loss: 8.963882e-06\n",
      "Loss: 8.948844e-06\n",
      "Loss: 8.940608e-06\n",
      "Loss: 8.935289e-06\n",
      "Loss: 8.9301475e-06\n",
      "Loss: 8.918831e-06\n",
      "Loss: 8.9080495e-06\n",
      "Loss: 8.900226e-06\n",
      "Loss: 8.884264e-06\n",
      "Loss: 8.872812e-06\n",
      "Loss: 8.864365e-06\n",
      "Loss: 8.855352e-06\n",
      "Loss: 8.848517e-06\n",
      "Loss: 8.838482e-06\n",
      "Loss: 8.8529505e-06\n",
      "Loss: 8.83257e-06\n",
      "Loss: 8.822754e-06\n",
      "Loss: 8.816915e-06\n",
      "Loss: 8.808738e-06\n",
      "Loss: 8.7950575e-06\n",
      "Loss: 8.826117e-06\n",
      "Loss: 8.791344e-06\n",
      "Loss: 8.782035e-06\n",
      "Loss: 8.768784e-06\n",
      "Loss: 8.754289e-06\n",
      "Loss: 8.743142e-06\n",
      "Loss: 8.734042e-06\n",
      "Loss: 8.72801e-06\n",
      "Loss: 8.72262e-06\n",
      "Loss: 8.711993e-06\n",
      "Loss: 8.709225e-06\n",
      "Loss: 8.702197e-06\n",
      "Loss: 8.6959e-06\n",
      "Loss: 8.68832e-06\n",
      "Loss: 8.67591e-06\n",
      "Loss: 8.664201e-06\n",
      "Loss: 8.646552e-06\n",
      "Loss: 8.638671e-06\n",
      "Loss: 8.629747e-06\n",
      "Loss: 8.621784e-06\n",
      "Loss: 8.6077525e-06\n",
      "Loss: 8.594388e-06\n",
      "Loss: 8.5872625e-06\n",
      "Loss: 8.583169e-06\n",
      "Loss: 8.5745905e-06\n",
      "Loss: 8.567915e-06\n",
      "Loss: 8.5622805e-06\n",
      "Loss: 8.55649e-06\n",
      "Loss: 8.547452e-06\n",
      "Loss: 8.539999e-06\n",
      "Loss: 8.535529e-06\n",
      "Loss: 8.531169e-06\n",
      "Loss: 8.5258e-06\n",
      "Loss: 8.519766e-06\n",
      "Loss: 8.508495e-06\n",
      "Loss: 8.500332e-06\n",
      "Loss: 8.494821e-06\n",
      "Loss: 8.489444e-06\n",
      "Loss: 8.482502e-06\n",
      "Loss: 8.474299e-06\n",
      "Loss: 8.466623e-06\n",
      "Loss: 8.458184e-06\n",
      "Loss: 8.445909e-06\n",
      "Loss: 8.444125e-06\n",
      "Loss: 8.433453e-06\n",
      "Loss: 8.426839e-06\n",
      "Loss: 8.42163e-06\n",
      "Loss: 8.4148505e-06\n",
      "Loss: 8.406048e-06\n",
      "Loss: 8.40222e-06\n",
      "Loss: 8.391487e-06\n",
      "Loss: 8.38855e-06\n",
      "Loss: 8.383908e-06\n",
      "Loss: 8.376799e-06\n",
      "Loss: 8.383837e-06\n",
      "Loss: 8.372701e-06\n",
      "Loss: 8.364253e-06\n",
      "Loss: 8.356952e-06\n",
      "Loss: 8.349853e-06\n",
      "Loss: 8.341385e-06\n",
      "Loss: 8.342159e-06\n",
      "Loss: 8.333939e-06\n",
      "Loss: 8.326495e-06\n",
      "Loss: 8.321271e-06\n",
      "Loss: 8.314371e-06\n",
      "Loss: 8.303703e-06\n",
      "Loss: 8.301167e-06\n",
      "Loss: 8.292174e-06\n",
      "Loss: 8.289593e-06\n",
      "Loss: 8.286261e-06\n",
      "Loss: 8.276485e-06\n",
      "Loss: 8.2597235e-06\n",
      "Loss: 8.258305e-06\n",
      "Loss: 8.247302e-06\n",
      "Loss: 8.242459e-06\n",
      "Loss: 8.234323e-06\n",
      "Loss: 8.2250135e-06\n",
      "Loss: 8.228567e-06\n",
      "Loss: 8.2201e-06\n",
      "Loss: 8.211458e-06\n",
      "Loss: 8.206802e-06\n",
      "Loss: 8.201268e-06\n",
      "Loss: 8.197123e-06\n",
      "Loss: 8.186695e-06\n",
      "Loss: 8.186586e-06\n",
      "Loss: 8.183393e-06\n",
      "Loss: 8.178998e-06\n",
      "Loss: 8.172505e-06\n",
      "Loss: 8.167701e-06\n",
      "Loss: 8.164606e-06\n",
      "Loss: 8.158648e-06\n",
      "Loss: 8.154997e-06\n",
      "Loss: 8.150452e-06\n",
      "Loss: 8.144812e-06\n",
      "Loss: 8.137499e-06\n",
      "Loss: 8.128536e-06\n",
      "Loss: 8.123678e-06\n",
      "Loss: 8.118044e-06\n",
      "Loss: 8.110148e-06\n",
      "Loss: 8.102314e-06\n",
      "Loss: 8.09573e-06\n",
      "Loss: 8.085823e-06\n",
      "Loss: 8.079726e-06\n",
      "Loss: 8.072165e-06\n",
      "Loss: 8.069088e-06\n",
      "Loss: 8.063615e-06\n",
      "Loss: 8.055516e-06\n",
      "Loss: 8.044137e-06\n",
      "Loss: 8.031326e-06\n",
      "Loss: 8.022005e-06\n",
      "Loss: 8.016689e-06\n",
      "Loss: 8.014401e-06\n",
      "Loss: 8.011516e-06\n",
      "Loss: 8.001952e-06\n",
      "Loss: 7.991255e-06\n",
      "Loss: 7.985925e-06\n",
      "Loss: 7.973092e-06\n",
      "Loss: 7.968297e-06\n",
      "Loss: 7.960011e-06\n",
      "Loss: 7.95296e-06\n",
      "Loss: 7.950686e-06\n",
      "Loss: 7.944615e-06\n",
      "Loss: 7.941591e-06\n",
      "Loss: 7.938742e-06\n",
      "Loss: 7.933977e-06\n",
      "Loss: 7.927163e-06\n",
      "Loss: 7.918159e-06\n",
      "Loss: 7.908469e-06\n",
      "Loss: 7.901985e-06\n",
      "Loss: 7.895824e-06\n",
      "Loss: 7.888217e-06\n",
      "Loss: 7.878551e-06\n",
      "Loss: 7.86863e-06\n",
      "Loss: 7.862572e-06\n",
      "Loss: 7.858246e-06\n",
      "Loss: 7.852784e-06\n",
      "Loss: 7.843101e-06\n",
      "Loss: 7.8384055e-06\n",
      "Loss: 7.830454e-06\n",
      "Loss: 7.82713e-06\n",
      "Loss: 7.821724e-06\n",
      "Loss: 7.813522e-06\n",
      "Loss: 7.807833e-06\n",
      "Loss: 7.7944505e-06\n",
      "Loss: 7.788995e-06\n",
      "Loss: 7.7837585e-06\n",
      "Loss: 7.7806235e-06\n",
      "Loss: 7.775799e-06\n",
      "Loss: 7.767404e-06\n",
      "Loss: 7.76223e-06\n",
      "Loss: 7.75387e-06\n",
      "Loss: 7.747105e-06\n",
      "Loss: 7.739552e-06\n",
      "Loss: 7.72951e-06\n",
      "Loss: 7.722841e-06\n",
      "Loss: 7.718153e-06\n",
      "Loss: 7.713885e-06\n",
      "Loss: 7.704069e-06\n",
      "Loss: 7.6932865e-06\n",
      "Loss: 7.685325e-06\n",
      "Loss: 7.680074e-06\n",
      "Loss: 7.674853e-06\n",
      "Loss: 7.669706e-06\n",
      "Loss: 7.6620345e-06\n",
      "Loss: 7.6603355e-06\n",
      "Loss: 7.654493e-06\n",
      "Loss: 7.65175e-06\n",
      "Loss: 7.648051e-06\n",
      "Loss: 7.643701e-06\n",
      "Loss: 7.637921e-06\n",
      "Loss: 7.642627e-06\n",
      "Loss: 7.632059e-06\n",
      "Loss: 7.627207e-06\n",
      "Loss: 7.6227893e-06\n",
      "Loss: 7.6163305e-06\n",
      "Loss: 7.609874e-06\n",
      "Loss: 7.6040324e-06\n",
      "Loss: 7.600595e-06\n",
      "Loss: 7.596797e-06\n",
      "Loss: 7.589956e-06\n",
      "Loss: 7.5817115e-06\n",
      "Loss: 7.5739554e-06\n",
      "Loss: 7.567027e-06\n",
      "Loss: 7.563426e-06\n",
      "Loss: 7.55937e-06\n",
      "Loss: 7.5591415e-06\n",
      "Loss: 7.556791e-06\n",
      "Loss: 7.5520793e-06\n",
      "Loss: 7.547126e-06\n",
      "Loss: 7.5415023e-06\n",
      "Loss: 7.5352923e-06\n",
      "Loss: 7.5406506e-06\n",
      "Loss: 7.5320822e-06\n",
      "Loss: 7.5252524e-06\n",
      "Loss: 7.520326e-06\n",
      "Loss: 7.5148237e-06\n",
      "Loss: 7.510519e-06\n",
      "Loss: 7.503352e-06\n",
      "Loss: 7.5016433e-06\n",
      "Loss: 7.4946993e-06\n",
      "Loss: 7.4912514e-06\n",
      "Loss: 7.487066e-06\n",
      "Loss: 7.488344e-06\n",
      "Loss: 7.4847785e-06\n",
      "Loss: 7.4812806e-06\n",
      "Loss: 7.476279e-06\n",
      "Loss: 7.4714944e-06\n",
      "Loss: 7.4643904e-06\n",
      "Loss: 7.4607583e-06\n",
      "Loss: 7.4575255e-06\n",
      "Loss: 7.4537606e-06\n",
      "Loss: 7.4503255e-06\n",
      "Loss: 7.445491e-06\n",
      "Loss: 7.439474e-06\n",
      "Loss: 7.4374007e-06\n",
      "Loss: 7.4258196e-06\n",
      "Loss: 7.4230616e-06\n",
      "Loss: 7.4196705e-06\n",
      "Loss: 7.415255e-06\n",
      "Loss: 7.4101704e-06\n",
      "Loss: 7.404626e-06\n",
      "Loss: 7.4017107e-06\n",
      "Loss: 7.39641e-06\n",
      "Loss: 7.393701e-06\n",
      "Loss: 7.3886513e-06\n",
      "Loss: 7.3822303e-06\n",
      "Loss: 7.3776305e-06\n",
      "Loss: 7.3700935e-06\n",
      "Loss: 7.366438e-06\n",
      "Loss: 7.3623805e-06\n",
      "Loss: 7.3581436e-06\n",
      "Loss: 7.3522046e-06\n",
      "Loss: 7.345673e-06\n",
      "Loss: 7.3402607e-06\n",
      "Loss: 7.3333767e-06\n",
      "Loss: 7.323294e-06\n",
      "Loss: 7.3374285e-06\n",
      "Loss: 7.317195e-06\n",
      "Loss: 7.320366e-06\n",
      "Loss: 7.3145466e-06\n",
      "Loss: 7.3107285e-06\n",
      "Loss: 7.307037e-06\n",
      "Loss: 7.304786e-06\n",
      "Loss: 7.3227848e-06\n",
      "Loss: 7.3027527e-06\n",
      "Loss: 7.297337e-06\n",
      "Loss: 7.292755e-06\n",
      "Loss: 7.2883563e-06\n",
      "Loss: 7.285047e-06\n",
      "Loss: 7.2804273e-06\n",
      "Loss: 7.276223e-06\n",
      "Loss: 7.2714574e-06\n",
      "Loss: 7.2640432e-06\n",
      "Loss: 7.258742e-06\n",
      "Loss: 7.254671e-06\n",
      "Loss: 7.251346e-06\n",
      "Loss: 7.246539e-06\n",
      "Loss: 7.2408384e-06\n",
      "Loss: 7.23523e-06\n",
      "Loss: 7.2306993e-06\n",
      "Loss: 7.2280445e-06\n",
      "Loss: 7.225819e-06\n",
      "Loss: 7.221577e-06\n",
      "Loss: 7.210786e-06\n",
      "Loss: 7.237165e-06\n",
      "Loss: 7.2081903e-06\n",
      "Loss: 7.2033054e-06\n",
      "Loss: 7.1966774e-06\n",
      "Loss: 7.190732e-06\n",
      "Loss: 7.1812788e-06\n",
      "Loss: 7.1712934e-06\n",
      "Loss: 7.1625705e-06\n",
      "Loss: 7.154842e-06\n",
      "Loss: 7.1494733e-06\n",
      "Loss: 7.1467243e-06\n",
      "Loss: 7.142739e-06\n",
      "Loss: 7.1324303e-06\n",
      "Loss: 7.130765e-06\n",
      "Loss: 7.122091e-06\n",
      "Loss: 7.1191926e-06\n",
      "Loss: 7.1138e-06\n",
      "Loss: 7.1086083e-06\n",
      "Loss: 7.103185e-06\n",
      "Loss: 7.0932283e-06\n",
      "Loss: 7.085516e-06\n",
      "Loss: 7.076673e-06\n",
      "Loss: 7.0704673e-06\n",
      "Loss: 7.065216e-06\n",
      "Loss: 7.0600504e-06\n",
      "Loss: 7.0516485e-06\n",
      "Loss: 7.041098e-06\n",
      "Loss: 7.0325423e-06\n",
      "Loss: 7.0280335e-06\n",
      "Loss: 7.023469e-06\n",
      "Loss: 7.017006e-06\n",
      "Loss: 7.0065366e-06\n",
      "Loss: 7.0005435e-06\n",
      "Loss: 6.9983444e-06\n",
      "Loss: 6.994435e-06\n",
      "Loss: 6.991776e-06\n",
      "Loss: 6.9876455e-06\n",
      "Loss: 6.9834914e-06\n",
      "Loss: 7.0045517e-06\n",
      "Loss: 6.9815633e-06\n",
      "Loss: 6.975851e-06\n",
      "Loss: 6.971994e-06\n",
      "Loss: 6.966533e-06\n",
      "Loss: 6.9608936e-06\n",
      "Loss: 6.955619e-06\n",
      "Loss: 6.9491625e-06\n",
      "Loss: 6.945075e-06\n",
      "Loss: 6.940578e-06\n",
      "Loss: 6.9317853e-06\n",
      "Loss: 6.9365233e-06\n",
      "Loss: 6.926977e-06\n",
      "Loss: 6.918549e-06\n",
      "Loss: 6.913935e-06\n",
      "Loss: 6.9085772e-06\n",
      "Loss: 6.9038006e-06\n",
      "Loss: 6.901909e-06\n",
      "Loss: 6.8965674e-06\n",
      "Loss: 6.893113e-06\n",
      "Loss: 6.890188e-06\n",
      "Loss: 6.885888e-06\n",
      "Loss: 6.877439e-06\n",
      "Loss: 6.8702384e-06\n",
      "Loss: 6.863927e-06\n",
      "Loss: 6.8598256e-06\n",
      "Loss: 6.8524596e-06\n",
      "Loss: 6.854906e-06\n",
      "Loss: 6.847594e-06\n",
      "Loss: 6.8401987e-06\n",
      "Loss: 6.8314466e-06\n",
      "Loss: 6.828278e-06\n",
      "Loss: 6.824265e-06\n",
      "Loss: 6.817557e-06\n",
      "Loss: 6.815771e-06\n",
      "Loss: 6.810322e-06\n",
      "Loss: 6.8076797e-06\n",
      "Loss: 6.8034824e-06\n",
      "Loss: 6.798726e-06\n",
      "Loss: 6.793994e-06\n",
      "Loss: 6.7881406e-06\n",
      "Loss: 6.7852616e-06\n",
      "Loss: 6.7804112e-06\n",
      "Loss: 6.776273e-06\n",
      "Loss: 6.7709043e-06\n",
      "Loss: 6.7651636e-06\n",
      "Loss: 6.7605006e-06\n",
      "Loss: 6.755351e-06\n",
      "Loss: 6.7522315e-06\n",
      "Loss: 6.748013e-06\n",
      "Loss: 6.7432184e-06\n",
      "Loss: 6.7387937e-06\n",
      "Loss: 6.7367346e-06\n",
      "Loss: 6.734153e-06\n",
      "Loss: 6.7301044e-06\n",
      "Loss: 6.7247265e-06\n",
      "Loss: 6.7203246e-06\n",
      "Loss: 6.717724e-06\n",
      "Loss: 6.715174e-06\n",
      "Loss: 6.712264e-06\n",
      "Loss: 6.7073734e-06\n",
      "Loss: 6.7025376e-06\n",
      "Loss: 6.700736e-06\n",
      "Loss: 6.695832e-06\n",
      "Loss: 6.694113e-06\n",
      "Loss: 6.689385e-06\n",
      "Loss: 6.68375e-06\n",
      "Loss: 6.676532e-06\n",
      "Loss: 6.6763337e-06\n",
      "Loss: 6.6718558e-06\n",
      "Loss: 6.6674656e-06\n",
      "Loss: 6.6642438e-06\n",
      "Loss: 6.6620114e-06\n",
      "Loss: 6.6586617e-06\n",
      "Loss: 6.6559387e-06\n",
      "Loss: 6.6522443e-06\n",
      "Loss: 6.6475295e-06\n",
      "Loss: 6.6404596e-06\n",
      "Loss: 6.643758e-06\n",
      "Loss: 6.637283e-06\n",
      "Loss: 6.633029e-06\n",
      "Loss: 6.628797e-06\n",
      "Loss: 6.6235602e-06\n",
      "Loss: 6.6204266e-06\n",
      "Loss: 6.613292e-06\n",
      "Loss: 6.6091925e-06\n",
      "Loss: 6.6048187e-06\n",
      "Loss: 6.599902e-06\n",
      "Loss: 6.5939666e-06\n",
      "Loss: 6.589614e-06\n",
      "Loss: 6.5841605e-06\n",
      "Loss: 6.580398e-06\n",
      "Loss: 6.5761374e-06\n",
      "Loss: 6.569895e-06\n",
      "Loss: 6.560374e-06\n",
      "Loss: 6.5528384e-06\n",
      "Loss: 6.5495324e-06\n",
      "Loss: 6.5399545e-06\n",
      "Loss: 6.535546e-06\n",
      "Loss: 6.531124e-06\n",
      "Loss: 6.5255654e-06\n",
      "Loss: 6.517013e-06\n",
      "Loss: 6.512455e-06\n",
      "Loss: 6.5067616e-06\n",
      "Loss: 6.502052e-06\n",
      "Loss: 6.498015e-06\n",
      "Loss: 6.5069753e-06\n",
      "Loss: 6.495868e-06\n",
      "Loss: 6.4919386e-06\n",
      "Loss: 6.487914e-06\n",
      "Loss: 6.481417e-06\n",
      "Loss: 6.4754067e-06\n",
      "Loss: 6.4798114e-06\n",
      "Loss: 6.47266e-06\n",
      "Loss: 6.4686265e-06\n",
      "Loss: 6.4657347e-06\n",
      "Loss: 6.4618616e-06\n",
      "Loss: 6.45754e-06\n",
      "Loss: 6.454472e-06\n",
      "Loss: 6.4514797e-06\n",
      "Loss: 6.448728e-06\n",
      "Loss: 6.445707e-06\n",
      "Loss: 6.4392752e-06\n",
      "Loss: 6.4282735e-06\n",
      "Loss: 6.4341766e-06\n",
      "Loss: 6.423631e-06\n",
      "Loss: 6.416767e-06\n",
      "Loss: 6.4137266e-06\n",
      "Loss: 6.408887e-06\n",
      "Loss: 6.4019305e-06\n",
      "Loss: 6.4235073e-06\n",
      "Loss: 6.399863e-06\n",
      "Loss: 6.395257e-06\n",
      "Loss: 6.3917932e-06\n",
      "Loss: 6.3881434e-06\n",
      "Loss: 6.38259e-06\n",
      "Loss: 6.381116e-06\n",
      "Loss: 6.3746425e-06\n",
      "Loss: 6.372275e-06\n",
      "Loss: 6.369649e-06\n",
      "Loss: 6.366472e-06\n",
      "Loss: 6.3612806e-06\n",
      "Loss: 6.358297e-06\n",
      "Loss: 6.354902e-06\n",
      "Loss: 6.353136e-06\n",
      "Loss: 6.3503085e-06\n",
      "Loss: 6.344634e-06\n",
      "Loss: 6.33949e-06\n",
      "Loss: 6.3344187e-06\n",
      "Loss: 6.3300895e-06\n",
      "Loss: 6.3273305e-06\n",
      "Loss: 6.318006e-06\n",
      "Loss: 6.3204498e-06\n",
      "Loss: 6.3152766e-06\n",
      "Loss: 6.3109637e-06\n",
      "Loss: 6.307593e-06\n",
      "Loss: 6.3041302e-06\n",
      "Loss: 6.3001357e-06\n",
      "Loss: 6.2957088e-06\n",
      "Loss: 6.290983e-06\n",
      "Loss: 6.2858735e-06\n",
      "Loss: 6.285021e-06\n",
      "Loss: 6.2793724e-06\n",
      "Loss: 6.2769564e-06\n",
      "Loss: 6.2744857e-06\n",
      "Loss: 6.271331e-06\n",
      "Loss: 6.2675863e-06\n",
      "Loss: 6.262224e-06\n",
      "Loss: 6.2585004e-06\n",
      "Loss: 6.2577733e-06\n",
      "Loss: 6.250954e-06\n",
      "Loss: 6.248527e-06\n",
      "Loss: 6.245692e-06\n",
      "Loss: 6.241695e-06\n",
      "Loss: 6.2365816e-06\n",
      "Loss: 6.23321e-06\n",
      "Loss: 6.229515e-06\n",
      "Loss: 6.223842e-06\n",
      "Loss: 6.2162826e-06\n",
      "Loss: 6.2126687e-06\n",
      "Loss: 6.2083204e-06\n",
      "Loss: 6.205936e-06\n",
      "Loss: 6.2026697e-06\n",
      "Loss: 6.1979754e-06\n",
      "Loss: 6.194272e-06\n",
      "Loss: 6.1906658e-06\n",
      "Loss: 6.187647e-06\n",
      "Loss: 6.1848064e-06\n",
      "Loss: 6.1806586e-06\n",
      "Loss: 6.1753262e-06\n",
      "Loss: 6.170738e-06\n",
      "Loss: 6.1655046e-06\n",
      "Loss: 6.161506e-06\n",
      "Loss: 6.1560813e-06\n",
      "Loss: 6.1524015e-06\n",
      "Loss: 6.147843e-06\n",
      "Loss: 6.1428577e-06\n",
      "Loss: 6.139515e-06\n",
      "Loss: 6.135184e-06\n",
      "Loss: 6.1296796e-06\n",
      "Loss: 6.123404e-06\n",
      "Loss: 6.1206647e-06\n",
      "Loss: 6.1157652e-06\n",
      "Loss: 6.1135597e-06\n",
      "Loss: 6.110202e-06\n",
      "Loss: 6.1049354e-06\n",
      "Loss: 6.0976427e-06\n",
      "Loss: 6.0927523e-06\n",
      "Loss: 6.0867615e-06\n",
      "Loss: 6.082294e-06\n",
      "Loss: 6.0785605e-06\n",
      "Loss: 6.0708785e-06\n",
      "Loss: 6.0650705e-06\n",
      "Loss: 6.0517923e-06\n",
      "Loss: 6.047851e-06\n",
      "Loss: 6.043385e-06\n",
      "Loss: 6.044672e-06\n",
      "Loss: 6.0412876e-06\n",
      "Loss: 6.037912e-06\n",
      "Loss: 6.032269e-06\n",
      "Loss: 6.028513e-06\n",
      "Loss: 6.0249167e-06\n",
      "Loss: 6.0218335e-06\n",
      "Loss: 6.0190005e-06\n",
      "Loss: 6.01586e-06\n",
      "Loss: 6.0124275e-06\n",
      "Loss: 6.010513e-06\n",
      "Loss: 6.006432e-06\n",
      "Loss: 6.006249e-06\n",
      "Loss: 6.0038947e-06\n",
      "Loss: 5.999217e-06\n",
      "Loss: 5.994806e-06\n",
      "Loss: 5.9899335e-06\n",
      "Loss: 5.9871745e-06\n",
      "Loss: 5.9851536e-06\n",
      "Loss: 5.98269e-06\n",
      "Loss: 5.9803706e-06\n",
      "Loss: 5.9761023e-06\n",
      "Loss: 5.9736194e-06\n",
      "Loss: 5.969706e-06\n",
      "Loss: 5.968358e-06\n",
      "Loss: 5.966161e-06\n",
      "Loss: 5.9628746e-06\n",
      "Loss: 5.9592658e-06\n",
      "Loss: 5.9618324e-06\n",
      "Loss: 5.9569197e-06\n",
      "Loss: 5.95391e-06\n",
      "Loss: 5.9502145e-06\n",
      "Loss: 5.9468575e-06\n",
      "Loss: 5.9428e-06\n",
      "Loss: 5.938272e-06\n",
      "Loss: 5.9374224e-06\n",
      "Loss: 5.931918e-06\n",
      "Loss: 5.9294034e-06\n",
      "Loss: 5.925256e-06\n",
      "Loss: 5.919828e-06\n",
      "Loss: 5.9279646e-06\n",
      "Loss: 5.9175763e-06\n",
      "Loss: 5.9142385e-06\n",
      "Loss: 5.9120853e-06\n",
      "Loss: 5.908638e-06\n",
      "Loss: 5.9033846e-06\n",
      "Loss: 5.8955707e-06\n",
      "Loss: 5.8919836e-06\n",
      "Loss: 5.8881415e-06\n",
      "Loss: 5.8860533e-06\n",
      "Loss: 5.884196e-06\n",
      "Loss: 5.881192e-06\n",
      "Loss: 5.877069e-06\n",
      "Loss: 5.8731353e-06\n",
      "Loss: 5.8710584e-06\n",
      "Loss: 5.871311e-06\n",
      "Loss: 5.868469e-06\n",
      "Loss: 5.864584e-06\n",
      "Loss: 5.860988e-06\n",
      "Loss: 5.858187e-06\n",
      "Loss: 5.8548903e-06\n",
      "Loss: 5.8541086e-06\n",
      "Loss: 5.8515247e-06\n",
      "Loss: 5.847296e-06\n",
      "Loss: 5.84493e-06\n",
      "Loss: 5.8424293e-06\n",
      "Loss: 5.841325e-06\n",
      "Loss: 5.8381233e-06\n",
      "Loss: 5.834743e-06\n",
      "Loss: 5.8318938e-06\n",
      "Loss: 5.8280393e-06\n",
      "Loss: 5.8263463e-06\n",
      "Loss: 5.823606e-06\n",
      "Loss: 5.821706e-06\n",
      "Loss: 5.8185606e-06\n",
      "Loss: 5.8164683e-06\n",
      "Loss: 5.813966e-06\n",
      "Loss: 5.810838e-06\n",
      "Loss: 5.8063642e-06\n",
      "Loss: 5.801933e-06\n",
      "Loss: 5.7992047e-06\n",
      "Loss: 5.7959614e-06\n",
      "Loss: 5.7930247e-06\n",
      "Loss: 5.7862185e-06\n",
      "Loss: 5.7841335e-06\n",
      "Loss: 5.773496e-06\n",
      "Loss: 5.7678662e-06\n",
      "Loss: 5.764361e-06\n",
      "Loss: 5.7607213e-06\n",
      "Loss: 5.75755e-06\n",
      "Loss: 5.753186e-06\n",
      "Loss: 5.7504467e-06\n",
      "Loss: 5.747689e-06\n",
      "Loss: 5.7428415e-06\n",
      "Loss: 5.7396755e-06\n",
      "Loss: 5.736574e-06\n",
      "Loss: 5.7317056e-06\n",
      "Loss: 5.728586e-06\n",
      "Loss: 5.722548e-06\n",
      "Loss: 5.718816e-06\n",
      "Loss: 5.7112693e-06\n",
      "Loss: 5.7204843e-06\n",
      "Loss: 5.7083007e-06\n",
      "Loss: 5.703924e-06\n",
      "Loss: 5.7000834e-06\n",
      "Loss: 5.694794e-06\n",
      "Loss: 5.6859535e-06\n",
      "Loss: 5.6835406e-06\n",
      "Loss: 5.677456e-06\n",
      "Loss: 5.674943e-06\n",
      "Loss: 5.673268e-06\n",
      "Loss: 5.6707527e-06\n",
      "Loss: 5.6672598e-06\n",
      "Loss: 5.6644376e-06\n",
      "Loss: 5.6623085e-06\n",
      "Loss: 5.6588406e-06\n",
      "Loss: 5.6555027e-06\n",
      "Loss: 5.6500967e-06\n",
      "Loss: 5.654705e-06\n",
      "Loss: 5.648184e-06\n",
      "Loss: 5.6445833e-06\n",
      "Loss: 5.6422427e-06\n",
      "Loss: 5.639893e-06\n",
      "Loss: 5.6353674e-06\n",
      "Loss: 5.6348135e-06\n",
      "Loss: 5.628448e-06\n",
      "Loss: 5.626417e-06\n",
      "Loss: 5.624101e-06\n",
      "Loss: 5.621742e-06\n",
      "Loss: 5.625378e-06\n",
      "Loss: 5.6195945e-06\n",
      "Loss: 5.6163544e-06\n",
      "Loss: 5.613779e-06\n",
      "Loss: 5.6109375e-06\n",
      "Loss: 5.6068184e-06\n",
      "Loss: 5.622971e-06\n",
      "Loss: 5.6054596e-06\n",
      "Loss: 5.6020717e-06\n",
      "Loss: 5.5985074e-06\n",
      "Loss: 5.595626e-06\n",
      "Loss: 5.5915525e-06\n",
      "Loss: 5.587419e-06\n",
      "Loss: 5.5845185e-06\n",
      "Loss: 5.582693e-06\n",
      "Loss: 5.57934e-06\n",
      "Loss: 5.5765036e-06\n",
      "Loss: 5.5719293e-06\n",
      "Loss: 5.5662977e-06\n",
      "Loss: 5.5612472e-06\n",
      "Loss: 5.557703e-06\n",
      "Loss: 5.555647e-06\n",
      "Loss: 5.5524238e-06\n",
      "Loss: 5.5480537e-06\n",
      "Loss: 5.544156e-06\n",
      "Loss: 5.5417127e-06\n",
      "Loss: 5.5385735e-06\n",
      "Loss: 5.5364653e-06\n",
      "Loss: 5.5301566e-06\n",
      "Loss: 5.5311652e-06\n",
      "Loss: 5.5270402e-06\n",
      "Loss: 5.5201085e-06\n",
      "Loss: 5.5156825e-06\n",
      "Loss: 5.512172e-06\n",
      "Loss: 5.51016e-06\n",
      "Loss: 5.506897e-06\n",
      "Loss: 5.5032033e-06\n",
      "Loss: 5.5006476e-06\n",
      "Loss: 5.4979355e-06\n",
      "Loss: 5.4939246e-06\n",
      "Loss: 5.48997e-06\n",
      "Loss: 5.4874995e-06\n",
      "Loss: 5.4823886e-06\n",
      "Loss: 5.4765105e-06\n",
      "Loss: 5.472794e-06\n",
      "Loss: 5.4708325e-06\n",
      "Loss: 5.468395e-06\n",
      "Loss: 5.466579e-06\n",
      "Loss: 5.4638685e-06\n",
      "Loss: 5.459357e-06\n",
      "Loss: 5.456366e-06\n",
      "Loss: 5.4532793e-06\n",
      "Loss: 5.451734e-06\n",
      "Loss: 5.449392e-06\n",
      "Loss: 5.44653e-06\n",
      "Loss: 5.4418324e-06\n",
      "Loss: 5.448178e-06\n",
      "Loss: 5.4397287e-06\n",
      "Loss: 5.4350735e-06\n",
      "Loss: 5.4315406e-06\n",
      "Loss: 5.4283364e-06\n",
      "Loss: 5.425635e-06\n",
      "Loss: 5.4233424e-06\n",
      "Loss: 5.4209772e-06\n",
      "Loss: 5.419258e-06\n",
      "Loss: 5.417788e-06\n",
      "Loss: 5.4139487e-06\n",
      "Loss: 5.4099173e-06\n",
      "Loss: 5.407478e-06\n",
      "Loss: 5.4041025e-06\n",
      "Loss: 5.4026623e-06\n",
      "Loss: 5.399788e-06\n",
      "Loss: 5.3942886e-06\n",
      "Loss: 5.4088205e-06\n",
      "Loss: 5.3925824e-06\n",
      "Loss: 5.388385e-06\n",
      "Loss: 5.384734e-06\n",
      "Loss: 5.380258e-06\n",
      "Loss: 5.376586e-06\n",
      "Loss: 5.375002e-06\n",
      "Loss: 5.3712283e-06\n",
      "Loss: 5.3686995e-06\n",
      "Loss: 5.3665253e-06\n",
      "Loss: 5.361906e-06\n",
      "Loss: 5.360404e-06\n",
      "Loss: 5.3519416e-06\n",
      "Loss: 5.348985e-06\n",
      "Loss: 5.3458507e-06\n",
      "Loss: 5.342169e-06\n",
      "Loss: 5.337979e-06\n",
      "Loss: 5.3303133e-06\n",
      "Loss: 5.3251724e-06\n",
      "Loss: 5.321397e-06\n",
      "Loss: 5.317932e-06\n",
      "Loss: 5.3148915e-06\n",
      "Loss: 5.3119684e-06\n",
      "Loss: 5.309697e-06\n",
      "Loss: 5.3070244e-06\n",
      "Loss: 5.3035747e-06\n",
      "Loss: 5.3006434e-06\n",
      "Loss: 5.299448e-06\n",
      "Loss: 5.296318e-06\n",
      "Loss: 5.2952087e-06\n",
      "Loss: 5.29292e-06\n",
      "Loss: 5.2898044e-06\n",
      "Loss: 5.2847154e-06\n",
      "Loss: 5.2867535e-06\n",
      "Loss: 5.282287e-06\n",
      "Loss: 5.2796686e-06\n",
      "Loss: 5.277703e-06\n",
      "Loss: 5.2756523e-06\n",
      "Loss: 5.2713804e-06\n",
      "Loss: 5.2690475e-06\n",
      "Loss: 5.2665946e-06\n",
      "Loss: 5.2645064e-06\n",
      "Loss: 5.2632276e-06\n",
      "Loss: 5.2609294e-06\n",
      "Loss: 5.2575097e-06\n",
      "Loss: 5.256242e-06\n",
      "Loss: 5.253897e-06\n",
      "Loss: 5.251735e-06\n",
      "Loss: 5.247265e-06\n",
      "Loss: 5.2474197e-06\n",
      "Loss: 5.245278e-06\n",
      "Loss: 5.243327e-06\n",
      "Loss: 5.2420173e-06\n",
      "Loss: 5.2396663e-06\n",
      "Loss: 5.235932e-06\n",
      "Loss: 5.232647e-06\n",
      "Loss: 5.2289724e-06\n",
      "Loss: 5.2262126e-06\n",
      "Loss: 5.22385e-06\n",
      "Loss: 5.2205824e-06\n",
      "Loss: 5.2169453e-06\n",
      "Loss: 5.2146584e-06\n",
      "Loss: 5.212813e-06\n",
      "Loss: 5.2087894e-06\n",
      "Loss: 5.2055807e-06\n",
      "Loss: 5.201761e-06\n",
      "Loss: 5.199159e-06\n",
      "Loss: 5.1967295e-06\n",
      "Loss: 5.194359e-06\n",
      "Loss: 5.191567e-06\n",
      "Loss: 5.1892794e-06\n",
      "Loss: 5.1865723e-06\n",
      "Loss: 5.1843117e-06\n",
      "Loss: 5.1788775e-06\n",
      "Loss: 5.175487e-06\n",
      "Loss: 5.17106e-06\n",
      "Loss: 5.1678726e-06\n",
      "Loss: 5.1652232e-06\n",
      "Loss: 5.161697e-06\n",
      "Loss: 5.165635e-06\n",
      "Loss: 5.1596217e-06\n",
      "Loss: 5.1569436e-06\n",
      "Loss: 5.1544334e-06\n",
      "Loss: 5.153698e-06\n",
      "Loss: 5.1509437e-06\n",
      "Loss: 5.1495335e-06\n",
      "Loss: 5.1470806e-06\n",
      "Loss: 5.144028e-06\n",
      "Loss: 5.1555107e-06\n",
      "Loss: 5.142637e-06\n",
      "Loss: 5.138644e-06\n",
      "Loss: 5.1363036e-06\n",
      "Loss: 5.133068e-06\n",
      "Loss: 5.1293136e-06\n",
      "Loss: 5.1364877e-06\n",
      "Loss: 5.1272127e-06\n",
      "Loss: 5.1224533e-06\n",
      "Loss: 5.118799e-06\n",
      "Loss: 5.113888e-06\n",
      "Loss: 5.1098577e-06\n",
      "Loss: 5.1048128e-06\n",
      "Loss: 5.1017514e-06\n",
      "Loss: 5.0989747e-06\n",
      "Loss: 5.095978e-06\n",
      "Loss: 5.0910958e-06\n",
      "Loss: 5.085255e-06\n",
      "Loss: 5.086395e-06\n",
      "Loss: 5.082268e-06\n",
      "Loss: 5.078371e-06\n",
      "Loss: 5.075836e-06\n",
      "Loss: 5.073536e-06\n",
      "Loss: 5.071521e-06\n",
      "Loss: 5.0690014e-06\n",
      "Loss: 5.065795e-06\n",
      "Loss: 5.062717e-06\n",
      "Loss: 5.059089e-06\n",
      "Loss: 5.05982e-06\n",
      "Loss: 5.0570893e-06\n",
      "Loss: 5.0551494e-06\n",
      "Loss: 5.0526032e-06\n",
      "Loss: 5.050357e-06\n",
      "Loss: 5.045801e-06\n",
      "Loss: 5.040269e-06\n",
      "Loss: 5.038349e-06\n",
      "Loss: 5.0314748e-06\n",
      "Loss: 5.0281787e-06\n",
      "Loss: 5.0238855e-06\n",
      "Loss: 5.0215667e-06\n",
      "Loss: 5.018519e-06\n",
      "Loss: 5.014845e-06\n",
      "Loss: 5.012827e-06\n",
      "Loss: 5.0088047e-06\n",
      "Loss: 5.0054177e-06\n",
      "Loss: 5.0029203e-06\n",
      "Loss: 4.999549e-06\n",
      "Loss: 4.99458e-06\n",
      "Loss: 4.990623e-06\n",
      "Loss: 4.9868513e-06\n",
      "Loss: 4.9841065e-06\n",
      "Loss: 4.9787213e-06\n",
      "Loss: 4.978632e-06\n",
      "Loss: 4.976085e-06\n",
      "Loss: 4.9737755e-06\n",
      "Loss: 4.969751e-06\n",
      "Loss: 4.9655628e-06\n",
      "Loss: 4.9719797e-06\n",
      "Loss: 4.9634846e-06\n",
      "Loss: 4.95908e-06\n",
      "Loss: 4.9569862e-06\n",
      "Loss: 4.953041e-06\n",
      "Loss: 4.948622e-06\n",
      "Loss: 4.950033e-06\n",
      "Loss: 4.9461974e-06\n",
      "Loss: 4.9409464e-06\n",
      "Loss: 4.9376536e-06\n",
      "Loss: 4.933494e-06\n",
      "Loss: 4.9299065e-06\n",
      "Loss: 4.9332384e-06\n",
      "Loss: 4.9283517e-06\n",
      "Loss: 4.9257915e-06\n",
      "Loss: 4.9243854e-06\n",
      "Loss: 4.9228934e-06\n",
      "Loss: 4.919375e-06\n",
      "Loss: 4.9120717e-06\n",
      "Loss: 4.9452783e-06\n",
      "Loss: 4.910932e-06\n",
      "Loss: 4.90593e-06\n",
      "Loss: 4.9026758e-06\n",
      "Loss: 4.898537e-06\n",
      "Loss: 4.8942047e-06\n",
      "Loss: 4.8912125e-06\n",
      "Loss: 4.8862557e-06\n",
      "Loss: 4.881542e-06\n",
      "Loss: 4.878063e-06\n",
      "Loss: 4.8748166e-06\n",
      "Loss: 4.873183e-06\n",
      "Loss: 4.871622e-06\n",
      "Loss: 4.869329e-06\n",
      "Loss: 4.8664147e-06\n",
      "Loss: 4.8606694e-06\n",
      "Loss: 4.8571314e-06\n",
      "Loss: 4.851165e-06\n",
      "Loss: 4.848436e-06\n",
      "Loss: 4.8450565e-06\n",
      "Loss: 4.8423194e-06\n",
      "Loss: 4.8396764e-06\n",
      "Loss: 4.8358606e-06\n",
      "Loss: 4.832711e-06\n",
      "Loss: 4.829404e-06\n",
      "Loss: 4.827023e-06\n",
      "Loss: 4.825308e-06\n",
      "Loss: 4.821646e-06\n",
      "Loss: 4.819346e-06\n",
      "Loss: 4.8169486e-06\n",
      "Loss: 4.816101e-06\n",
      "Loss: 4.81386e-06\n",
      "Loss: 4.8087654e-06\n",
      "Loss: 4.8074726e-06\n",
      "Loss: 4.804817e-06\n",
      "Loss: 4.803408e-06\n",
      "Loss: 4.800936e-06\n",
      "Loss: 4.798011e-06\n",
      "Loss: 4.8043103e-06\n",
      "Loss: 4.796876e-06\n",
      "Loss: 4.794225e-06\n",
      "Loss: 4.792754e-06\n",
      "Loss: 4.790203e-06\n",
      "Loss: 4.788012e-06\n",
      "Loss: 4.7866088e-06\n",
      "Loss: 4.783885e-06\n",
      "Loss: 4.781904e-06\n",
      "Loss: 4.7805943e-06\n",
      "Loss: 4.7797294e-06\n",
      "Loss: 4.776947e-06\n",
      "Loss: 4.7751864e-06\n",
      "Loss: 4.773874e-06\n",
      "Loss: 4.7710473e-06\n",
      "Loss: 4.766782e-06\n",
      "Loss: 4.764337e-06\n",
      "Loss: 4.7619706e-06\n",
      "Loss: 4.75829e-06\n",
      "Loss: 4.7564727e-06\n",
      "Loss: 4.754502e-06\n",
      "Loss: 4.7512704e-06\n",
      "Loss: 4.7480335e-06\n",
      "Loss: 4.745946e-06\n",
      "Loss: 4.743913e-06\n",
      "Loss: 4.7430703e-06\n",
      "Loss: 4.7417957e-06\n",
      "Loss: 4.7395492e-06\n",
      "Loss: 4.738402e-06\n",
      "Loss: 4.734194e-06\n",
      "Loss: 4.732217e-06\n",
      "Loss: 4.730915e-06\n",
      "Loss: 4.729107e-06\n",
      "Loss: 4.7233943e-06\n",
      "Loss: 4.7285603e-06\n",
      "Loss: 4.721648e-06\n",
      "Loss: 4.71779e-06\n",
      "Loss: 4.717085e-06\n",
      "Loss: 4.712335e-06\n",
      "Loss: 4.7095295e-06\n",
      "Loss: 4.7060207e-06\n",
      "Loss: 4.704405e-06\n",
      "Loss: 4.701566e-06\n",
      "Loss: 4.6994255e-06\n",
      "Loss: 4.696887e-06\n",
      "Loss: 4.6940054e-06\n",
      "Loss: 4.6917175e-06\n",
      "Loss: 4.688728e-06\n",
      "Loss: 4.6865453e-06\n",
      "Loss: 4.6849295e-06\n",
      "Loss: 4.682072e-06\n",
      "Loss: 4.6775517e-06\n",
      "Loss: 4.680918e-06\n",
      "Loss: 4.67581e-06\n",
      "Loss: 4.6736113e-06\n",
      "Loss: 4.672101e-06\n",
      "Loss: 4.6707873e-06\n",
      "Loss: 4.669326e-06\n",
      "Loss: 4.6671207e-06\n",
      "Loss: 4.664652e-06\n",
      "Loss: 4.6615487e-06\n",
      "Loss: 4.657593e-06\n",
      "Loss: 4.6556506e-06\n",
      "Loss: 4.6517325e-06\n",
      "Loss: 4.6499927e-06\n",
      "Loss: 4.64853e-06\n",
      "Loss: 4.6468335e-06\n",
      "Loss: 4.644994e-06\n",
      "Loss: 4.641757e-06\n",
      "Loss: 4.6386544e-06\n",
      "Loss: 4.635554e-06\n",
      "Loss: 4.6316936e-06\n",
      "Loss: 4.6292257e-06\n",
      "Loss: 4.627025e-06\n",
      "Loss: 4.62528e-06\n",
      "Loss: 4.6229043e-06\n",
      "Loss: 4.6187743e-06\n",
      "Loss: 4.6154064e-06\n",
      "Loss: 4.6138803e-06\n",
      "Loss: 4.612237e-06\n",
      "Loss: 4.609204e-06\n",
      "Loss: 4.605012e-06\n",
      "Loss: 4.603166e-06\n",
      "Loss: 4.599935e-06\n",
      "Loss: 4.598628e-06\n",
      "Loss: 4.596884e-06\n",
      "Loss: 4.5944726e-06\n",
      "Loss: 4.589986e-06\n",
      "Loss: 4.5893e-06\n",
      "Loss: 4.5860857e-06\n",
      "Loss: 4.580915e-06\n",
      "Loss: 4.577452e-06\n",
      "Loss: 4.575064e-06\n",
      "Loss: 4.5724983e-06\n",
      "Loss: 4.569112e-06\n",
      "Loss: 4.5670454e-06\n",
      "Loss: 4.563497e-06\n",
      "Loss: 4.5598767e-06\n",
      "Loss: 4.557236e-06\n",
      "Loss: 4.553986e-06\n",
      "Loss: 4.5516854e-06\n",
      "Loss: 4.546716e-06\n",
      "Loss: 4.573958e-06\n",
      "Loss: 4.5447723e-06\n",
      "Loss: 4.541053e-06\n",
      "Loss: 4.5388974e-06\n",
      "Loss: 4.5362717e-06\n",
      "Loss: 4.5336574e-06\n",
      "Loss: 4.531582e-06\n",
      "Loss: 4.530564e-06\n",
      "Loss: 4.52871e-06\n",
      "Loss: 4.5273105e-06\n",
      "Loss: 4.5252345e-06\n",
      "Loss: 4.522376e-06\n",
      "Loss: 4.5190795e-06\n",
      "Loss: 4.5168435e-06\n",
      "Loss: 4.51397e-06\n",
      "Loss: 4.5158263e-06\n",
      "Loss: 4.512395e-06\n",
      "Loss: 4.5094603e-06\n",
      "Loss: 4.5076113e-06\n",
      "Loss: 4.5065253e-06\n",
      "Loss: 4.5052075e-06\n",
      "Loss: 4.5040597e-06\n",
      "Loss: 4.5026045e-06\n",
      "Loss: 4.5003044e-06\n",
      "Loss: 4.497762e-06\n",
      "Loss: 4.495985e-06\n",
      "Loss: 4.494879e-06\n",
      "Loss: 4.4937906e-06\n",
      "Loss: 4.492501e-06\n",
      "Loss: 4.4884646e-06\n",
      "Loss: 4.484089e-06\n",
      "Loss: 4.4943e-06\n",
      "Loss: 4.481702e-06\n",
      "Loss: 4.4766566e-06\n",
      "Loss: 4.4740173e-06\n",
      "Loss: 4.4710887e-06\n",
      "Loss: 4.468918e-06\n",
      "Loss: 4.465498e-06\n",
      "Loss: 4.4627823e-06\n",
      "Loss: 4.4600365e-06\n",
      "Loss: 4.457052e-06\n",
      "Loss: 4.451606e-06\n",
      "Loss: 4.4482335e-06\n",
      "Loss: 4.443369e-06\n",
      "Loss: 4.438711e-06\n",
      "Loss: 4.4362196e-06\n",
      "Loss: 4.4348267e-06\n",
      "Loss: 4.432016e-06\n",
      "Loss: 4.4316425e-06\n",
      "Loss: 4.4294175e-06\n",
      "Loss: 4.426821e-06\n",
      "Loss: 4.425366e-06\n",
      "Loss: 4.42248e-06\n",
      "Loss: 4.4195535e-06\n",
      "Loss: 4.4150447e-06\n",
      "Loss: 4.411784e-06\n",
      "Loss: 4.4062704e-06\n",
      "Loss: 4.4024073e-06\n",
      "Loss: 4.4004187e-06\n",
      "Loss: 4.3976297e-06\n",
      "Loss: 4.3942837e-06\n",
      "Loss: 4.392254e-06\n",
      "Loss: 4.3889413e-06\n",
      "Loss: 4.3867485e-06\n",
      "Loss: 4.3842188e-06\n",
      "Loss: 4.378747e-06\n",
      "Loss: 4.37714e-06\n",
      "Loss: 4.3744094e-06\n",
      "Loss: 4.373248e-06\n",
      "Loss: 4.371268e-06\n",
      "Loss: 4.367725e-06\n",
      "Loss: 4.3709297e-06\n",
      "Loss: 4.3660207e-06\n",
      "Loss: 4.3623836e-06\n",
      "Loss: 4.3599352e-06\n",
      "Loss: 4.3587024e-06\n",
      "Loss: 4.357073e-06\n",
      "Loss: 4.355907e-06\n",
      "Loss: 4.3539594e-06\n",
      "Loss: 4.3524287e-06\n",
      "Loss: 4.3511122e-06\n",
      "Loss: 4.348875e-06\n",
      "Loss: 4.347249e-06\n",
      "Loss: 4.3449813e-06\n",
      "Loss: 4.342493e-06\n",
      "Loss: 4.3407395e-06\n",
      "Loss: 4.337593e-06\n",
      "Loss: 4.3354644e-06\n",
      "Loss: 4.338888e-06\n",
      "Loss: 4.332843e-06\n",
      "Loss: 4.33121e-06\n",
      "Loss: 4.3298096e-06\n",
      "Loss: 4.328888e-06\n",
      "Loss: 4.326719e-06\n",
      "Loss: 4.3244495e-06\n",
      "Loss: 4.3213613e-06\n",
      "Loss: 4.3168825e-06\n",
      "Loss: 4.31405e-06\n",
      "Loss: 4.3102496e-06\n",
      "Loss: 4.3076047e-06\n",
      "Loss: 4.3044356e-06\n",
      "Loss: 4.3001783e-06\n",
      "Loss: 4.29633e-06\n",
      "Loss: 4.2935776e-06\n",
      "Loss: 4.292181e-06\n",
      "Loss: 4.29007e-06\n",
      "Loss: 4.2878514e-06\n",
      "Loss: 4.2856996e-06\n",
      "Loss: 4.2832335e-06\n",
      "Loss: 4.2779166e-06\n",
      "Loss: 4.272853e-06\n",
      "Loss: 4.270387e-06\n",
      "Loss: 4.266349e-06\n",
      "Loss: 4.263933e-06\n",
      "Loss: 4.262165e-06\n",
      "Loss: 4.260059e-06\n",
      "Loss: 4.2580195e-06\n",
      "Loss: 4.2569695e-06\n",
      "Loss: 4.254462e-06\n",
      "Loss: 4.253013e-06\n",
      "Loss: 4.251462e-06\n",
      "Loss: 4.2495985e-06\n",
      "Loss: 4.2462357e-06\n",
      "Loss: 4.2464967e-06\n",
      "Loss: 4.24478e-06\n",
      "Loss: 4.2427387e-06\n",
      "Loss: 4.2406755e-06\n",
      "Loss: 4.239206e-06\n",
      "Loss: 4.238061e-06\n",
      "Loss: 4.235214e-06\n",
      "Loss: 4.23422e-06\n",
      "Loss: 4.232361e-06\n",
      "Loss: 4.2304155e-06\n",
      "Loss: 4.2290403e-06\n",
      "Loss: 4.2274532e-06\n",
      "Loss: 4.225224e-06\n",
      "Loss: 4.2237248e-06\n",
      "Loss: 4.2210177e-06\n",
      "Loss: 4.2218026e-06\n",
      "Loss: 4.219748e-06\n",
      "Loss: 4.217619e-06\n",
      "Loss: 4.2157735e-06\n",
      "Loss: 4.2125866e-06\n",
      "Loss: 4.2094093e-06\n",
      "Loss: 4.2065253e-06\n",
      "Loss: 4.203985e-06\n",
      "Loss: 4.2016636e-06\n",
      "Loss: 4.198339e-06\n",
      "Loss: 4.195049e-06\n",
      "Loss: 4.192703e-06\n",
      "Loss: 4.1884828e-06\n",
      "Loss: 4.1862695e-06\n",
      "Loss: 4.1829744e-06\n",
      "Loss: 4.179416e-06\n",
      "Loss: 4.1754324e-06\n",
      "Loss: 4.171793e-06\n",
      "Loss: 4.168178e-06\n",
      "Loss: 4.165703e-06\n",
      "Loss: 4.1623225e-06\n",
      "Loss: 4.161031e-06\n",
      "Loss: 4.1577673e-06\n",
      "Loss: 4.156316e-06\n",
      "Loss: 4.154841e-06\n",
      "Loss: 4.15334e-06\n",
      "Loss: 4.1508183e-06\n",
      "Loss: 4.1476583e-06\n",
      "Loss: 4.1429685e-06\n",
      "Loss: 4.142329e-06\n",
      "Loss: 4.1396697e-06\n",
      "Loss: 4.1369753e-06\n",
      "Loss: 4.135295e-06\n",
      "Loss: 4.1339313e-06\n",
      "Loss: 4.1311177e-06\n",
      "Loss: 4.128782e-06\n",
      "Loss: 4.1260555e-06\n",
      "Loss: 4.1238154e-06\n",
      "Loss: 4.1204967e-06\n",
      "Loss: 4.117599e-06\n",
      "Loss: 4.114068e-06\n",
      "Loss: 4.1274498e-06\n",
      "Loss: 4.1123944e-06\n",
      "Loss: 4.109607e-06\n",
      "Loss: 4.1061908e-06\n",
      "Loss: 4.1041703e-06\n",
      "Loss: 4.1018375e-06\n",
      "Loss: 4.100073e-06\n",
      "Loss: 4.0976865e-06\n",
      "Loss: 4.095697e-06\n",
      "Loss: 4.0945492e-06\n",
      "Loss: 4.091045e-06\n",
      "Loss: 4.0899695e-06\n",
      "Loss: 4.0865e-06\n",
      "Loss: 4.0855853e-06\n",
      "Loss: 4.0819878e-06\n",
      "Loss: 4.0795962e-06\n",
      "Loss: 4.077125e-06\n",
      "Loss: 4.0748732e-06\n",
      "Loss: 4.072637e-06\n",
      "Loss: 4.072024e-06\n",
      "Loss: 4.0669556e-06\n",
      "Loss: 4.065062e-06\n",
      "Loss: 4.063171e-06\n",
      "Loss: 4.0604245e-06\n",
      "Loss: 4.0569953e-06\n",
      "Loss: 4.0545697e-06\n",
      "Loss: 4.052115e-06\n",
      "Loss: 4.0502773e-06\n",
      "Loss: 4.0473724e-06\n",
      "Loss: 4.0440264e-06\n",
      "Loss: 4.0426494e-06\n",
      "Loss: 4.0409113e-06\n",
      "Loss: 4.036523e-06\n",
      "Loss: 4.0374143e-06\n",
      "Loss: 4.0342575e-06\n",
      "Loss: 4.0317072e-06\n",
      "Loss: 4.029656e-06\n",
      "Loss: 4.0274244e-06\n",
      "Loss: 4.025389e-06\n",
      "Loss: 4.0229347e-06\n",
      "Loss: 4.0203904e-06\n",
      "Loss: 4.016083e-06\n",
      "Loss: 4.015461e-06\n",
      "Loss: 4.011815e-06\n",
      "Loss: 4.0107107e-06\n",
      "Loss: 4.008954e-06\n",
      "Loss: 4.0062473e-06\n",
      "Loss: 4.0031546e-06\n",
      "Loss: 4.002469e-06\n",
      "Loss: 3.999519e-06\n",
      "Loss: 3.998514e-06\n",
      "Loss: 3.997407e-06\n",
      "Loss: 3.9948736e-06\n",
      "Loss: 3.9921533e-06\n",
      "Loss: 3.990318e-06\n",
      "Loss: 3.98891e-06\n",
      "Loss: 3.987329e-06\n",
      "Loss: 3.985566e-06\n",
      "Loss: 3.983236e-06\n",
      "Loss: 3.981262e-06\n",
      "Loss: 3.9795395e-06\n",
      "Loss: 3.977545e-06\n",
      "Loss: 3.975704e-06\n",
      "Loss: 3.9734678e-06\n",
      "Loss: 3.9719143e-06\n",
      "Loss: 3.9696606e-06\n",
      "Loss: 3.9668003e-06\n",
      "Loss: 3.9650663e-06\n",
      "Loss: 3.9636957e-06\n",
      "Loss: 3.9627316e-06\n",
      "Loss: 3.9605425e-06\n",
      "Loss: 3.9582783e-06\n",
      "Loss: 3.9555157e-06\n",
      "Loss: 3.9529805e-06\n",
      "Loss: 3.9512793e-06\n",
      "Loss: 3.949417e-06\n",
      "Loss: 3.947702e-06\n",
      "Loss: 3.9456636e-06\n",
      "Loss: 3.943186e-06\n",
      "Loss: 3.9407046e-06\n",
      "Loss: 3.938754e-06\n",
      "Loss: 3.9372203e-06\n",
      "Loss: 3.9357865e-06\n",
      "Loss: 3.9335073e-06\n",
      "Loss: 3.93095e-06\n",
      "Loss: 3.928646e-06\n",
      "Loss: 3.926556e-06\n",
      "Loss: 3.924677e-06\n",
      "Loss: 3.9234196e-06\n",
      "Loss: 3.9203424e-06\n",
      "Loss: 3.9185024e-06\n",
      "Loss: 3.9169263e-06\n",
      "Loss: 3.91487e-06\n",
      "Loss: 3.911547e-06\n",
      "Loss: 3.909614e-06\n",
      "Loss: 3.9063752e-06\n",
      "Loss: 3.9048286e-06\n",
      "Loss: 3.9028737e-06\n",
      "Loss: 3.9012793e-06\n",
      "Loss: 3.8981443e-06\n",
      "Loss: 3.895491e-06\n",
      "Loss: 3.8932085e-06\n",
      "Loss: 3.8908483e-06\n",
      "Loss: 3.888822e-06\n",
      "Loss: 3.8850753e-06\n",
      "Loss: 3.8818303e-06\n",
      "Loss: 3.882975e-06\n",
      "Loss: 3.8794824e-06\n",
      "Loss: 3.876286e-06\n",
      "Loss: 3.874157e-06\n",
      "Loss: 3.872565e-06\n",
      "Loss: 3.870744e-06\n",
      "Loss: 3.8681765e-06\n",
      "Loss: 3.8656895e-06\n",
      "Loss: 3.864369e-06\n",
      "Loss: 3.8618373e-06\n",
      "Loss: 3.8605835e-06\n",
      "Loss: 3.8592093e-06\n",
      "Loss: 3.8574753e-06\n",
      "Loss: 3.855492e-06\n",
      "Loss: 3.8537496e-06\n",
      "Loss: 3.852386e-06\n",
      "Loss: 3.8512653e-06\n",
      "Loss: 3.8492963e-06\n",
      "Loss: 3.847652e-06\n",
      "Loss: 3.8449643e-06\n",
      "Loss: 3.8433964e-06\n",
      "Loss: 3.841882e-06\n",
      "Loss: 3.8405487e-06\n",
      "Loss: 3.839027e-06\n",
      "Loss: 3.835907e-06\n",
      "Loss: 3.8336e-06\n",
      "Loss: 3.8320272e-06\n",
      "Loss: 3.8303447e-06\n",
      "Loss: 3.8290977e-06\n",
      "Loss: 3.8276658e-06\n",
      "Loss: 3.8249796e-06\n",
      "Loss: 3.8322214e-06\n",
      "Loss: 3.823547e-06\n",
      "Loss: 3.8211906e-06\n",
      "Loss: 3.8190287e-06\n",
      "Loss: 3.8171215e-06\n",
      "Loss: 3.8153476e-06\n",
      "Loss: 3.8131723e-06\n",
      "Loss: 3.8107014e-06\n",
      "Loss: 3.8086723e-06\n",
      "Loss: 3.8071232e-06\n",
      "Loss: 3.8047174e-06\n",
      "Loss: 3.8028834e-06\n",
      "Loss: 3.800831e-06\n",
      "Loss: 3.7992857e-06\n",
      "Loss: 3.79716e-06\n",
      "Loss: 3.7963446e-06\n",
      "Loss: 3.7948555e-06\n",
      "Loss: 3.793219e-06\n",
      "Loss: 3.7908321e-06\n",
      "Loss: 3.790156e-06\n",
      "Loss: 3.787618e-06\n",
      "Loss: 3.786176e-06\n",
      "Loss: 3.7853251e-06\n",
      "Loss: 3.783537e-06\n",
      "Loss: 3.782047e-06\n",
      "Loss: 3.7808436e-06\n",
      "Loss: 3.7800323e-06\n",
      "Loss: 3.7788259e-06\n",
      "Loss: 3.7770728e-06\n",
      "Loss: 3.775026e-06\n",
      "Loss: 3.7729844e-06\n",
      "Loss: 3.7716009e-06\n",
      "Loss: 3.770004e-06\n",
      "Loss: 3.7691166e-06\n",
      "Loss: 3.7681652e-06\n",
      "Loss: 3.7671825e-06\n",
      "Loss: 3.7665015e-06\n",
      "Loss: 3.765841e-06\n",
      "Loss: 3.7649297e-06\n",
      "Loss: 3.7640448e-06\n",
      "Loss: 3.7630016e-06\n",
      "Loss: 3.761625e-06\n",
      "Loss: 3.7598677e-06\n",
      "Loss: 3.7595703e-06\n",
      "Loss: 3.7577554e-06\n",
      "Loss: 3.7568511e-06\n",
      "Loss: 3.7563673e-06\n",
      "Loss: 3.7544964e-06\n",
      "Loss: 3.7521338e-06\n",
      "Loss: 3.7517211e-06\n",
      "Loss: 3.7494754e-06\n",
      "Loss: 3.7483267e-06\n",
      "Loss: 3.747322e-06\n",
      "Loss: 3.7455698e-06\n",
      "Loss: 3.743517e-06\n",
      "Loss: 3.7431996e-06\n",
      "Loss: 3.741882e-06\n",
      "Loss: 3.7411005e-06\n",
      "Loss: 3.7400391e-06\n",
      "Loss: 3.7383452e-06\n",
      "Loss: 3.736856e-06\n",
      "Loss: 3.7352022e-06\n",
      "Loss: 3.734147e-06\n",
      "Loss: 3.7325922e-06\n",
      "Loss: 3.7308237e-06\n",
      "Loss: 3.7311243e-06\n",
      "Loss: 3.7296873e-06\n",
      "Loss: 3.7285627e-06\n",
      "Loss: 3.7277098e-06\n",
      "Loss: 3.7267596e-06\n",
      "Loss: 3.7251325e-06\n",
      "Loss: 3.7225846e-06\n",
      "Loss: 3.7222246e-06\n",
      "Loss: 3.719019e-06\n",
      "Loss: 3.7162504e-06\n",
      "Loss: 3.7150958e-06\n",
      "Loss: 3.7123964e-06\n",
      "Loss: 3.7098412e-06\n",
      "Loss: 3.7254158e-06\n",
      "Loss: 3.7085613e-06\n",
      "Loss: 3.706356e-06\n",
      "Loss: 3.704734e-06\n",
      "Loss: 3.7023565e-06\n",
      "Loss: 3.7009218e-06\n",
      "Loss: 3.6993679e-06\n",
      "Loss: 3.6981987e-06\n",
      "Loss: 3.697553e-06\n",
      "Loss: 3.6968327e-06\n",
      "Loss: 3.6960835e-06\n",
      "Loss: 3.6947395e-06\n",
      "Loss: 3.6940664e-06\n",
      "Loss: 3.6930692e-06\n",
      "Loss: 3.6915285e-06\n",
      "Loss: 3.6903698e-06\n",
      "Loss: 3.6886104e-06\n",
      "Loss: 3.6877675e-06\n",
      "Loss: 3.6865981e-06\n",
      "Loss: 3.6840042e-06\n",
      "Loss: 3.684519e-06\n",
      "Loss: 3.6824856e-06\n",
      "Loss: 3.6798497e-06\n",
      "Loss: 3.6777917e-06\n",
      "Loss: 3.675655e-06\n",
      "Loss: 3.674202e-06\n",
      "Loss: 3.6729796e-06\n",
      "Loss: 3.670868e-06\n",
      "Loss: 3.6697393e-06\n",
      "Loss: 3.6680547e-06\n",
      "Loss: 3.6668198e-06\n",
      "Loss: 3.6657457e-06\n",
      "Loss: 3.6633082e-06\n",
      "Loss: 3.6632591e-06\n",
      "Loss: 3.6617137e-06\n",
      "Loss: 3.6594934e-06\n",
      "Loss: 3.6582953e-06\n",
      "Loss: 3.656553e-06\n",
      "Loss: 3.6541792e-06\n",
      "Loss: 3.6565702e-06\n",
      "Loss: 3.6526935e-06\n",
      "Loss: 3.6488732e-06\n",
      "Loss: 3.6460247e-06\n",
      "Loss: 3.6437282e-06\n",
      "Loss: 3.6426336e-06\n",
      "Loss: 3.6409692e-06\n",
      "Loss: 3.6388599e-06\n",
      "Loss: 3.63778e-06\n",
      "Loss: 3.6358088e-06\n",
      "Loss: 3.6336796e-06\n",
      "Loss: 3.6317165e-06\n",
      "Loss: 3.6287624e-06\n",
      "Loss: 3.6263998e-06\n",
      "Loss: 3.6239844e-06\n",
      "Loss: 3.6217093e-06\n",
      "Loss: 3.6197337e-06\n",
      "Loss: 3.6174097e-06\n",
      "Loss: 3.6171682e-06\n",
      "Loss: 3.614816e-06\n",
      "Loss: 3.6136385e-06\n",
      "Loss: 3.6120496e-06\n",
      "Loss: 3.6103363e-06\n",
      "Loss: 3.6081717e-06\n",
      "Loss: 3.606241e-06\n",
      "Loss: 3.6049619e-06\n",
      "Loss: 3.603635e-06\n",
      "Loss: 3.6022966e-06\n",
      "Loss: 3.601096e-06\n",
      "Loss: 3.6001e-06\n",
      "Loss: 3.5994003e-06\n",
      "Loss: 3.5972964e-06\n",
      "Loss: 3.5945309e-06\n",
      "Loss: 3.605492e-06\n",
      "Loss: 3.594127e-06\n",
      "Loss: 3.5924852e-06\n",
      "Loss: 3.5911617e-06\n",
      "Loss: 3.5894002e-06\n",
      "Loss: 3.5878115e-06\n",
      "Loss: 3.5851679e-06\n",
      "Loss: 3.5826808e-06\n",
      "Loss: 3.5808084e-06\n",
      "Loss: 3.578898e-06\n",
      "Loss: 3.5777266e-06\n",
      "Loss: 3.5769144e-06\n",
      "Loss: 3.5754992e-06\n",
      "Loss: 3.5749426e-06\n",
      "Loss: 3.5723865e-06\n",
      "Loss: 3.5716344e-06\n",
      "Loss: 3.5707646e-06\n",
      "Loss: 3.5698117e-06\n",
      "Loss: 3.5682924e-06\n",
      "Loss: 3.5669136e-06\n",
      "Loss: 3.5649923e-06\n",
      "Loss: 3.5629412e-06\n",
      "Loss: 3.56066e-06\n",
      "Loss: 3.5588546e-06\n",
      "Loss: 3.556922e-06\n",
      "Loss: 3.5567768e-06\n",
      "Loss: 3.5561416e-06\n",
      "Loss: 3.5551884e-06\n",
      "Loss: 3.553379e-06\n",
      "Loss: 3.5517987e-06\n",
      "Loss: 3.548972e-06\n",
      "Loss: 3.5557578e-06\n",
      "Loss: 3.5474332e-06\n",
      "Loss: 3.5451121e-06\n",
      "Loss: 3.5426829e-06\n",
      "Loss: 3.5402622e-06\n",
      "Loss: 3.5377607e-06\n",
      "Loss: 3.534989e-06\n",
      "Loss: 3.5332914e-06\n",
      "Loss: 3.5313035e-06\n",
      "Loss: 3.529392e-06\n",
      "Loss: 3.5267801e-06\n",
      "Loss: 3.5246935e-06\n",
      "Loss: 3.5229225e-06\n",
      "Loss: 3.5220091e-06\n",
      "Loss: 3.5201756e-06\n",
      "Loss: 3.5185044e-06\n",
      "Loss: 3.5180626e-06\n",
      "Loss: 3.5167159e-06\n",
      "Loss: 3.5144224e-06\n",
      "Loss: 3.5127275e-06\n",
      "Loss: 3.5109751e-06\n",
      "Loss: 3.5130383e-06\n",
      "Loss: 3.5101607e-06\n",
      "Loss: 3.508826e-06\n",
      "Loss: 3.506889e-06\n",
      "Loss: 3.5061464e-06\n",
      "Loss: 3.5051617e-06\n",
      "Loss: 3.5035864e-06\n",
      "Loss: 3.5021358e-06\n",
      "Loss: 3.4996813e-06\n",
      "Loss: 3.4984253e-06\n",
      "Loss: 3.497029e-06\n",
      "Loss: 3.4960608e-06\n",
      "Loss: 3.4946604e-06\n",
      "Loss: 3.4933473e-06\n",
      "Loss: 3.4916957e-06\n",
      "Loss: 3.4898676e-06\n",
      "Loss: 3.4881296e-06\n",
      "Loss: 3.4867971e-06\n",
      "Loss: 3.4867319e-06\n",
      "Loss: 3.4844345e-06\n",
      "Loss: 3.4837028e-06\n",
      "Loss: 3.4830123e-06\n",
      "Loss: 3.482297e-06\n",
      "Loss: 3.4806092e-06\n",
      "Loss: 3.4810378e-06\n",
      "Loss: 3.4796192e-06\n",
      "Loss: 3.4779255e-06\n",
      "Loss: 3.4767836e-06\n",
      "Loss: 3.4754396e-06\n",
      "Loss: 3.473513e-06\n",
      "Loss: 3.474126e-06\n",
      "Loss: 3.4720347e-06\n",
      "Loss: 3.470549e-06\n",
      "Loss: 3.4692403e-06\n",
      "Loss: 3.4681834e-06\n",
      "Loss: 3.466244e-06\n",
      "Loss: 3.4671625e-06\n",
      "Loss: 3.4650698e-06\n",
      "Loss: 3.4632626e-06\n",
      "Loss: 3.4614638e-06\n",
      "Loss: 3.460002e-06\n",
      "Loss: 3.4590703e-06\n",
      "Loss: 3.456636e-06\n",
      "Loss: 3.455405e-06\n",
      "Loss: 3.4543855e-06\n",
      "Loss: 3.4532713e-06\n",
      "Loss: 3.4509665e-06\n",
      "Loss: 3.4486807e-06\n",
      "Loss: 3.4467075e-06\n",
      "Loss: 3.4456289e-06\n",
      "Loss: 3.4441255e-06\n",
      "Loss: 3.4428463e-06\n",
      "Loss: 3.4410623e-06\n",
      "Loss: 3.4415598e-06\n",
      "Loss: 3.440612e-06\n",
      "Loss: 3.4397153e-06\n",
      "Loss: 3.4380291e-06\n",
      "Loss: 3.436489e-06\n",
      "Loss: 3.4348964e-06\n",
      "Loss: 3.4355776e-06\n",
      "Loss: 3.4339532e-06\n",
      "Loss: 3.4328878e-06\n",
      "Loss: 3.4321085e-06\n",
      "Loss: 3.4307213e-06\n",
      "Loss: 3.4292402e-06\n",
      "Loss: 3.4278726e-06\n",
      "Loss: 3.4272716e-06\n",
      "Loss: 3.4265415e-06\n",
      "Loss: 3.426136e-06\n",
      "Loss: 3.425505e-06\n",
      "Loss: 3.4242537e-06\n",
      "Loss: 3.4224427e-06\n",
      "Loss: 3.4209995e-06\n",
      "Loss: 3.4190186e-06\n",
      "Loss: 3.4178086e-06\n",
      "Loss: 3.4158238e-06\n",
      "Loss: 3.4202858e-06\n",
      "Loss: 3.415355e-06\n",
      "Loss: 3.414214e-06\n",
      "Loss: 3.4125715e-06\n",
      "Loss: 3.4112954e-06\n",
      "Loss: 3.4097861e-06\n",
      "Loss: 3.4100285e-06\n",
      "Loss: 3.4091327e-06\n",
      "Loss: 3.407853e-06\n",
      "Loss: 3.4069656e-06\n",
      "Loss: 3.4058503e-06\n",
      "Loss: 3.4041122e-06\n",
      "Loss: 3.404637e-06\n",
      "Loss: 3.4030772e-06\n",
      "Loss: 3.4010013e-06\n",
      "Loss: 3.3996594e-06\n",
      "Loss: 3.3979436e-06\n",
      "Loss: 3.3965694e-06\n",
      "Loss: 3.3951205e-06\n",
      "Loss: 3.3936258e-06\n",
      "Loss: 3.3925198e-06\n",
      "Loss: 3.3917845e-06\n",
      "Loss: 3.3909214e-06\n",
      "Loss: 3.3894808e-06\n",
      "Loss: 3.3882875e-06\n",
      "Loss: 3.3867148e-06\n",
      "Loss: 3.3854833e-06\n",
      "Loss: 3.3846038e-06\n",
      "Loss: 3.3826118e-06\n",
      "Loss: 3.3832146e-06\n",
      "Loss: 3.3817005e-06\n",
      "Loss: 3.3806768e-06\n",
      "Loss: 3.3796985e-06\n",
      "Loss: 3.378313e-06\n",
      "Loss: 3.376486e-06\n",
      "Loss: 3.375099e-06\n",
      "Loss: 3.3743445e-06\n",
      "Loss: 3.3734086e-06\n",
      "Loss: 3.3715264e-06\n",
      "Loss: 3.3704468e-06\n",
      "Loss: 3.3689191e-06\n",
      "Loss: 3.3681367e-06\n",
      "Loss: 3.3673732e-06\n",
      "Loss: 3.3672559e-06\n",
      "Loss: 3.366206e-06\n",
      "Loss: 3.3647184e-06\n",
      "Loss: 3.3629701e-06\n",
      "Loss: 3.3654367e-06\n",
      "Loss: 3.3623687e-06\n",
      "Loss: 3.361497e-06\n",
      "Loss: 3.3606243e-06\n",
      "Loss: 3.3600045e-06\n",
      "Loss: 3.3588494e-06\n",
      "Loss: 3.3575034e-06\n",
      "Loss: 3.356613e-06\n",
      "Loss: 3.355222e-06\n",
      "Loss: 3.3538067e-06\n",
      "Loss: 3.3526808e-06\n",
      "Loss: 3.3517758e-06\n",
      "Loss: 3.351092e-06\n",
      "Loss: 3.3501e-06\n",
      "Loss: 3.3487045e-06\n",
      "Loss: 3.3467622e-06\n",
      "Loss: 3.3523052e-06\n",
      "Loss: 3.3462316e-06\n",
      "Loss: 3.34469e-06\n",
      "Loss: 3.3438425e-06\n",
      "Loss: 3.3425272e-06\n",
      "Loss: 3.341067e-06\n",
      "Loss: 3.340028e-06\n",
      "Loss: 3.3388806e-06\n",
      "Loss: 3.3381007e-06\n",
      "Loss: 3.33743e-06\n",
      "Loss: 3.335927e-06\n",
      "Loss: 3.3344904e-06\n",
      "Loss: 3.3323256e-06\n",
      "Loss: 3.3315343e-06\n",
      "Loss: 3.3301933e-06\n",
      "Loss: 3.3287229e-06\n",
      "Loss: 3.3273186e-06\n",
      "Loss: 3.3258361e-06\n",
      "Loss: 3.3248562e-06\n",
      "Loss: 3.324032e-06\n",
      "Loss: 3.3221372e-06\n",
      "Loss: 3.3204706e-06\n",
      "Loss: 3.3222e-06\n",
      "Loss: 3.3193292e-06\n",
      "Loss: 3.317467e-06\n",
      "Loss: 3.316953e-06\n",
      "Loss: 3.3150015e-06\n",
      "Loss: 3.3138372e-06\n",
      "Loss: 3.3126894e-06\n",
      "Loss: 3.311817e-06\n",
      "Loss: 3.3105448e-06\n",
      "Loss: 3.3090494e-06\n",
      "Loss: 3.3084652e-06\n",
      "Loss: 3.3073188e-06\n",
      "Loss: 3.306523e-06\n",
      "Loss: 3.30578e-06\n",
      "Loss: 3.3045758e-06\n",
      "Loss: 3.3055144e-06\n",
      "Loss: 3.3037593e-06\n",
      "Loss: 3.3024003e-06\n",
      "Loss: 3.3013062e-06\n",
      "Loss: 3.2999837e-06\n",
      "Loss: 3.2991438e-06\n",
      "Loss: 3.298109e-06\n",
      "Loss: 3.2971748e-06\n",
      "Loss: 3.2962325e-06\n",
      "Loss: 3.2947028e-06\n",
      "Loss: 3.2985977e-06\n",
      "Loss: 3.2941834e-06\n",
      "Loss: 3.2929884e-06\n",
      "Loss: 3.2925084e-06\n",
      "Loss: 3.291605e-06\n",
      "Loss: 3.2908515e-06\n",
      "Loss: 3.290138e-06\n",
      "Loss: 3.2892153e-06\n",
      "Loss: 3.2877565e-06\n",
      "Loss: 3.2866242e-06\n",
      "Loss: 3.2852731e-06\n",
      "Loss: 3.2839143e-06\n",
      "Loss: 3.2823802e-06\n",
      "Loss: 3.28027e-06\n",
      "Loss: 3.2782834e-06\n",
      "Loss: 3.2788555e-06\n",
      "Loss: 3.2774924e-06\n",
      "Loss: 3.2765151e-06\n",
      "Loss: 3.2755092e-06\n",
      "Loss: 3.2748917e-06\n",
      "Loss: 3.2738585e-06\n",
      "Loss: 3.2723813e-06\n",
      "Loss: 3.2717276e-06\n",
      "Loss: 3.2700996e-06\n",
      "Loss: 3.269503e-06\n",
      "Loss: 3.268807e-06\n",
      "Loss: 3.2675803e-06\n",
      "Loss: 3.265942e-06\n",
      "Loss: 3.2644123e-06\n",
      "Loss: 3.2623443e-06\n",
      "Loss: 3.2620555e-06\n",
      "Loss: 3.2608139e-06\n",
      "Loss: 3.2603698e-06\n",
      "Loss: 3.2595417e-06\n",
      "Loss: 3.2588107e-06\n",
      "Loss: 3.2567873e-06\n",
      "Loss: 3.2553367e-06\n",
      "Loss: 3.2547764e-06\n",
      "Loss: 3.2542293e-06\n",
      "Loss: 3.2536423e-06\n",
      "Loss: 3.25246e-06\n",
      "Loss: 3.2520043e-06\n",
      "Loss: 3.25008e-06\n",
      "Loss: 3.249123e-06\n",
      "Loss: 3.2483422e-06\n",
      "Loss: 3.247688e-06\n",
      "Loss: 3.2458656e-06\n",
      "Loss: 3.2436851e-06\n",
      "Loss: 3.2425228e-06\n",
      "Loss: 3.2412686e-06\n",
      "Loss: 3.240418e-06\n",
      "Loss: 3.2393068e-06\n",
      "Loss: 3.2368316e-06\n",
      "Loss: 3.2491e-06\n",
      "Loss: 3.236255e-06\n",
      "Loss: 3.2343123e-06\n",
      "Loss: 3.2333223e-06\n",
      "Loss: 3.232765e-06\n",
      "Loss: 3.2318544e-06\n",
      "Loss: 3.2309981e-06\n",
      "Loss: 3.229859e-06\n",
      "Loss: 3.2287026e-06\n",
      "Loss: 3.227252e-06\n",
      "Loss: 3.2256999e-06\n",
      "Loss: 3.224457e-06\n",
      "Loss: 3.2237567e-06\n",
      "Loss: 3.223413e-06\n",
      "Loss: 3.221232e-06\n",
      "Loss: 3.2203807e-06\n",
      "Loss: 3.2195558e-06\n",
      "Loss: 3.2187936e-06\n",
      "Loss: 3.217787e-06\n",
      "Loss: 3.2161697e-06\n",
      "Loss: 3.214664e-06\n",
      "Loss: 3.2143707e-06\n",
      "Loss: 3.2130606e-06\n",
      "Loss: 3.2124558e-06\n",
      "Loss: 3.2117368e-06\n",
      "Loss: 3.2110365e-06\n",
      "Loss: 3.2096818e-06\n",
      "Loss: 3.2084513e-06\n",
      "Loss: 3.2062226e-06\n",
      "Loss: 3.2053567e-06\n",
      "Loss: 3.204574e-06\n",
      "Loss: 3.20356e-06\n",
      "Loss: 3.2023852e-06\n",
      "Loss: 3.201095e-06\n",
      "Loss: 3.2002386e-06\n",
      "Loss: 3.1990871e-06\n",
      "Loss: 3.197732e-06\n",
      "Loss: 3.1965417e-06\n",
      "Loss: 3.195391e-06\n",
      "Loss: 3.1941438e-06\n",
      "Loss: 3.1925365e-06\n",
      "Loss: 3.1899126e-06\n",
      "Loss: 3.194621e-06\n",
      "Loss: 3.1890013e-06\n",
      "Loss: 3.187449e-06\n",
      "Loss: 3.1868412e-06\n",
      "Loss: 3.1855727e-06\n",
      "Loss: 3.1844418e-06\n",
      "Loss: 3.1829354e-06\n",
      "Loss: 3.1817317e-06\n",
      "Loss: 3.1813593e-06\n",
      "Loss: 3.1798081e-06\n",
      "Loss: 3.1786649e-06\n",
      "Loss: 3.1774566e-06\n",
      "Loss: 3.1757181e-06\n",
      "Loss: 3.1737613e-06\n",
      "Loss: 3.1733623e-06\n",
      "Loss: 3.1724442e-06\n",
      "Loss: 3.1720756e-06\n",
      "Loss: 3.1715485e-06\n",
      "Loss: 3.1708323e-06\n",
      "Loss: 3.1700374e-06\n",
      "Loss: 3.1690506e-06\n",
      "Loss: 3.1681573e-06\n",
      "Loss: 3.16748e-06\n",
      "Loss: 3.1668255e-06\n",
      "Loss: 3.1660768e-06\n",
      "Loss: 3.1651907e-06\n",
      "Loss: 3.1651382e-06\n",
      "Loss: 3.1637696e-06\n",
      "Loss: 3.1632267e-06\n",
      "Loss: 3.1627428e-06\n",
      "Loss: 3.1619463e-06\n",
      "Loss: 3.1608147e-06\n",
      "Loss: 3.1599116e-06\n",
      "Loss: 3.1589316e-06\n",
      "Loss: 3.158325e-06\n",
      "Loss: 3.1573081e-06\n",
      "Loss: 3.155993e-06\n",
      "Loss: 3.1549537e-06\n",
      "Loss: 3.1530667e-06\n",
      "Loss: 3.1513998e-06\n",
      "Loss: 3.150334e-06\n",
      "Loss: 3.1491318e-06\n",
      "Loss: 3.1484249e-06\n",
      "Loss: 3.1475402e-06\n",
      "Loss: 3.1464235e-06\n",
      "Loss: 3.1456716e-06\n",
      "Loss: 3.145007e-06\n",
      "Loss: 3.1446264e-06\n",
      "Loss: 3.1441496e-06\n",
      "Loss: 3.1435525e-06\n",
      "Loss: 3.1424515e-06\n",
      "Loss: 3.1411118e-06\n",
      "Loss: 3.1393881e-06\n",
      "Loss: 3.1395002e-06\n",
      "Loss: 3.1385212e-06\n",
      "Loss: 3.1378813e-06\n",
      "Loss: 3.1369723e-06\n",
      "Loss: 3.1359855e-06\n",
      "Loss: 3.1349532e-06\n",
      "Loss: 3.134128e-06\n",
      "Loss: 3.1331574e-06\n",
      "Loss: 3.1322884e-06\n",
      "Loss: 3.131308e-06\n",
      "Loss: 3.1307538e-06\n",
      "Loss: 3.1300617e-06\n",
      "Loss: 3.1289978e-06\n",
      "Loss: 3.1285995e-06\n",
      "Loss: 3.1272716e-06\n",
      "Loss: 3.1263703e-06\n",
      "Loss: 3.125556e-06\n",
      "Loss: 3.1244977e-06\n",
      "Loss: 3.1237587e-06\n",
      "Loss: 3.1229627e-06\n",
      "Loss: 3.1223117e-06\n",
      "Loss: 3.1217553e-06\n",
      "Loss: 3.1214208e-06\n",
      "Loss: 3.1202699e-06\n",
      "Loss: 3.120948e-06\n",
      "Loss: 3.1196535e-06\n",
      "Loss: 3.1187997e-06\n",
      "Loss: 3.11773e-06\n",
      "Loss: 3.1170293e-06\n",
      "Loss: 3.1165678e-06\n",
      "Loss: 3.1159293e-06\n",
      "Loss: 3.1148686e-06\n",
      "Loss: 3.114156e-06\n",
      "Loss: 3.113319e-06\n",
      "Loss: 3.1118993e-06\n",
      "Loss: 3.1114248e-06\n",
      "Loss: 3.1105153e-06\n",
      "Loss: 3.1096752e-06\n",
      "Loss: 3.1086631e-06\n",
      "Loss: 3.1079471e-06\n",
      "Loss: 3.107462e-06\n",
      "Loss: 3.1068446e-06\n",
      "Loss: 3.105627e-06\n",
      "Loss: 3.1049162e-06\n",
      "Loss: 3.1036675e-06\n",
      "Loss: 3.1029856e-06\n",
      "Loss: 3.1015902e-06\n",
      "Loss: 3.100662e-06\n",
      "Loss: 3.0995357e-06\n",
      "Loss: 3.099237e-06\n",
      "Loss: 3.0975407e-06\n",
      "Loss: 3.0968959e-06\n",
      "Loss: 3.0958313e-06\n",
      "Loss: 3.0946062e-06\n",
      "Loss: 3.0930732e-06\n",
      "Loss: 3.0911895e-06\n",
      "Loss: 3.0905248e-06\n",
      "Loss: 3.0890897e-06\n",
      "Loss: 3.0881768e-06\n",
      "Loss: 3.0873632e-06\n",
      "Loss: 3.0863575e-06\n",
      "Loss: 3.0855153e-06\n",
      "Loss: 3.0842307e-06\n",
      "Loss: 3.0831711e-06\n",
      "Loss: 3.0826425e-06\n",
      "Loss: 3.08157e-06\n",
      "Loss: 3.0805952e-06\n",
      "Loss: 3.0794436e-06\n",
      "Loss: 3.0784624e-06\n",
      "Loss: 3.0772715e-06\n",
      "Loss: 3.076277e-06\n",
      "Loss: 3.075092e-06\n",
      "Loss: 3.073419e-06\n",
      "Loss: 3.0727974e-06\n",
      "Loss: 3.0710564e-06\n",
      "Loss: 3.0695167e-06\n",
      "Loss: 3.0682727e-06\n",
      "Loss: 3.0674173e-06\n",
      "Loss: 3.0666201e-06\n",
      "Loss: 3.065367e-06\n",
      "Loss: 3.0662081e-06\n",
      "Loss: 3.0646388e-06\n",
      "Loss: 3.0635301e-06\n",
      "Loss: 3.0626588e-06\n",
      "Loss: 3.0624349e-06\n",
      "Loss: 3.06171e-06\n",
      "Loss: 3.0612305e-06\n",
      "Loss: 3.0608403e-06\n",
      "Loss: 3.060028e-06\n",
      "Loss: 3.059748e-06\n",
      "Loss: 3.0582146e-06\n",
      "Loss: 3.0575866e-06\n",
      "Loss: 3.056949e-06\n",
      "Loss: 3.0562082e-06\n",
      "Loss: 3.0566252e-06\n",
      "Loss: 3.0555543e-06\n",
      "Loss: 3.0542353e-06\n",
      "Loss: 3.0539902e-06\n",
      "Loss: 3.0528968e-06\n",
      "Loss: 3.0523943e-06\n",
      "Loss: 3.051628e-06\n",
      "Loss: 3.0501487e-06\n",
      "Loss: 3.048919e-06\n",
      "Loss: 3.0468952e-06\n",
      "Loss: 3.0458082e-06\n",
      "Loss: 3.0447352e-06\n",
      "Loss: 3.0435776e-06\n",
      "Loss: 3.0421793e-06\n",
      "Loss: 3.041473e-06\n",
      "Loss: 3.040655e-06\n",
      "Loss: 3.040076e-06\n",
      "Loss: 3.0394458e-06\n",
      "Loss: 3.0384695e-06\n",
      "Loss: 3.0369404e-06\n",
      "Loss: 3.0406322e-06\n",
      "Loss: 3.0362569e-06\n",
      "Loss: 3.0350902e-06\n",
      "Loss: 3.0348483e-06\n",
      "Loss: 3.034347e-06\n",
      "Loss: 3.0336216e-06\n",
      "Loss: 3.0328947e-06\n",
      "Loss: 3.0314263e-06\n",
      "Loss: 3.0323727e-06\n",
      "Loss: 3.0310393e-06\n",
      "Loss: 3.0301817e-06\n",
      "Loss: 3.0294318e-06\n",
      "Loss: 3.0284602e-06\n",
      "Loss: 3.0286697e-06\n",
      "Loss: 3.0279768e-06\n",
      "Loss: 3.0269498e-06\n",
      "Loss: 3.0261517e-06\n",
      "Loss: 3.0255198e-06\n",
      "Loss: 3.024933e-06\n",
      "Loss: 3.023634e-06\n",
      "Loss: 3.0224537e-06\n",
      "Loss: 3.0209033e-06\n",
      "Loss: 3.0198644e-06\n",
      "Loss: 3.0193528e-06\n",
      "Loss: 3.0184783e-06\n",
      "Loss: 3.0177189e-06\n",
      "Loss: 3.017082e-06\n",
      "Loss: 3.016149e-06\n",
      "Loss: 3.0159908e-06\n",
      "Loss: 3.0152119e-06\n",
      "Loss: 3.0142205e-06\n",
      "Loss: 3.013513e-06\n",
      "Loss: 3.012608e-06\n",
      "Loss: 3.0123329e-06\n",
      "Loss: 3.0101983e-06\n",
      "Loss: 3.0096246e-06\n",
      "Loss: 3.0088627e-06\n",
      "Loss: 3.0077113e-06\n",
      "Loss: 3.0068568e-06\n",
      "Loss: 3.0059916e-06\n",
      "Loss: 3.005694e-06\n",
      "Loss: 3.0045812e-06\n",
      "Loss: 3.0028596e-06\n",
      "Loss: 3.001278e-06\n",
      "Loss: 2.9998278e-06\n",
      "Loss: 2.998363e-06\n",
      "Loss: 2.997321e-06\n",
      "Loss: 2.9963944e-06\n",
      "Loss: 2.9956368e-06\n",
      "Loss: 2.9948947e-06\n",
      "Loss: 2.9943208e-06\n",
      "Loss: 2.993388e-06\n",
      "Loss: 2.9929015e-06\n",
      "Loss: 2.9923194e-06\n",
      "Loss: 2.9915097e-06\n",
      "Loss: 2.9903308e-06\n",
      "Loss: 2.9896385e-06\n",
      "Loss: 2.98916e-06\n",
      "Loss: 2.9886296e-06\n",
      "Loss: 2.9881958e-06\n",
      "Loss: 2.987491e-06\n",
      "Loss: 2.986699e-06\n",
      "Loss: 2.9858024e-06\n",
      "Loss: 2.9849675e-06\n",
      "Loss: 2.9844398e-06\n",
      "Loss: 2.9836056e-06\n",
      "Loss: 2.9896569e-06\n",
      "Loss: 2.9832859e-06\n",
      "Loss: 2.9827017e-06\n",
      "Loss: 2.9816883e-06\n",
      "Loss: 2.981165e-06\n",
      "Loss: 2.9801906e-06\n",
      "Loss: 2.9789376e-06\n",
      "Loss: 2.9783541e-06\n",
      "Loss: 2.9776518e-06\n",
      "Loss: 2.9770035e-06\n",
      "Loss: 2.9762648e-06\n",
      "Loss: 2.9746543e-06\n",
      "Loss: 2.9735784e-06\n",
      "Loss: 2.9725434e-06\n",
      "Loss: 2.9717694e-06\n",
      "Loss: 2.9709213e-06\n",
      "Loss: 2.9700545e-06\n",
      "Loss: 2.9706073e-06\n",
      "Loss: 2.9690373e-06\n",
      "Loss: 2.967498e-06\n",
      "Loss: 2.966565e-06\n",
      "Loss: 2.9650494e-06\n",
      "Loss: 2.96456e-06\n",
      "Loss: 2.9636715e-06\n",
      "Loss: 2.9630778e-06\n",
      "Loss: 2.9622713e-06\n",
      "Loss: 2.9613911e-06\n",
      "Loss: 2.9602365e-06\n",
      "Loss: 2.9595103e-06\n",
      "Loss: 2.958282e-06\n",
      "Loss: 2.9571956e-06\n",
      "Loss: 2.955972e-06\n",
      "Loss: 2.9551868e-06\n",
      "Loss: 2.9540015e-06\n",
      "Loss: 2.9531277e-06\n",
      "Loss: 2.9521989e-06\n",
      "Loss: 2.9522378e-06\n",
      "Loss: 2.9515613e-06\n",
      "Loss: 2.9503897e-06\n",
      "Loss: 2.9496869e-06\n",
      "Loss: 2.949163e-06\n",
      "Loss: 2.9485811e-06\n",
      "Loss: 2.947867e-06\n",
      "Loss: 2.9472437e-06\n",
      "Loss: 2.9464527e-06\n",
      "Loss: 2.946094e-06\n",
      "Loss: 2.9448654e-06\n",
      "Loss: 2.9440882e-06\n",
      "Loss: 2.943479e-06\n",
      "Loss: 2.9427383e-06\n",
      "Loss: 2.9430498e-06\n",
      "Loss: 2.941988e-06\n",
      "Loss: 2.9409903e-06\n",
      "Loss: 2.940572e-06\n",
      "Loss: 2.9396128e-06\n",
      "Loss: 2.9388125e-06\n",
      "Loss: 2.937688e-06\n",
      "Loss: 2.9368862e-06\n",
      "Loss: 2.9362668e-06\n",
      "Loss: 2.935455e-06\n",
      "Loss: 2.9343637e-06\n",
      "Loss: 2.933503e-06\n",
      "Loss: 2.932647e-06\n",
      "Loss: 2.9321732e-06\n",
      "Loss: 2.931718e-06\n",
      "Loss: 2.9313337e-06\n",
      "Loss: 2.9306134e-06\n",
      "Loss: 2.9295595e-06\n",
      "Loss: 2.9285766e-06\n",
      "Loss: 2.9269725e-06\n",
      "Loss: 2.926206e-06\n",
      "Loss: 2.9253988e-06\n",
      "Loss: 2.9247808e-06\n",
      "Loss: 2.924206e-06\n",
      "Loss: 2.923327e-06\n",
      "Loss: 2.921654e-06\n",
      "Loss: 2.9207445e-06\n",
      "Loss: 2.919925e-06\n",
      "Loss: 2.9192902e-06\n",
      "Loss: 2.918523e-06\n",
      "Loss: 2.9170922e-06\n",
      "Loss: 2.9177997e-06\n",
      "Loss: 2.9166363e-06\n",
      "Loss: 2.9156715e-06\n",
      "Loss: 2.9142632e-06\n",
      "Loss: 2.9130656e-06\n",
      "Loss: 2.9115836e-06\n",
      "Loss: 2.9094736e-06\n",
      "Loss: 2.9111375e-06\n",
      "Loss: 2.908583e-06\n",
      "Loss: 2.907388e-06\n",
      "Loss: 2.906527e-06\n",
      "Loss: 2.9054104e-06\n",
      "Loss: 2.9037992e-06\n",
      "Loss: 2.9024661e-06\n",
      "Loss: 2.9009107e-06\n",
      "Loss: 2.900128e-06\n",
      "Loss: 2.8995273e-06\n",
      "Loss: 2.8986287e-06\n",
      "Loss: 2.8974052e-06\n",
      "Loss: 2.8962536e-06\n",
      "Loss: 2.8956847e-06\n",
      "Loss: 2.8952782e-06\n",
      "Loss: 2.8948161e-06\n",
      "Loss: 2.8943941e-06\n",
      "Loss: 2.8931877e-06\n",
      "Loss: 2.892169e-06\n",
      "Loss: 2.8913064e-06\n",
      "Loss: 2.8903569e-06\n",
      "Loss: 2.889631e-06\n",
      "Loss: 2.8886368e-06\n",
      "Loss: 2.887723e-06\n",
      "Loss: 2.8866375e-06\n",
      "Loss: 2.885563e-06\n",
      "Loss: 2.8838301e-06\n",
      "Loss: 2.882348e-06\n",
      "Loss: 2.8809168e-06\n",
      "Loss: 2.8802178e-06\n",
      "Loss: 2.879172e-06\n",
      "Loss: 2.8784038e-06\n",
      "Loss: 2.8776817e-06\n",
      "Loss: 2.8764448e-06\n",
      "Loss: 2.8749537e-06\n",
      "Loss: 2.874428e-06\n",
      "Loss: 2.8725121e-06\n",
      "Loss: 2.8714558e-06\n",
      "Loss: 2.8699537e-06\n",
      "Loss: 2.8718105e-06\n",
      "Loss: 2.8693319e-06\n",
      "Loss: 2.868489e-06\n",
      "Loss: 2.8678082e-06\n",
      "Loss: 2.8670304e-06\n",
      "Loss: 2.8658378e-06\n",
      "Loss: 2.864831e-06\n",
      "Loss: 2.8637214e-06\n",
      "Loss: 2.8626073e-06\n",
      "Loss: 2.861867e-06\n",
      "Loss: 2.8611717e-06\n",
      "Loss: 2.8610239e-06\n",
      "Loss: 2.8600796e-06\n",
      "Loss: 2.8586746e-06\n",
      "Loss: 2.8575719e-06\n",
      "Loss: 2.8568936e-06\n",
      "Loss: 2.8558713e-06\n",
      "Loss: 2.8543263e-06\n",
      "Loss: 2.8526988e-06\n",
      "Loss: 2.8536178e-06\n",
      "Loss: 2.851206e-06\n",
      "Loss: 2.8494683e-06\n",
      "Loss: 2.848361e-06\n",
      "Loss: 2.8475397e-06\n",
      "Loss: 2.846408e-06\n",
      "Loss: 2.8454492e-06\n",
      "Loss: 2.8443587e-06\n",
      "Loss: 2.8433387e-06\n",
      "Loss: 2.842211e-06\n",
      "Loss: 2.8414206e-06\n",
      "Loss: 2.8400354e-06\n",
      "Loss: 2.839271e-06\n",
      "Loss: 2.838569e-06\n",
      "Loss: 2.8374204e-06\n",
      "Loss: 2.838468e-06\n",
      "Loss: 2.8367594e-06\n",
      "Loss: 2.835758e-06\n",
      "Loss: 2.8350048e-06\n",
      "Loss: 2.8340114e-06\n",
      "Loss: 2.8335462e-06\n",
      "Loss: 2.832489e-06\n",
      "Loss: 2.8316276e-06\n",
      "Loss: 2.830725e-06\n",
      "Loss: 2.8296797e-06\n",
      "Loss: 2.8286472e-06\n",
      "Loss: 2.8274642e-06\n",
      "Loss: 2.8264215e-06\n",
      "Loss: 2.8255713e-06\n",
      "Loss: 2.8247418e-06\n",
      "Loss: 2.823827e-06\n",
      "Loss: 2.8230018e-06\n",
      "Loss: 2.8221466e-06\n",
      "Loss: 2.8208838e-06\n",
      "Loss: 2.8284378e-06\n",
      "Loss: 2.8204727e-06\n",
      "Loss: 2.8195605e-06\n",
      "Loss: 2.8185227e-06\n",
      "Loss: 2.8172356e-06\n",
      "Loss: 2.8192385e-06\n",
      "Loss: 2.8169814e-06\n",
      "Loss: 2.8162062e-06\n",
      "Loss: 2.8153495e-06\n",
      "Loss: 2.8145787e-06\n",
      "Loss: 2.8135387e-06\n",
      "Loss: 2.8119919e-06\n",
      "Loss: 2.811556e-06\n",
      "Loss: 2.8100458e-06\n",
      "Loss: 2.8094994e-06\n",
      "Loss: 2.8087557e-06\n",
      "Loss: 2.8076197e-06\n",
      "Loss: 2.8096392e-06\n",
      "Loss: 2.8070976e-06\n",
      "Loss: 2.8061907e-06\n",
      "Loss: 2.8054078e-06\n",
      "Loss: 2.804527e-06\n",
      "Loss: 2.803411e-06\n",
      "Loss: 2.8016038e-06\n",
      "Loss: 2.8013856e-06\n",
      "Loss: 2.8005525e-06\n",
      "Loss: 2.7993626e-06\n",
      "Loss: 2.7984124e-06\n",
      "Loss: 2.7978513e-06\n",
      "Loss: 2.7980664e-06\n",
      "Loss: 2.7973701e-06\n",
      "Loss: 2.7966119e-06\n",
      "Loss: 2.7958968e-06\n",
      "Loss: 2.7953442e-06\n",
      "Loss: 2.794557e-06\n",
      "Loss: 2.793066e-06\n",
      "Loss: 2.7948631e-06\n",
      "Loss: 2.7928331e-06\n",
      "Loss: 2.7918986e-06\n",
      "Loss: 2.7912602e-06\n",
      "Loss: 2.7906103e-06\n",
      "Loss: 2.7898063e-06\n",
      "Loss: 2.790268e-06\n",
      "Loss: 2.7894857e-06\n",
      "Loss: 2.7887513e-06\n",
      "Loss: 2.7879928e-06\n",
      "Loss: 2.7875067e-06\n",
      "Loss: 2.7869908e-06\n",
      "Loss: 2.786232e-06\n",
      "Loss: 2.7853403e-06\n",
      "Loss: 2.784895e-06\n",
      "Loss: 2.7844171e-06\n",
      "Loss: 2.783765e-06\n",
      "Loss: 2.7832211e-06\n",
      "Loss: 2.782741e-06\n",
      "Loss: 2.7815558e-06\n",
      "Loss: 2.7809788e-06\n",
      "Loss: 2.7805522e-06\n",
      "Loss: 2.7798128e-06\n",
      "Loss: 2.7790566e-06\n",
      "Loss: 2.7779793e-06\n",
      "Loss: 2.777264e-06\n",
      "Loss: 2.7762785e-06\n",
      "Loss: 2.7776368e-06\n",
      "Loss: 2.7758301e-06\n",
      "Loss: 2.7754168e-06\n",
      "Loss: 2.7744013e-06\n",
      "Loss: 2.7734823e-06\n",
      "Loss: 2.7721671e-06\n",
      "Loss: 2.7709218e-06\n",
      "Loss: 2.7697222e-06\n",
      "Loss: 2.7686192e-06\n",
      "Loss: 2.7674814e-06\n",
      "Loss: 2.7669191e-06\n",
      "Loss: 2.7651804e-06\n",
      "Loss: 2.764861e-06\n",
      "Loss: 2.7635506e-06\n",
      "Loss: 2.7629394e-06\n",
      "Loss: 2.7620615e-06\n",
      "Loss: 2.7605479e-06\n",
      "Loss: 2.758808e-06\n",
      "Loss: 2.758129e-06\n",
      "Loss: 2.7567287e-06\n",
      "Loss: 2.7560523e-06\n",
      "Loss: 2.7551337e-06\n",
      "Loss: 2.7536285e-06\n",
      "Loss: 2.7521348e-06\n",
      "Loss: 2.7513452e-06\n",
      "Loss: 2.7494884e-06\n",
      "Loss: 2.749066e-06\n",
      "Loss: 2.748514e-06\n",
      "Loss: 2.7475025e-06\n",
      "Loss: 2.7513074e-06\n",
      "Loss: 2.7470123e-06\n",
      "Loss: 2.7459796e-06\n",
      "Loss: 2.7453202e-06\n",
      "Loss: 2.7443607e-06\n",
      "Loss: 2.7436536e-06\n",
      "Loss: 2.7424421e-06\n",
      "Loss: 2.7414214e-06\n",
      "Loss: 2.7403134e-06\n",
      "Loss: 2.7402125e-06\n",
      "Loss: 2.7386502e-06\n",
      "Loss: 2.7381416e-06\n",
      "Loss: 2.7373767e-06\n",
      "Loss: 2.7366818e-06\n",
      "Loss: 2.7354517e-06\n",
      "Loss: 2.734459e-06\n",
      "Loss: 2.7339595e-06\n",
      "Loss: 2.7334872e-06\n",
      "Loss: 2.7339695e-06\n",
      "Loss: 2.7332367e-06\n",
      "Loss: 2.73276e-06\n",
      "Loss: 2.7313324e-06\n",
      "Loss: 2.730382e-06\n",
      "Loss: 2.7296735e-06\n",
      "Loss: 2.7289318e-06\n",
      "Loss: 2.728223e-06\n",
      "Loss: 2.7277047e-06\n",
      "Loss: 2.7271249e-06\n",
      "Loss: 2.7266665e-06\n",
      "Loss: 2.725682e-06\n",
      "Loss: 2.724472e-06\n",
      "Loss: 2.7236415e-06\n",
      "Loss: 2.721673e-06\n",
      "Loss: 2.7208764e-06\n",
      "Loss: 2.719698e-06\n",
      "Loss: 2.71829e-06\n",
      "Loss: 2.7166639e-06\n",
      "Loss: 2.715355e-06\n",
      "Loss: 2.7145268e-06\n",
      "Loss: 2.7141537e-06\n",
      "Loss: 2.713025e-06\n",
      "Loss: 2.711255e-06\n",
      "Loss: 2.7099277e-06\n",
      "Loss: 2.7088947e-06\n",
      "Loss: 2.7085207e-06\n",
      "Loss: 2.7072351e-06\n",
      "Loss: 2.7063356e-06\n",
      "Loss: 2.7053486e-06\n",
      "Loss: 2.7044064e-06\n",
      "Loss: 2.7031115e-06\n",
      "Loss: 2.7020974e-06\n",
      "Loss: 2.7011756e-06\n",
      "Loss: 2.7000626e-06\n",
      "Loss: 2.6989314e-06\n",
      "Loss: 2.6988087e-06\n",
      "Loss: 2.6978094e-06\n",
      "Loss: 2.6965752e-06\n",
      "Loss: 2.6962161e-06\n",
      "Loss: 2.695508e-06\n",
      "Loss: 2.6945088e-06\n",
      "Loss: 2.693734e-06\n",
      "Loss: 2.6931084e-06\n",
      "Loss: 2.6922035e-06\n",
      "Loss: 2.691257e-06\n",
      "Loss: 2.6900736e-06\n",
      "Loss: 2.6889193e-06\n",
      "Loss: 2.6879663e-06\n",
      "Loss: 2.686594e-06\n",
      "Loss: 2.687908e-06\n",
      "Loss: 2.686152e-06\n",
      "Loss: 2.6854773e-06\n",
      "Loss: 2.6849484e-06\n",
      "Loss: 2.6836979e-06\n",
      "Loss: 2.6829011e-06\n",
      "Loss: 2.6819303e-06\n",
      "Loss: 2.6812304e-06\n",
      "Loss: 2.6805449e-06\n",
      "Loss: 2.6796179e-06\n",
      "Loss: 2.6784412e-06\n",
      "Loss: 2.6775515e-06\n",
      "Loss: 2.6768435e-06\n",
      "Loss: 2.6764712e-06\n",
      "Loss: 2.6762414e-06\n",
      "Loss: 2.6758144e-06\n",
      "Loss: 2.6749935e-06\n",
      "Loss: 2.6743478e-06\n",
      "Loss: 2.6734829e-06\n",
      "Loss: 2.6724515e-06\n",
      "Loss: 2.6719854e-06\n",
      "Loss: 2.6711637e-06\n",
      "Loss: 2.670332e-06\n",
      "Loss: 2.6697082e-06\n",
      "Loss: 2.6686203e-06\n",
      "Loss: 2.6680923e-06\n",
      "Loss: 2.667242e-06\n",
      "Loss: 2.6663047e-06\n",
      "Loss: 2.6654216e-06\n",
      "Loss: 2.6643024e-06\n",
      "Loss: 2.6630732e-06\n",
      "Loss: 2.661785e-06\n",
      "Loss: 2.6606328e-06\n",
      "Loss: 2.6588175e-06\n",
      "Loss: 2.6580594e-06\n",
      "Loss: 2.6565333e-06\n",
      "Loss: 2.6569078e-06\n",
      "Loss: 2.6560617e-06\n",
      "Loss: 2.6553187e-06\n",
      "Loss: 2.6544344e-06\n",
      "Loss: 2.6534842e-06\n",
      "Loss: 2.652484e-06\n",
      "Loss: 2.6510984e-06\n",
      "Loss: 2.6500818e-06\n",
      "Loss: 2.649436e-06\n",
      "Loss: 2.6482378e-06\n",
      "Loss: 2.6456337e-06\n",
      "Loss: 2.6530047e-06\n",
      "Loss: 2.6450075e-06\n",
      "Loss: 2.6441016e-06\n",
      "Loss: 2.6433656e-06\n",
      "Loss: 2.6432942e-06\n",
      "Loss: 2.643112e-06\n",
      "Loss: 2.6425078e-06\n",
      "Loss: 2.6417306e-06\n",
      "Loss: 2.6411153e-06\n",
      "Loss: 2.6400257e-06\n",
      "Loss: 2.6394007e-06\n",
      "Loss: 2.638687e-06\n",
      "Loss: 2.6383284e-06\n",
      "Loss: 2.6378918e-06\n",
      "Loss: 2.6366179e-06\n",
      "Loss: 2.635959e-06\n",
      "Loss: 2.6343912e-06\n",
      "Loss: 2.6338262e-06\n",
      "Loss: 2.6326402e-06\n",
      "Loss: 2.6315108e-06\n",
      "Loss: 2.631189e-06\n",
      "Loss: 2.629794e-06\n",
      "Loss: 2.6294108e-06\n",
      "Loss: 2.6290372e-06\n",
      "Loss: 2.6286448e-06\n",
      "Loss: 2.6282041e-06\n",
      "Loss: 2.6271591e-06\n",
      "Loss: 2.6267776e-06\n",
      "Loss: 2.625971e-06\n",
      "Loss: 2.6253658e-06\n",
      "Loss: 2.6244716e-06\n",
      "Loss: 2.6232378e-06\n",
      "Loss: 2.6267599e-06\n",
      "Loss: 2.6227608e-06\n",
      "Loss: 2.6215087e-06\n",
      "Loss: 2.6204107e-06\n",
      "Loss: 2.6195803e-06\n",
      "Loss: 2.618817e-06\n",
      "Loss: 2.6175712e-06\n",
      "Loss: 2.6172665e-06\n",
      "Loss: 2.6159144e-06\n",
      "Loss: 2.6153882e-06\n",
      "Loss: 2.6149746e-06\n",
      "Loss: 2.614283e-06\n",
      "Loss: 2.6134749e-06\n",
      "Loss: 2.6126302e-06\n",
      "Loss: 2.6119176e-06\n",
      "Loss: 2.6113269e-06\n",
      "Loss: 2.6109306e-06\n",
      "Loss: 2.6098649e-06\n",
      "Loss: 2.6097769e-06\n",
      "Loss: 2.6091254e-06\n",
      "Loss: 2.6086695e-06\n",
      "Loss: 2.6078865e-06\n",
      "Loss: 2.6071802e-06\n",
      "Loss: 2.6061532e-06\n",
      "Loss: 2.6060686e-06\n",
      "Loss: 2.6054868e-06\n",
      "Loss: 2.6042574e-06\n",
      "Loss: 2.6036955e-06\n",
      "Loss: 2.6031723e-06\n",
      "Loss: 2.6026814e-06\n",
      "Loss: 2.6022985e-06\n",
      "Loss: 2.601537e-06\n",
      "Loss: 2.6011521e-06\n",
      "Loss: 2.5996817e-06\n",
      "Loss: 2.599567e-06\n",
      "Loss: 2.5984123e-06\n",
      "Loss: 2.59806e-06\n",
      "Loss: 2.597375e-06\n",
      "Loss: 2.5965087e-06\n",
      "Loss: 2.596158e-06\n",
      "Loss: 2.595174e-06\n",
      "Loss: 2.594706e-06\n",
      "Loss: 2.5937302e-06\n",
      "Loss: 2.5923036e-06\n",
      "Loss: 2.59369e-06\n",
      "Loss: 2.5919892e-06\n",
      "Loss: 2.5910506e-06\n",
      "Loss: 2.5904487e-06\n",
      "Loss: 2.589852e-06\n",
      "Loss: 2.5894553e-06\n",
      "Loss: 2.5888944e-06\n",
      "Loss: 2.5883583e-06\n",
      "Loss: 2.5881131e-06\n",
      "Loss: 2.5873105e-06\n",
      "Loss: 2.5866518e-06\n",
      "Loss: 2.585486e-06\n",
      "Loss: 2.5848276e-06\n",
      "Loss: 2.5841798e-06\n",
      "Loss: 2.583968e-06\n",
      "Loss: 2.5828658e-06\n",
      "Loss: 2.5823445e-06\n",
      "Loss: 2.5819138e-06\n",
      "Loss: 2.5812406e-06\n",
      "Loss: 2.5801646e-06\n",
      "Loss: 2.5804352e-06\n",
      "Loss: 2.5794561e-06\n",
      "Loss: 2.5786835e-06\n",
      "Loss: 2.5781706e-06\n",
      "Loss: 2.5771317e-06\n",
      "Loss: 2.5765257e-06\n",
      "Loss: 2.5747242e-06\n",
      "Loss: 2.5740144e-06\n",
      "Loss: 2.5727277e-06\n",
      "Loss: 2.576086e-06\n",
      "Loss: 2.5724348e-06\n",
      "Loss: 2.5718614e-06\n",
      "Loss: 2.571148e-06\n",
      "Loss: 2.5700774e-06\n",
      "Loss: 2.5685888e-06\n",
      "Loss: 2.566557e-06\n",
      "Loss: 2.5744077e-06\n",
      "Loss: 2.5659876e-06\n",
      "Loss: 2.5652664e-06\n",
      "Loss: 2.5644076e-06\n",
      "Loss: 2.5637873e-06\n",
      "Loss: 2.5630425e-06\n",
      "Loss: 2.5623099e-06\n",
      "Loss: 2.5614786e-06\n",
      "Loss: 2.5604643e-06\n",
      "Loss: 2.5590668e-06\n",
      "Loss: 2.5592276e-06\n",
      "Loss: 2.55873e-06\n",
      "Loss: 2.5584377e-06\n",
      "Loss: 2.5577826e-06\n",
      "Loss: 2.5571048e-06\n",
      "Loss: 2.556106e-06\n",
      "Loss: 2.5545366e-06\n",
      "Loss: 2.552939e-06\n",
      "Loss: 2.5525326e-06\n",
      "Loss: 2.5516447e-06\n",
      "Loss: 2.5511383e-06\n",
      "Loss: 2.5505028e-06\n",
      "Loss: 2.5498136e-06\n",
      "Loss: 2.5488455e-06\n",
      "Loss: 2.5479667e-06\n",
      "Loss: 2.5470924e-06\n",
      "Loss: 2.5465235e-06\n",
      "Loss: 2.545974e-06\n",
      "Loss: 2.5451093e-06\n",
      "Loss: 2.5445986e-06\n",
      "Loss: 2.5431918e-06\n",
      "Loss: 2.542046e-06\n",
      "Loss: 2.5411005e-06\n",
      "Loss: 2.5406516e-06\n",
      "Loss: 2.540192e-06\n",
      "Loss: 2.5395798e-06\n",
      "Loss: 2.53852e-06\n",
      "Loss: 2.5403292e-06\n",
      "Loss: 2.5380914e-06\n",
      "Loss: 2.5375439e-06\n",
      "Loss: 2.5363995e-06\n",
      "Loss: 2.5351965e-06\n",
      "Loss: 2.5336794e-06\n",
      "Loss: 2.5314994e-06\n",
      "Loss: 2.5307343e-06\n",
      "Loss: 2.5290356e-06\n",
      "Loss: 2.5286388e-06\n",
      "Loss: 2.5280492e-06\n",
      "Loss: 2.527436e-06\n",
      "Loss: 2.5272025e-06\n",
      "Loss: 2.5264271e-06\n",
      "Loss: 2.526091e-06\n",
      "Loss: 2.5256381e-06\n",
      "Loss: 2.5250201e-06\n",
      "Loss: 2.5239433e-06\n",
      "Loss: 2.5229228e-06\n",
      "Loss: 2.5223499e-06\n",
      "Loss: 2.5217867e-06\n",
      "Loss: 2.5214053e-06\n",
      "Loss: 2.5209567e-06\n",
      "Loss: 2.5205713e-06\n",
      "Loss: 2.5198806e-06\n",
      "Loss: 2.5193713e-06\n",
      "Loss: 2.518953e-06\n",
      "Loss: 2.5184304e-06\n",
      "Loss: 2.5182633e-06\n",
      "Loss: 2.517321e-06\n",
      "Loss: 2.517013e-06\n",
      "Loss: 2.516425e-06\n",
      "Loss: 2.5154318e-06\n",
      "Loss: 2.5141974e-06\n",
      "Loss: 2.5138934e-06\n",
      "Loss: 2.512665e-06\n",
      "Loss: 2.512145e-06\n",
      "Loss: 2.5111683e-06\n",
      "Loss: 2.5103247e-06\n",
      "Loss: 2.5106835e-06\n",
      "Loss: 2.509681e-06\n",
      "Loss: 2.5087247e-06\n",
      "Loss: 2.508133e-06\n",
      "Loss: 2.5073732e-06\n",
      "Loss: 2.5069269e-06\n",
      "Loss: 2.5065144e-06\n",
      "Loss: 2.505885e-06\n",
      "Loss: 2.5052782e-06\n",
      "Loss: 2.504567e-06\n",
      "Loss: 2.5044005e-06\n",
      "Loss: 2.5037245e-06\n",
      "Loss: 2.5031345e-06\n",
      "Loss: 2.5026588e-06\n",
      "Loss: 2.5022691e-06\n",
      "Loss: 2.5011745e-06\n",
      "Loss: 2.5026648e-06\n",
      "Loss: 2.5005384e-06\n",
      "Loss: 2.499522e-06\n",
      "Loss: 2.4984543e-06\n",
      "Loss: 2.4976111e-06\n",
      "Loss: 2.4967985e-06\n",
      "Loss: 2.4957096e-06\n",
      "Loss: 2.4951983e-06\n",
      "Loss: 2.4939704e-06\n",
      "Loss: 2.4932642e-06\n",
      "Loss: 2.492892e-06\n",
      "Loss: 2.4917272e-06\n",
      "Loss: 2.4922622e-06\n",
      "Loss: 2.4914386e-06\n",
      "Loss: 2.4910585e-06\n",
      "Loss: 2.4906635e-06\n",
      "Loss: 2.4902256e-06\n",
      "Loss: 2.4895676e-06\n",
      "Loss: 2.4886358e-06\n",
      "Loss: 2.488076e-06\n",
      "Loss: 2.4870278e-06\n",
      "Loss: 2.486729e-06\n",
      "Loss: 2.486375e-06\n",
      "Loss: 2.4858614e-06\n",
      "Loss: 2.4851497e-06\n",
      "Loss: 2.4845933e-06\n",
      "Loss: 2.4841156e-06\n",
      "Loss: 2.4832916e-06\n",
      "Loss: 2.482643e-06\n",
      "Loss: 2.481813e-06\n",
      "Loss: 2.4806532e-06\n",
      "Loss: 2.4800902e-06\n",
      "Loss: 2.4795281e-06\n",
      "Loss: 2.4788217e-06\n",
      "Loss: 2.4771389e-06\n",
      "Loss: 2.482598e-06\n",
      "Loss: 2.4767164e-06\n",
      "Loss: 2.4756569e-06\n",
      "Loss: 2.4749647e-06\n",
      "Loss: 2.4742915e-06\n",
      "Loss: 2.473462e-06\n",
      "Loss: 2.4728806e-06\n",
      "Loss: 2.4722208e-06\n",
      "Loss: 2.4717378e-06\n",
      "Loss: 2.4711908e-06\n",
      "Loss: 2.4704843e-06\n",
      "Loss: 2.4698961e-06\n",
      "Loss: 2.469289e-06\n",
      "Loss: 2.4686979e-06\n",
      "Loss: 2.4677236e-06\n",
      "Loss: 2.4667902e-06\n",
      "Loss: 2.4659184e-06\n",
      "Loss: 2.465218e-06\n",
      "Loss: 2.4641772e-06\n",
      "Loss: 2.4635265e-06\n",
      "Loss: 2.4625033e-06\n",
      "Loss: 2.4617423e-06\n",
      "Loss: 2.460928e-06\n",
      "Loss: 2.4604528e-06\n",
      "Loss: 2.459368e-06\n",
      "Loss: 2.458533e-06\n",
      "Loss: 2.4570388e-06\n",
      "Loss: 2.4556657e-06\n",
      "Loss: 2.4550075e-06\n",
      "Loss: 2.4542828e-06\n",
      "Loss: 2.4535966e-06\n",
      "Loss: 2.4526862e-06\n",
      "Loss: 2.4527712e-06\n",
      "Loss: 2.45225e-06\n",
      "Loss: 2.4517221e-06\n",
      "Loss: 2.451064e-06\n",
      "Loss: 2.450383e-06\n",
      "Loss: 2.4493027e-06\n",
      "Loss: 2.448865e-06\n",
      "Loss: 2.447833e-06\n",
      "Loss: 2.447478e-06\n",
      "Loss: 2.4467581e-06\n",
      "Loss: 2.4459086e-06\n",
      "Loss: 2.4455385e-06\n",
      "Loss: 2.444951e-06\n",
      "Loss: 2.4445187e-06\n",
      "Loss: 2.4438182e-06\n",
      "Loss: 2.443082e-06\n",
      "Loss: 2.4422056e-06\n",
      "Loss: 2.4416743e-06\n",
      "Loss: 2.4411074e-06\n",
      "Loss: 2.4399405e-06\n",
      "Loss: 2.4382857e-06\n",
      "Loss: 2.4385836e-06\n",
      "Loss: 2.4375568e-06\n",
      "Loss: 2.4367494e-06\n",
      "Loss: 2.436099e-06\n",
      "Loss: 2.435473e-06\n",
      "Loss: 2.434274e-06\n",
      "Loss: 2.4331034e-06\n",
      "Loss: 2.432821e-06\n",
      "Loss: 2.4316982e-06\n",
      "Loss: 2.4312653e-06\n",
      "Loss: 2.430774e-06\n",
      "Loss: 2.4300107e-06\n",
      "Loss: 2.4291749e-06\n",
      "Loss: 2.4286035e-06\n",
      "Loss: 2.4279343e-06\n",
      "Loss: 2.4275255e-06\n",
      "Loss: 2.4266676e-06\n",
      "Loss: 2.4260685e-06\n",
      "Loss: 2.4254791e-06\n",
      "Loss: 2.4249298e-06\n",
      "Loss: 2.4245487e-06\n",
      "Loss: 2.4236124e-06\n",
      "Loss: 2.4228952e-06\n",
      "Loss: 2.4224198e-06\n",
      "Loss: 2.421783e-06\n",
      "Loss: 2.4209885e-06\n",
      "Loss: 2.419771e-06\n",
      "Loss: 2.423521e-06\n",
      "Loss: 2.419353e-06\n",
      "Loss: 2.4182568e-06\n",
      "Loss: 2.4175088e-06\n",
      "Loss: 2.4160654e-06\n",
      "Loss: 2.4146943e-06\n",
      "Loss: 2.4139429e-06\n",
      "Loss: 2.412775e-06\n",
      "Loss: 2.4122899e-06\n",
      "Loss: 2.411403e-06\n",
      "Loss: 2.410264e-06\n",
      "Loss: 2.4090054e-06\n",
      "Loss: 2.4083386e-06\n",
      "Loss: 2.4071517e-06\n",
      "Loss: 2.4066196e-06\n",
      "Loss: 2.4059498e-06\n",
      "Loss: 2.405297e-06\n",
      "Loss: 2.4044596e-06\n",
      "Loss: 2.4038845e-06\n",
      "Loss: 2.4033366e-06\n",
      "Loss: 2.4028132e-06\n",
      "Loss: 2.402473e-06\n",
      "Loss: 2.4017136e-06\n",
      "Loss: 2.401509e-06\n",
      "Loss: 2.4007259e-06\n",
      "Loss: 2.4004737e-06\n",
      "Loss: 2.3998834e-06\n",
      "Loss: 2.3992561e-06\n",
      "Loss: 2.3980656e-06\n",
      "Loss: 2.397535e-06\n",
      "Loss: 2.3968134e-06\n",
      "Loss: 2.3962164e-06\n",
      "Loss: 2.3957075e-06\n",
      "Loss: 2.3945581e-06\n",
      "Loss: 2.393877e-06\n",
      "Loss: 2.3933349e-06\n",
      "Loss: 2.3923335e-06\n",
      "Loss: 2.3918437e-06\n",
      "Loss: 2.3912617e-06\n",
      "Loss: 2.3906189e-06\n",
      "Loss: 2.3900593e-06\n",
      "Loss: 2.3897064e-06\n",
      "Loss: 2.3893324e-06\n",
      "Loss: 2.388525e-06\n",
      "Loss: 2.3880427e-06\n",
      "Loss: 2.3871314e-06\n",
      "Loss: 2.3867985e-06\n",
      "Loss: 2.386208e-06\n",
      "Loss: 2.3854116e-06\n",
      "Loss: 2.385082e-06\n",
      "Loss: 2.3847902e-06\n",
      "Loss: 2.3836467e-06\n",
      "Loss: 2.3832233e-06\n",
      "Loss: 2.3827747e-06\n",
      "Loss: 2.382367e-06\n",
      "Loss: 2.381148e-06\n",
      "Loss: 2.3836762e-06\n",
      "Loss: 2.3807809e-06\n",
      "Loss: 2.3800276e-06\n",
      "Loss: 2.379345e-06\n",
      "Loss: 2.3785726e-06\n",
      "Loss: 2.3778189e-06\n",
      "Loss: 2.377365e-06\n",
      "Loss: 2.3768496e-06\n",
      "Loss: 2.3765224e-06\n",
      "Loss: 2.3768152e-06\n",
      "Loss: 2.376055e-06\n",
      "Loss: 2.3754183e-06\n",
      "Loss: 2.3750567e-06\n",
      "Loss: 2.3747248e-06\n",
      "Loss: 2.373258e-06\n",
      "Loss: 2.3723017e-06\n",
      "Loss: 2.372647e-06\n",
      "Loss: 2.3719588e-06\n",
      "Loss: 2.3717191e-06\n",
      "Loss: 2.3713858e-06\n",
      "Loss: 2.3711143e-06\n",
      "Loss: 2.370498e-06\n",
      "Loss: 2.3697726e-06\n",
      "Loss: 2.3689636e-06\n",
      "Loss: 2.368913e-06\n",
      "Loss: 2.3680068e-06\n",
      "Loss: 2.3676694e-06\n",
      "Loss: 2.367278e-06\n",
      "Loss: 2.3667797e-06\n",
      "Loss: 2.3660825e-06\n",
      "Loss: 2.365162e-06\n",
      "Loss: 2.3646464e-06\n",
      "Loss: 2.3641232e-06\n",
      "Loss: 2.3633793e-06\n",
      "Loss: 2.3627333e-06\n",
      "Loss: 2.362245e-06\n",
      "Loss: 2.3619361e-06\n",
      "Loss: 2.3610405e-06\n",
      "Loss: 2.3603543e-06\n",
      "Loss: 2.3599341e-06\n",
      "Loss: 2.359477e-06\n",
      "Loss: 2.3589764e-06\n",
      "Loss: 2.3585949e-06\n",
      "Loss: 2.3580994e-06\n",
      "Loss: 2.35756e-06\n",
      "Loss: 2.356325e-06\n",
      "Loss: 2.3561001e-06\n",
      "Loss: 2.3551224e-06\n",
      "Loss: 2.3548737e-06\n",
      "Loss: 2.3540365e-06\n",
      "Loss: 2.3531798e-06\n",
      "Loss: 2.3520931e-06\n",
      "Loss: 2.3542284e-06\n",
      "Loss: 2.3517903e-06\n",
      "Loss: 2.3512312e-06\n",
      "Loss: 2.3507012e-06\n",
      "Loss: 2.350289e-06\n",
      "Loss: 2.3503146e-06\n",
      "Loss: 2.3499285e-06\n",
      "Loss: 2.3493926e-06\n",
      "Loss: 2.3488392e-06\n",
      "Loss: 2.3485177e-06\n",
      "Loss: 2.3479965e-06\n",
      "Loss: 2.3470054e-06\n",
      "Loss: 2.346284e-06\n",
      "Loss: 2.3457974e-06\n",
      "Loss: 2.3454813e-06\n",
      "Loss: 2.345135e-06\n",
      "Loss: 2.3444672e-06\n",
      "Loss: 2.3437165e-06\n",
      "Loss: 2.3443536e-06\n",
      "Loss: 2.3435678e-06\n",
      "Loss: 2.3429757e-06\n",
      "Loss: 2.3424932e-06\n",
      "Loss: 2.341904e-06\n",
      "Loss: 2.341578e-06\n",
      "Loss: 2.340826e-06\n",
      "Loss: 2.339997e-06\n",
      "Loss: 2.3393304e-06\n",
      "Loss: 2.3388134e-06\n",
      "Loss: 2.3382904e-06\n",
      "Loss: 2.3380087e-06\n",
      "Loss: 2.3376292e-06\n",
      "Loss: 2.3373389e-06\n",
      "Loss: 2.336574e-06\n",
      "Loss: 2.335805e-06\n",
      "Loss: 2.335077e-06\n",
      "Loss: 2.3345244e-06\n",
      "Loss: 2.3341158e-06\n",
      "Loss: 2.3338202e-06\n",
      "Loss: 2.3332304e-06\n",
      "Loss: 2.3325156e-06\n",
      "Loss: 2.3323562e-06\n",
      "Loss: 2.3317725e-06\n",
      "Loss: 2.331398e-06\n",
      "Loss: 2.3309194e-06\n",
      "Loss: 2.330573e-06\n",
      "Loss: 2.3302441e-06\n",
      "Loss: 2.329673e-06\n",
      "Loss: 2.3291377e-06\n",
      "Loss: 2.3287198e-06\n",
      "Loss: 2.327878e-06\n",
      "Loss: 2.3272003e-06\n",
      "Loss: 2.326555e-06\n",
      "Loss: 2.3260066e-06\n",
      "Loss: 2.3250586e-06\n",
      "Loss: 2.3243688e-06\n",
      "Loss: 2.3237822e-06\n",
      "Loss: 2.3232287e-06\n",
      "Loss: 2.3223477e-06\n",
      "Loss: 2.3217833e-06\n",
      "Loss: 2.3224936e-06\n",
      "Loss: 2.3214016e-06\n",
      "Loss: 2.3208413e-06\n",
      "Loss: 2.3199361e-06\n",
      "Loss: 2.3189684e-06\n",
      "Loss: 2.3184618e-06\n",
      "Loss: 2.3180328e-06\n",
      "Loss: 2.3172752e-06\n",
      "Loss: 2.3161379e-06\n",
      "Loss: 2.3159012e-06\n",
      "Loss: 2.3150637e-06\n",
      "Loss: 2.3147904e-06\n",
      "Loss: 2.3144314e-06\n",
      "Loss: 2.3139933e-06\n",
      "Loss: 2.3135983e-06\n",
      "Loss: 2.3133196e-06\n",
      "Loss: 2.3128468e-06\n",
      "Loss: 2.312465e-06\n",
      "Loss: 2.3118464e-06\n",
      "Loss: 2.311106e-06\n",
      "Loss: 2.310611e-06\n",
      "Loss: 2.3100008e-06\n",
      "Loss: 2.309348e-06\n",
      "Loss: 2.3135506e-06\n",
      "Loss: 2.3090224e-06\n",
      "Loss: 2.3084049e-06\n",
      "Loss: 2.3078346e-06\n",
      "Loss: 2.307117e-06\n",
      "Loss: 2.306554e-06\n",
      "Loss: 2.3073364e-06\n",
      "Loss: 2.3061748e-06\n",
      "Loss: 2.305413e-06\n",
      "Loss: 2.3048499e-06\n",
      "Loss: 2.3043021e-06\n",
      "Loss: 2.3037508e-06\n",
      "Loss: 2.3048526e-06\n",
      "Loss: 2.3034495e-06\n",
      "Loss: 2.3027192e-06\n",
      "Loss: 2.3022e-06\n",
      "Loss: 2.3019172e-06\n",
      "Loss: 2.3015784e-06\n",
      "Loss: 2.3012228e-06\n",
      "Loss: 2.3007788e-06\n",
      "Loss: 2.300167e-06\n",
      "Loss: 2.302232e-06\n",
      "Loss: 2.2999711e-06\n",
      "Loss: 2.299441e-06\n",
      "Loss: 2.29896e-06\n",
      "Loss: 2.2986244e-06\n",
      "Loss: 2.297874e-06\n",
      "Loss: 2.2975278e-06\n",
      "Loss: 2.2969807e-06\n",
      "Loss: 2.2964555e-06\n",
      "Loss: 2.2959246e-06\n",
      "Loss: 2.295372e-06\n",
      "Loss: 2.2947993e-06\n",
      "Loss: 2.293994e-06\n",
      "Loss: 2.2932809e-06\n",
      "Loss: 2.2939275e-06\n",
      "Loss: 2.2929557e-06\n",
      "Loss: 2.292314e-06\n",
      "Loss: 2.2917602e-06\n",
      "Loss: 2.2914896e-06\n",
      "Loss: 2.290844e-06\n",
      "Loss: 2.2900965e-06\n",
      "Loss: 2.2891784e-06\n",
      "Loss: 2.2894562e-06\n",
      "Loss: 2.2887182e-06\n",
      "Loss: 2.2878965e-06\n",
      "Loss: 2.2873455e-06\n",
      "Loss: 2.2870147e-06\n",
      "Loss: 2.2864003e-06\n",
      "Loss: 2.2857064e-06\n",
      "Loss: 2.2853167e-06\n",
      "Loss: 2.2849601e-06\n",
      "Loss: 2.2846548e-06\n",
      "Loss: 2.284257e-06\n",
      "Loss: 2.2835538e-06\n",
      "Loss: 2.2830882e-06\n",
      "Loss: 2.282515e-06\n",
      "Loss: 2.2820784e-06\n",
      "Loss: 2.2814675e-06\n",
      "Loss: 2.2806475e-06\n",
      "Loss: 2.281101e-06\n",
      "Loss: 2.2803454e-06\n",
      "Loss: 2.2792476e-06\n",
      "Loss: 2.27877e-06\n",
      "Loss: 2.2782356e-06\n",
      "Loss: 2.2778695e-06\n",
      "Loss: 2.2776155e-06\n",
      "Loss: 2.2770657e-06\n",
      "Loss: 2.2767192e-06\n",
      "Loss: 2.2758807e-06\n",
      "Loss: 2.2753634e-06\n",
      "Loss: 2.2748172e-06\n",
      "Loss: 2.2742292e-06\n",
      "Loss: 2.2736513e-06\n",
      "Loss: 2.2730715e-06\n",
      "Loss: 2.2725974e-06\n",
      "Loss: 2.2717816e-06\n",
      "Loss: 2.2714642e-06\n",
      "Loss: 2.2710635e-06\n",
      "Loss: 2.2707945e-06\n",
      "Loss: 2.2703796e-06\n",
      "Loss: 2.270038e-06\n",
      "Loss: 2.2749807e-06\n",
      "Loss: 2.269822e-06\n",
      "Loss: 2.2693553e-06\n",
      "Loss: 2.2690542e-06\n",
      "Loss: 2.2687223e-06\n",
      "Loss: 2.2682923e-06\n",
      "Loss: 2.2680417e-06\n",
      "Loss: 2.267742e-06\n",
      "Loss: 2.2674394e-06\n",
      "Loss: 2.2670956e-06\n",
      "Loss: 2.2666386e-06\n",
      "Loss: 2.266292e-06\n",
      "Loss: 2.2656823e-06\n",
      "Loss: 2.2651175e-06\n",
      "Loss: 2.2642853e-06\n",
      "Loss: 2.2636527e-06\n",
      "Loss: 2.26314e-06\n",
      "Loss: 2.2626853e-06\n",
      "Loss: 2.262214e-06\n",
      "Loss: 2.2619524e-06\n",
      "Loss: 2.2611507e-06\n",
      "Loss: 2.2604258e-06\n",
      "Loss: 2.2590814e-06\n",
      "Loss: 2.2600623e-06\n",
      "Loss: 2.258616e-06\n",
      "Loss: 2.2579839e-06\n",
      "Loss: 2.2573847e-06\n",
      "Loss: 2.2564554e-06\n",
      "Loss: 2.255542e-06\n",
      "Loss: 2.2546894e-06\n",
      "Loss: 2.2541653e-06\n",
      "Loss: 2.2535817e-06\n",
      "Loss: 2.2532877e-06\n",
      "Loss: 2.2522208e-06\n",
      "Loss: 2.2524823e-06\n",
      "Loss: 2.251749e-06\n",
      "Loss: 2.2509614e-06\n",
      "Loss: 2.2502136e-06\n",
      "Loss: 2.2498343e-06\n",
      "Loss: 2.2492006e-06\n",
      "Loss: 2.2488025e-06\n",
      "Loss: 2.2481724e-06\n",
      "Loss: 2.2474387e-06\n",
      "Loss: 2.2463355e-06\n",
      "Loss: 2.245431e-06\n",
      "Loss: 2.2448344e-06\n",
      "Loss: 2.2445327e-06\n",
      "Loss: 2.2442273e-06\n",
      "Loss: 2.2437177e-06\n",
      "Loss: 2.2429394e-06\n",
      "Loss: 2.242292e-06\n",
      "Loss: 2.2417069e-06\n",
      "Loss: 2.2412892e-06\n",
      "Loss: 2.2409e-06\n",
      "Loss: 2.2406628e-06\n",
      "Loss: 2.2403224e-06\n",
      "Loss: 2.2399504e-06\n",
      "Loss: 2.2397949e-06\n",
      "Loss: 2.239133e-06\n",
      "Loss: 2.2389104e-06\n",
      "Loss: 2.2383156e-06\n",
      "Loss: 2.2380013e-06\n",
      "Loss: 2.2378558e-06\n",
      "Loss: 2.2375643e-06\n",
      "Loss: 2.237025e-06\n",
      "Loss: 2.2361626e-06\n",
      "Loss: 2.2357126e-06\n",
      "Loss: 2.2351346e-06\n",
      "Loss: 2.234814e-06\n",
      "Loss: 2.234388e-06\n",
      "Loss: 2.233905e-06\n",
      "Loss: 2.234136e-06\n",
      "Loss: 2.233452e-06\n",
      "Loss: 2.2329832e-06\n",
      "Loss: 2.2324266e-06\n",
      "Loss: 2.2315871e-06\n",
      "Loss: 2.2305628e-06\n",
      "Loss: 2.2338545e-06\n",
      "Loss: 2.2302866e-06\n",
      "Loss: 2.2296551e-06\n",
      "Loss: 2.2292631e-06\n",
      "Loss: 2.2287536e-06\n",
      "Loss: 2.2276115e-06\n",
      "Loss: 2.229212e-06\n",
      "Loss: 2.2271151e-06\n",
      "Loss: 2.2259815e-06\n",
      "Loss: 2.2250613e-06\n",
      "Loss: 2.2240833e-06\n",
      "Loss: 2.2232384e-06\n",
      "Loss: 2.22253e-06\n",
      "Loss: 2.2225931e-06\n",
      "Loss: 2.2218883e-06\n",
      "Loss: 2.221256e-06\n",
      "Loss: 2.220504e-06\n",
      "Loss: 2.2197569e-06\n",
      "Loss: 2.2192883e-06\n",
      "Loss: 2.21895e-06\n",
      "Loss: 2.21858e-06\n",
      "Loss: 2.2180839e-06\n",
      "Loss: 2.2174827e-06\n",
      "Loss: 2.2169943e-06\n",
      "Loss: 2.2165877e-06\n",
      "Loss: 2.2162449e-06\n",
      "Loss: 2.2157242e-06\n",
      "Loss: 2.215185e-06\n",
      "Loss: 2.2142933e-06\n",
      "Loss: 2.2137067e-06\n",
      "Loss: 2.2131012e-06\n",
      "Loss: 2.212752e-06\n",
      "Loss: 2.2121249e-06\n",
      "Loss: 2.2116114e-06\n",
      "Loss: 2.2109311e-06\n",
      "Loss: 2.210421e-06\n",
      "Loss: 2.2100862e-06\n",
      "Loss: 2.2095026e-06\n",
      "Loss: 2.208596e-06\n",
      "Loss: 2.212883e-06\n",
      "Loss: 2.2083182e-06\n",
      "Loss: 2.2076679e-06\n",
      "Loss: 2.207143e-06\n",
      "Loss: 2.2069976e-06\n",
      "Loss: 2.2061076e-06\n",
      "Loss: 2.2055815e-06\n",
      "Loss: 2.2046943e-06\n",
      "Loss: 2.2040854e-06\n",
      "Loss: 2.2038778e-06\n",
      "Loss: 2.2034617e-06\n",
      "Loss: 2.2028048e-06\n",
      "Loss: 2.2039696e-06\n",
      "Loss: 2.2025913e-06\n",
      "Loss: 2.2020376e-06\n",
      "Loss: 2.2014992e-06\n",
      "Loss: 2.2007519e-06\n",
      "Loss: 2.2001411e-06\n",
      "Loss: 2.2006416e-06\n",
      "Loss: 2.1998471e-06\n",
      "Loss: 2.199217e-06\n",
      "Loss: 2.198834e-06\n",
      "Loss: 2.1983242e-06\n",
      "Loss: 2.1977335e-06\n",
      "Loss: 2.1971985e-06\n",
      "Loss: 2.1966157e-06\n",
      "Loss: 2.196221e-06\n",
      "Loss: 2.195929e-06\n",
      "Loss: 2.195007e-06\n",
      "Loss: 2.1943279e-06\n",
      "Loss: 2.1930603e-06\n",
      "Loss: 2.1924202e-06\n",
      "Loss: 2.1918056e-06\n",
      "Loss: 2.1918727e-06\n",
      "Loss: 2.1914607e-06\n",
      "Loss: 2.191189e-06\n",
      "Loss: 2.190825e-06\n",
      "Loss: 2.1905266e-06\n",
      "Loss: 2.1903058e-06\n",
      "Loss: 2.1896963e-06\n",
      "Loss: 2.1889794e-06\n",
      "Loss: 2.1885448e-06\n",
      "Loss: 2.1879744e-06\n",
      "Loss: 2.1874512e-06\n",
      "Loss: 2.1867775e-06\n",
      "Loss: 2.1867186e-06\n",
      "Loss: 2.1863762e-06\n",
      "Loss: 2.1858796e-06\n",
      "Loss: 2.1855894e-06\n",
      "Loss: 2.185263e-06\n",
      "Loss: 2.1847723e-06\n",
      "Loss: 2.1845913e-06\n",
      "Loss: 2.1840144e-06\n",
      "Loss: 2.1834664e-06\n",
      "Loss: 2.1833341e-06\n",
      "Loss: 2.182952e-06\n",
      "Loss: 2.1825417e-06\n",
      "Loss: 2.1821957e-06\n",
      "Loss: 2.1819988e-06\n",
      "Loss: 2.181654e-06\n",
      "Loss: 2.1814851e-06\n",
      "Loss: 2.1807425e-06\n",
      "Loss: 2.1799146e-06\n",
      "Loss: 2.1792143e-06\n",
      "Loss: 2.1783803e-06\n",
      "Loss: 2.1780202e-06\n",
      "Loss: 2.1775875e-06\n",
      "Loss: 2.1768258e-06\n",
      "Loss: 2.1763215e-06\n",
      "Loss: 2.1757935e-06\n",
      "Loss: 2.1754367e-06\n",
      "Loss: 2.1749743e-06\n",
      "Loss: 2.17472e-06\n",
      "Loss: 2.1742467e-06\n",
      "Loss: 2.1739856e-06\n",
      "Loss: 2.1736194e-06\n",
      "Loss: 2.1732797e-06\n",
      "Loss: 2.1729852e-06\n",
      "Loss: 2.1722535e-06\n",
      "Loss: 2.1742248e-06\n",
      "Loss: 2.1719798e-06\n",
      "Loss: 2.171255e-06\n",
      "Loss: 2.1708477e-06\n",
      "Loss: 2.1702585e-06\n",
      "Loss: 2.1696433e-06\n",
      "Loss: 2.1691997e-06\n",
      "Loss: 2.1688177e-06\n",
      "Loss: 2.1684946e-06\n",
      "Loss: 2.1681124e-06\n",
      "Loss: 2.1679343e-06\n",
      "Loss: 2.1676854e-06\n",
      "Loss: 2.1674239e-06\n",
      "Loss: 2.1671854e-06\n",
      "Loss: 2.166475e-06\n",
      "Loss: 2.166164e-06\n",
      "Loss: 2.16586e-06\n",
      "Loss: 2.165727e-06\n",
      "Loss: 2.1652636e-06\n",
      "Loss: 2.164489e-06\n",
      "Loss: 2.1640235e-06\n",
      "Loss: 2.1635567e-06\n",
      "Loss: 2.1631668e-06\n",
      "Loss: 2.1629085e-06\n",
      "Loss: 2.1623118e-06\n",
      "Loss: 2.1613905e-06\n",
      "Loss: 2.160411e-06\n",
      "Loss: 2.1605424e-06\n",
      "Loss: 2.1600163e-06\n",
      "Loss: 2.1593787e-06\n",
      "Loss: 2.1590402e-06\n",
      "Loss: 2.1585993e-06\n",
      "Loss: 2.1587819e-06\n",
      "Loss: 2.158346e-06\n",
      "Loss: 2.1579042e-06\n",
      "Loss: 2.1574583e-06\n",
      "Loss: 2.1570527e-06\n",
      "Loss: 2.1565504e-06\n",
      "Loss: 2.1557714e-06\n",
      "Loss: 2.1573458e-06\n",
      "Loss: 2.1554451e-06\n",
      "Loss: 2.155009e-06\n",
      "Loss: 2.154498e-06\n",
      "Loss: 2.1539336e-06\n",
      "Loss: 2.1534013e-06\n",
      "Loss: 2.152893e-06\n",
      "Loss: 2.1522783e-06\n",
      "Loss: 2.1520939e-06\n",
      "Loss: 2.1517963e-06\n",
      "Loss: 2.1514118e-06\n",
      "Loss: 2.15105e-06\n",
      "Loss: 2.1509195e-06\n",
      "Loss: 2.1506708e-06\n",
      "Loss: 2.1503731e-06\n",
      "Loss: 2.15006e-06\n",
      "Loss: 2.149536e-06\n",
      "Loss: 2.1490857e-06\n",
      "Loss: 2.1485964e-06\n",
      "Loss: 2.148023e-06\n",
      "Loss: 2.1475958e-06\n",
      "Loss: 2.147093e-06\n",
      "Loss: 2.1466042e-06\n",
      "Loss: 2.1457397e-06\n",
      "Loss: 2.145122e-06\n",
      "Loss: 2.1448245e-06\n",
      "Loss: 2.144524e-06\n",
      "Loss: 2.144207e-06\n",
      "Loss: 2.1436986e-06\n",
      "Loss: 2.1434098e-06\n",
      "Loss: 2.1428991e-06\n",
      "Loss: 2.1428045e-06\n",
      "Loss: 2.1424487e-06\n",
      "Loss: 2.1418314e-06\n",
      "Loss: 2.1414369e-06\n",
      "Loss: 2.1408237e-06\n",
      "Loss: 2.1400801e-06\n",
      "Loss: 2.1413118e-06\n",
      "Loss: 2.1399028e-06\n",
      "Loss: 2.1394362e-06\n",
      "Loss: 2.1388198e-06\n",
      "Loss: 2.1382912e-06\n",
      "Loss: 2.1379494e-06\n",
      "Loss: 2.1375843e-06\n",
      "Loss: 2.138465e-06\n",
      "Loss: 2.1375058e-06\n",
      "Loss: 2.1370824e-06\n",
      "Loss: 2.1368298e-06\n",
      "Loss: 2.1362848e-06\n",
      "Loss: 2.1357646e-06\n",
      "Loss: 2.1355322e-06\n",
      "Loss: 2.1351595e-06\n",
      "Loss: 2.1348242e-06\n",
      "Loss: 2.1346068e-06\n",
      "Loss: 2.1340466e-06\n",
      "Loss: 2.133887e-06\n",
      "Loss: 2.133273e-06\n",
      "Loss: 2.132942e-06\n",
      "Loss: 2.1325177e-06\n",
      "Loss: 2.1321284e-06\n",
      "Loss: 2.1317605e-06\n",
      "Loss: 2.1313554e-06\n",
      "Loss: 2.130967e-06\n",
      "Loss: 2.130535e-06\n",
      "Loss: 2.129958e-06\n",
      "Loss: 2.1295236e-06\n",
      "Loss: 2.1289773e-06\n",
      "Loss: 2.1283995e-06\n",
      "Loss: 2.1277724e-06\n",
      "Loss: 2.127477e-06\n",
      "Loss: 2.1270168e-06\n",
      "Loss: 2.126458e-06\n",
      "Loss: 2.1257474e-06\n",
      "Loss: 2.125444e-06\n",
      "Loss: 2.1248234e-06\n",
      "Loss: 2.1244618e-06\n",
      "Loss: 2.124151e-06\n",
      "Loss: 2.1234614e-06\n",
      "Loss: 2.1228568e-06\n",
      "Loss: 2.122542e-06\n",
      "Loss: 2.1221447e-06\n",
      "Loss: 2.1218257e-06\n",
      "Loss: 2.121387e-06\n",
      "Loss: 2.1205963e-06\n",
      "Loss: 2.121728e-06\n",
      "Loss: 2.120286e-06\n",
      "Loss: 2.119619e-06\n",
      "Loss: 2.1189985e-06\n",
      "Loss: 2.1184653e-06\n",
      "Loss: 2.118061e-06\n",
      "Loss: 2.1175688e-06\n",
      "Loss: 2.1168955e-06\n",
      "Loss: 2.1159758e-06\n",
      "Loss: 2.1158537e-06\n",
      "Loss: 2.1151582e-06\n",
      "Loss: 2.1148708e-06\n",
      "Loss: 2.1145197e-06\n",
      "Loss: 2.1140756e-06\n",
      "Loss: 2.1134351e-06\n",
      "Loss: 2.1132912e-06\n",
      "Loss: 2.1124347e-06\n",
      "Loss: 2.1121054e-06\n",
      "Loss: 2.1116655e-06\n",
      "Loss: 2.1111832e-06\n",
      "Loss: 2.110903e-06\n",
      "Loss: 2.1103403e-06\n",
      "Loss: 2.1099468e-06\n",
      "Loss: 2.109306e-06\n",
      "Loss: 2.1087444e-06\n",
      "Loss: 2.1085295e-06\n",
      "Loss: 2.1080727e-06\n",
      "Loss: 2.1078502e-06\n",
      "Loss: 2.1076874e-06\n",
      "Loss: 2.1072558e-06\n",
      "Loss: 2.1064716e-06\n",
      "Loss: 2.1060032e-06\n",
      "Loss: 2.1053336e-06\n",
      "Loss: 2.1050587e-06\n",
      "Loss: 2.104877e-06\n",
      "Loss: 2.1046417e-06\n",
      "Loss: 2.1042406e-06\n",
      "Loss: 2.1039923e-06\n",
      "Loss: 2.1038163e-06\n",
      "Loss: 2.1035644e-06\n",
      "Loss: 2.102864e-06\n",
      "Loss: 2.1023852e-06\n",
      "Loss: 2.1017622e-06\n",
      "Loss: 2.1012738e-06\n",
      "Loss: 2.100869e-06\n",
      "Loss: 2.1001456e-06\n",
      "Loss: 2.1000387e-06\n",
      "Loss: 2.099268e-06\n",
      "Loss: 2.0991388e-06\n",
      "Loss: 2.09864e-06\n",
      "Loss: 2.0981665e-06\n",
      "Loss: 2.0976884e-06\n",
      "Loss: 2.097304e-06\n",
      "Loss: 2.096952e-06\n",
      "Loss: 2.0965283e-06\n",
      "Loss: 2.0962893e-06\n",
      "Loss: 2.095854e-06\n",
      "Loss: 2.0953908e-06\n",
      "Loss: 2.0950972e-06\n",
      "Loss: 2.0945815e-06\n",
      "Loss: 2.0938214e-06\n",
      "Loss: 2.0943503e-06\n",
      "Loss: 2.093315e-06\n",
      "Loss: 2.0925174e-06\n",
      "Loss: 2.091693e-06\n",
      "Loss: 2.0911475e-06\n",
      "Loss: 2.0903435e-06\n",
      "Loss: 2.0898676e-06\n",
      "Loss: 2.0894222e-06\n",
      "Loss: 2.0888863e-06\n",
      "Loss: 2.088551e-06\n",
      "Loss: 2.0882594e-06\n",
      "Loss: 2.0878695e-06\n",
      "Loss: 2.087628e-06\n",
      "Loss: 2.0873176e-06\n",
      "Loss: 2.087166e-06\n",
      "Loss: 2.0865075e-06\n",
      "Loss: 2.0858615e-06\n",
      "Loss: 2.0861596e-06\n",
      "Loss: 2.0856564e-06\n",
      "Loss: 2.0852704e-06\n",
      "Loss: 2.0848904e-06\n",
      "Loss: 2.0844143e-06\n",
      "Loss: 2.0840514e-06\n",
      "Loss: 2.083379e-06\n",
      "Loss: 2.083054e-06\n",
      "Loss: 2.082527e-06\n",
      "Loss: 2.0821406e-06\n",
      "Loss: 2.081747e-06\n",
      "Loss: 2.0814875e-06\n",
      "Loss: 2.0810032e-06\n",
      "Loss: 2.0805692e-06\n",
      "Loss: 2.0803316e-06\n",
      "Loss: 2.0798038e-06\n",
      "Loss: 2.0793775e-06\n",
      "Loss: 2.0803034e-06\n",
      "Loss: 2.0791504e-06\n",
      "Loss: 2.078932e-06\n",
      "Loss: 2.0785787e-06\n",
      "Loss: 2.0781767e-06\n",
      "Loss: 2.0778446e-06\n",
      "Loss: 2.0773798e-06\n",
      "Loss: 2.0769246e-06\n",
      "Loss: 2.0765788e-06\n",
      "Loss: 2.0761045e-06\n",
      "Loss: 2.0757207e-06\n",
      "Loss: 2.0751008e-06\n",
      "Loss: 2.0745795e-06\n",
      "Loss: 2.0739087e-06\n",
      "Loss: 2.0737575e-06\n",
      "Loss: 2.0735336e-06\n",
      "Loss: 2.0731554e-06\n",
      "Loss: 2.0721675e-06\n",
      "Loss: 2.0726707e-06\n",
      "Loss: 2.0717707e-06\n",
      "Loss: 2.0710713e-06\n",
      "Loss: 2.0706848e-06\n",
      "Loss: 2.0704463e-06\n",
      "Loss: 2.0699654e-06\n",
      "Loss: 2.0694124e-06\n",
      "Loss: 2.069014e-06\n",
      "Loss: 2.0685043e-06\n",
      "Loss: 2.0682728e-06\n",
      "Loss: 2.0678374e-06\n",
      "Loss: 2.0676832e-06\n",
      "Loss: 2.067309e-06\n",
      "Loss: 2.0667514e-06\n",
      "Loss: 2.0663117e-06\n",
      "Loss: 2.0661505e-06\n",
      "Loss: 2.0659268e-06\n",
      "Loss: 2.0658067e-06\n",
      "Loss: 2.0655305e-06\n",
      "Loss: 2.065309e-06\n",
      "Loss: 2.064735e-06\n",
      "Loss: 2.0644675e-06\n",
      "Loss: 2.0641041e-06\n",
      "Loss: 2.0639832e-06\n",
      "Loss: 2.0635312e-06\n",
      "Loss: 2.063216e-06\n",
      "Loss: 2.0629122e-06\n",
      "Loss: 2.0623152e-06\n",
      "Loss: 2.0620678e-06\n",
      "Loss: 2.0618106e-06\n",
      "Loss: 2.0615475e-06\n",
      "Loss: 2.0612506e-06\n",
      "Loss: 2.0607526e-06\n",
      "Loss: 2.060418e-06\n",
      "Loss: 2.0599734e-06\n",
      "Loss: 2.0597697e-06\n",
      "Loss: 2.0594084e-06\n",
      "Loss: 2.0588755e-06\n",
      "Loss: 2.0584714e-06\n",
      "Loss: 2.0581674e-06\n",
      "Loss: 2.0577227e-06\n",
      "Loss: 2.057243e-06\n",
      "Loss: 2.0566529e-06\n",
      "Loss: 2.0562804e-06\n",
      "Loss: 2.0559664e-06\n",
      "Loss: 2.0554553e-06\n",
      "Loss: 2.0552648e-06\n",
      "Loss: 2.0546195e-06\n",
      "Loss: 2.0544824e-06\n",
      "Loss: 2.0539155e-06\n",
      "Loss: 2.0536436e-06\n",
      "Loss: 2.0532482e-06\n",
      "Loss: 2.0530235e-06\n",
      "Loss: 2.0526093e-06\n",
      "Loss: 2.052279e-06\n",
      "Loss: 2.0520542e-06\n",
      "Loss: 2.051687e-06\n",
      "Loss: 2.0511702e-06\n",
      "Loss: 2.0506016e-06\n",
      "Loss: 2.0502016e-06\n",
      "Loss: 2.049809e-06\n",
      "Loss: 2.0496038e-06\n",
      "Loss: 2.0490274e-06\n",
      "Loss: 2.0490684e-06\n",
      "Loss: 2.0485663e-06\n",
      "Loss: 2.048262e-06\n",
      "Loss: 2.0479245e-06\n",
      "Loss: 2.04778e-06\n",
      "Loss: 2.0473567e-06\n",
      "Loss: 2.0471637e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.0469581e-06\n",
      "Loss: 2.0468935e-06\n",
      "Loss: 2.046896e-06\n",
      "Loss: 2.046987e-06\n",
      "Loss: 2.0468901e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.046889e-06\n",
      "Loss: 2.046889e-06\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  Objective function value: 0.000002\n",
      "  Number of iterations: 5017\n",
      "  Number of functions evaluations: 5385\n",
      "Training time: 1765.3872\n",
      "Error u: 1.430376e-03\n",
      "Error v: 1.821405e-03\n",
      "Error h: 1.108188e-03\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "#Python中的一个常见模式。在Python文件被直接运行时，特殊变量__name__的值为\"__main__\"。所以，这行代码的意思是：如果这个文件被直接运行（而不是被导入作为模块），那么就执行后面的代码\n",
    "if __name__ == \"__main__\":   #这种模式常常用于在一个Python文件中区分出哪些代码是用于定义函数、类等，哪些代码是用于直接执行的。这样，当这个文件被导入作为模块时，只有函数和类的定义会被执行，而直接执行的代码则不会被执行\n",
    "    #设置噪声值为0 \n",
    "    noise = 0.0        \n",
    "    \n",
    "    # Doman bounds，定义两个一维数组lb和ub，问题域是一个二维空间，其中 x 的范围是 -5 到 5，t 的范围是 0 到 π/2(竖着的)\n",
    "    lb = np.array([-5.0, 0.0])\n",
    "    ub = np.array([5.0, np.pi/2])\n",
    "    #定义三个整数，分别表示初始条件点数量、边界条件点数量和在问题域内部的点的数量（这些点用于训练神经网络）\n",
    "    N0 = 50\n",
    "    N_b = 50\n",
    "    N_f = 20000\n",
    "    #定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "    layers = [2, 100, 100, 100, 100, 2]\n",
    "    #读取名为NLS.mat的Matlab文件，文件中的数据存储在data变量中。这里的路径也要随着设备的情况修改    \n",
    "    data = scipy.io.loadmat('C:/Users/cheny/Documents/GitHub/PINNs/main/Data/NLS.mat')\n",
    "    #从data字典中取出变量tt和x的值，并转换为一维数组（flatten方法），最后tongg[:,None]将一维数组转换为二维数组\n",
    "    t = data['tt'].flatten()[:,None]\n",
    "    x = data['x'].flatten()[:,None]\n",
    "    Exact = data['uu'] #从data字典中取出变量uu的值，并赋值给Exact\n",
    "    Exact_u = np.real(Exact)  #取Exact的实部，赋值给Exact_u\n",
    "    Exact_v = np.imag(Exact)  #取Exact的虚部，赋值给Exact_v\n",
    "    Exact_h = np.sqrt(Exact_u**2 + Exact_v**2) #计算复数uu的|uu|\n",
    "    #生成一个二位网络，X和T是输出的二维数组\n",
    "    X, T = np.meshgrid(x,t)\n",
    "    \n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  #X_star是一个二维数组，其中第一列是X的展平，第二列是T的展平\n",
    "    u_star = Exact_u.T.flatten()[:,None] #先对Exact_u进行转置，然后使用flatten方法将其转换为一维数组，最后使用[:,None]将其转换为二维数组\n",
    "    v_star = Exact_v.T.flatten()[:,None] #同上，比如Exact_v是m*n二维数组，Exact_v.T是n*m二维数组，Exact_v.T.flatten()是一个长度为n*m的一维数组，Exact_v.T.flatten()[:,None]是一个(n*m)*1的三维数组\n",
    "    h_star = Exact_h.T.flatten()[:,None]\n",
    "    #上面五行代码的意义见Numpy库的索引的介绍\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    \n",
    "    #从0~数组x的行数(256)中随机选择N0个数，replace=False表示不允许重复选择，最后将这N0个数赋值给idx_x\n",
    "    idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "    #从x中选择N0个对应的行(idx_x对应的行)，最后将这N0行赋值给x0\n",
    "    x0 = x[idx_x,:]\n",
    "    #从Exact_u中选择N0个对应的行(idx_x对应的行)的第一列元素，最后将这N0个元素赋值给u0\n",
    "    u0 = Exact_u[idx_x,0:1]\n",
    "    v0 = Exact_v[idx_x,0:1]\n",
    "    #从0~数组t的行数中随机选择N_b个数，replace=False表示不允许重复选择，最后将这N_b个数赋值给idx_t\n",
    "    idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "    #从t中选择N_b个对应的行(idx_t对应的行)，最后将这N_b行赋值给tb\n",
    "    tb = t[idx_t,:]\n",
    "    \n",
    "    X_f = lb + (ub-lb)*lhs(2, N_f) #lhs函数采用拉丁超采样方法，生成一个近似均匀分布的多维样本点集，返回的是一个形状为（$N_f$，2）的数组，每一行都是一个2维的样本点，所有样本点都在[0,1]范围内，并对该样本集进行缩放，把每个样本从[0,1]区间缩放到[lb,ub]区域内，即得到了指定范围内均匀分布的样本$X_f$。\n",
    "\n",
    "    #创建PINN模型并输入各种参数        \n",
    "    model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub)\n",
    "    #获取当前时间并赋值给start_time          \n",
    "    start_time = time.time()       \n",
    "    #训练模型50000次         \n",
    "    model.train(50000)\n",
    "    #获取当前时间并减去start_time，得到训练时间并赋值给elapsed\n",
    "    elapsed = time.time() - start_time                \n",
    "    #打印训练所需时间\n",
    "    print('Training time: %.4f' % (elapsed))\n",
    "    \n",
    "    #用训练好的模型进行预测，返回四个值（均为数组）    \n",
    "    u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
    "    #计算u_pred和v_pred的模（平方和的平方根），赋值给h_pred\n",
    "    h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
    "    #计算误差（基于2范数）        \n",
    "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
    "    error_h = np.linalg.norm(h_star-h_pred,2)/np.linalg.norm(h_star,2)\n",
    "    #打印误差\n",
    "    print('Error u: %e' % (error_u))\n",
    "    print('Error v: %e' % (error_v))\n",
    "    print('Error h: %e' % (error_h))\n",
    "\n",
    "    #使用griddata函数将X_star、u_pred、v_pred和h_pred插值到网格上，得到U_pred、V_pred和H_pred\n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "    V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
    "    H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
    "    #同上，使用griddata函数将X_star、f_u_pred和f_v_pred插值到网格上，得到FU_pred和FV_pred\n",
    "    FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')\n",
    "    FV_pred = griddata(X_star, f_v_pred.flatten(), (X, T), method='cubic')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 539.643x300.166 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################################################\n",
    "############################# Plotting ###############################\n",
    "######################################################################    \n",
    "#调用concatenate函数拼接数组\n",
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "X_u_train = np.vstack([X0, X_lb, X_ub])  #(X0;X_lb;X_ub)\n",
    "#调用plotting文件中的newfig函数，生成一个宽1英寸、高0.9英寸的图像fig和子图ax\n",
    "fig, ax = newfig(1.0, 0.9) #这里ax是一个axes对象，代表子图，figure是一个figure对象，是一个图形窗口，代表整个图形\n",
    "ax.axis('off') #关闭子图的轴的显示\n",
    "\n",
    "####### Row 0: h(t,x)，绘制第一个子图，展示x,t和|h(t,x)|的关系##################    \n",
    "#创建一个包含子图的网格，1行2列\n",
    "gs0 = gridspec.GridSpec(1,2)  #创建一个1×2的网络，用于存放子图\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0) #更新该网络的参数，第一个表示子图的顶部位置为0.94，第二个参数表示子图的底部位置为0.667，第三个表示子图左侧的位置为0.15，第四个参数表示子图的右侧位置为0.85，第五个参数表示子图之间的宽度，0表示子图之间没有空隙\n",
    "ax = plt.subplot(gs0[:,:]) #在gs0[:,:] 指定的位置创建了一个子图，并将返回的axes对象赋值给ax。gs0[:,:]表示GridSpec对象gs0的所有行和所有列，所以这行代码创建的子图占据了整个图形。\n",
    "\n",
    "h = ax.imshow(H_pred.T, interpolation='nearest', cmap='YlGnBu', \n",
    "                extent=[lb[1], ub[1], lb[0], ub[0]], \n",
    "                origin='lower', aspect='auto')  #imshow函数用于显示图像，接受一些参数，第一个参数是图像数据，这里是H_pred的转置；第二个参数是插值方法（用于在像素之间插入新的像素），这里是最邻近插值；\n",
    "                                                #第三个参数是颜色映射，这里是从黄色Yl到绿色Gn再到蓝色Bu；第四个参数是图像的范围，这里lb和ub分别是数据的下界和上界；第五个参数是图像的原点位置，这里表示原点在右下角；第六个参数是图像的纵横比，这里表示调整横纵比以填充整个axes对象\n",
    "                                                #最后的结果返回一个axesimage对象，也就是h，可以通过这个对象进一步设置图像的属性\n",
    "divider = make_axes_locatable(ax)  #使用 make_axes_locatable 函数创建了一个 AxesDivider 对象。这个函数接受一个 Axes 对象作为参数，返回一个 AxesDivider 对象。AxesDivider 对象可以用来管理子图的布局，特别是当你需要在一个图的旁边添加另一个图时。\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05) #使用append_axes方法在原始轴的右侧添加了一个新的轴。append_axes 方法接受三个参数：位置（\"right\"）、大小（\"5%\"）和间距（0.05）。在原始轴的右侧添加了一个新的轴，新轴的大小是原始轴的 5%，新轴与原始轴之间的间距是 0.05 英寸\n",
    "fig.colorbar(h, cax=cax) #使用colorbar方法在新轴上添加了一个颜色条。colorbar 方法接受两个参数：axesimage 对象（h）和新轴（cax）。\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (X_u_train.shape[0]), markersize = 4, clip_on = False) #在ax上绘制散点图，前两个参数是散点的x坐标和y坐标；kx表示黑色的x（散点形状是x），label是散点的标签，clip_on表示散点可以绘制在轴的边界外\n",
    "#在ax图上绘制三条虚线\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None] #生成了一个包含2个等间距的数值的数组，这些数值在 x.min() 到 x.max() 之间。[:,None] 是一个索引操作，用于将一维数组转换为二维数组。这里其实就是[-5;5]\n",
    "#第一个参数是虚线的x坐标，line是虚线y的坐标，第三个参数是虚线的样式，k表示黑色，--表示虚线，最后一个参数表示虚线的参数是1\n",
    "ax.plot(t[75]*np.ones((2,1)),line,'k--',linewidth=1) \n",
    "ax.plot(t[100]*np.ones((2,1)),line,'k--',linewidth=1)\n",
    "ax.plot(t[125]*np.ones((2,1)),line,'k--',linewidth=1)    \n",
    "#设置ax子图的x轴的标签为t，y轴的标签为x。这里$t$和$x$是latex格式的文本，用于生成数学公式\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "#设置子图ax的图例，frameon=False表示不显示图例的边框，loc='best'表示图例的位置是最佳位置，最后返回的leg是一个legend对象，表示图形的图例\n",
    "leg = ax.legend(frameon=False, loc = 'best')\n",
    "#    plt.setp(leg.get_texts(), color='w')   #用来设置图例中文本的颜色，这里是白色，取消注释后文本会变为白色\n",
    "ax.set_title('$|h(t,x)|$', fontsize = 10) #设置子图ax的标题为$|h(t,x)|$，表示latex格式的文本，用于生成数学公式，fontsize=10表示字体大小为10\n",
    "\n",
    "####### Row 1: h(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1,3) #创建一个1×3的网络，用于存放子图\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5) #更新该网络的参数，第一个表示子图的顶部位置为0.667，第二个参数表示子图的底部位置为0，第三个表示子图左侧的位置为0，第四个参数表示子图的右侧位置为0.9，第五个参数表示子图之间的宽度为0.5\n",
    "\n",
    "ax = plt.subplot(gs1[0,0])  #在gs1[0,0]指定的位置，也就是网格的第一行第一列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "#绘制了两条线，一条表示精确值，一条表示预测值\n",
    "ax.plot(x,Exact_h[:,75], 'b-', linewidth = 2, label = 'Exact')      #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[75,:], 'r--', linewidth = 2, label = 'Prediction') #同上\n",
    "#设置ax子图的x轴的标签为x，y轴的标签为|h(t,x)|。这里$x$和$|h(t,x)|$是latex格式的文本，用于生成数学公式\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$|h(t,x)|$')    \n",
    "#设置子图的标题，几个子图标题随着t的变化而变化，字体大小为10 \n",
    "ax.set_title('$t=%.2f$' % (t[75]), fontsize = 10)\n",
    "ax.axis('square') #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1]) #第一个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1]) #第一个子图的y轴范围是-0.1到5.1\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1]) #在gs1[0,1]指定的位置，也就是网格的第一行第二列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "#绘制了两条线，一条表示精确值，一条表示预测值\n",
    "ax.plot(x,Exact_h[:,100],'b-', linewidth = 2, label = 'Exact')        #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[100,:],'r--', linewidth = 2, label = 'Prediction')   #同上\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$|h(t,x)|$') #设置子图的y轴的标签为|h(t,x)|\n",
    "ax.axis('square')   #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1])     #第二个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1])     #第二个子图的y轴范围是-0.1到5.1\n",
    "ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)        #设置第二个子图的标题，标题随着t的变化而变化，字体大小为10\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.8), ncol=5, frameon=False)  #设置第二个子图的图例，loc='upper center'表示图例的位置是上方中心，bbox_to_anchor=(0.5,-0.8)表示图例的中心位置是在子图的中间偏下方0.8的位置，ncol=5表示图例的列数是5，frameon=False表示不显示图例的边框\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2]) #在gs1[0,2]指定的位置，也就是网格的第一行第三列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "ax.plot(x,Exact_h[:,125], 'b-', linewidth = 2, label = 'Exact')        #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[125,:], 'r--', linewidth = 2, label = 'Prediction')    #同上\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$|h(t,x)|$') #设置子图的y轴的标签为|h(t,x)|\n",
    "ax.axis('square')    #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1])    #第三个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1])    #第三个子图的y轴范围是-0.1到5.1\n",
    "ax.set_title('$t = %.2f$' % (t[125]), fontsize = 10)    #设置第三个子图的标题，标题随着t的变化而变化，字体大小为10\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
