{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:44.490141Z",
     "iopub.status.busy": "2025-01-01T03:28:44.489696Z",
     "iopub.status.idle": "2025-01-01T03:28:45.312090Z",
     "shell.execute_reply": "2025-01-01T03:28:45.311763Z"
    }
   },
   "outputs": [],
   "source": [
    "#下面这行代码，是为了把自己编写的代码文件当作一共模块导入，这里是把Utilities文件夹中的plotting.py文件当作python的模块导入，对应的是下面的from plotting import newfig, savefig。路径要随着不同设备的系统做相应的修改\n",
    "import sys #导入sys模块。sys模块提供了一些变量和函数，用于与 Python解释器进行交互和访问。例如，sys.path 是一个 Python 在导入模块时会查找的路径列表，sys.argv 是一个包含命令行参数的列表，sys.exit() 函数可以用于退出 Python 程序。导入 sys 模块后，你就可以在你的程序中使用这些变量和函数了。\n",
    "sys.path.insert(0, '../../Utilities/') #在 Python的sys.path列表中插入一个新的路径。sys.path是一个 Python 在导入模块时会查找的路径列表。新的路径'../../Utilities/'相对于当前脚本的路径。当你尝试导入一个模块时，Python 会在 sys.path 列表中的路径下查找这个模块。通过在列表开始位置插入一个路径，你可以让 Python 优先在这个路径下查找模块。这在你需要导入自定义模块或者不在 Python 标准库中的模块时非常有用。\n",
    "\n",
    "import torch\n",
    "#collections是python一个内置模块，提供了一些有用的数据结构\n",
    "from collections import OrderedDict  #这个类是字典dict的一个子类，用于创建有序的字典。普通字典中元素顺序是无序的，在OrderedDict中元素的顺序是有序的，元素的顺序是按照它们被添加到字典中的顺序决定的。\n",
    "\n",
    "from pyDOE import lhs #`pyDOE`是一个Python库，用于设计实验。它提供了一些函数来生成各种设计，如因子设计、拉丁超立方设计等。`lhs`是库中的一个函数，全名为\"Latin Hypercube Sampling\"，拉丁超立方采样。这是一种统计方法，用于生成一个近似均匀分布的多维样本点集。它在参数空间中生成一个非常均匀的样本，这对于高维数值优化问题非常有用，因为它可以更好地覆盖参数空间。\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io #导入了scipy库中的io模块。scipy.io模块包含了一些用于文件输入/输出的函数，例如读取和写入.mat文件（MATLAB格式）\n",
    "from scipy.interpolate import griddata #`scipy.interpolate`是`scipy`库中的一个模块，提供了许多插值工具，用于在给定的离散数据点之间进行插值和拟合。`griddata`是这个模块中的一个函数，用于在无规则的数据点上进行插值。\n",
    "\n",
    "import random\n",
    "\n",
    "import skopt #用于优化问题的库，特别是机器学习中的超参数优化\n",
    "from distutils.version import LooseVersion #distutils是Python的一个标准库，用于构建和安装Python包。LooseVersion是一个类，用于比较版本号\n",
    "\n",
    "\n",
    "from plotting_torch import newfig, savefig #从自定义的plotting_torch.py文件中导入了newfig和savefig函数。这两个函数用于创建和保存图形。这两个函数的定义在plotting_torch.py文件中\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #`mpl_toolkits.axes_grid1`是`matplotlib`库的一个模块，提供了一些高级的工具来控制matplotlib图形中的坐标轴和颜色条。`make_axes_locatable`是模块中的一个函数，用于创建一个可分割的坐标轴。可以在这个坐标轴的四个方向（上、下、左、右）添加新的坐标轴或颜色条。\n",
    "import matplotlib.gridspec as gridspec #是`matplotlib`库的一个模块，用于创建一个网格布局来放置子图。在`matplotlib`中可以创建一个或多个子图（subplot），每个子图都有自己的坐标轴，并可以在其中绘制图形。`gridspec`模块提供了一个灵活的方式来创建和放置子图。\n",
    "import time #一个内置模块，用于处理时间相关的操作。\n",
    "\n",
    "\n",
    "from tqdm import tqdm #一个快速，可扩展的python进度条库，可以在python长循环中添加一个进度提示信息，用户只需要封装任意的迭代器tqdm(iterator)。\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import subprocess\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.313721Z",
     "iopub.status.busy": "2025-01-01T03:28:45.313604Z",
     "iopub.status.idle": "2025-01-01T03:28:45.469242Z",
     "shell.execute_reply": "2025-01-01T03:28:45.468645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs:  2\n",
      "GPU 0: NVIDIA GeForce RTX 4090, Allocated: 0, Reserved: 0\n",
      "GPU 1: NVIDIA GeForce RTX 4090, Allocated: 0, Reserved: 0\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print('Number of available GPUs: ', num_gpus)\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    torch.cuda.set_device(i)\n",
    "    allocated = torch.cuda.memory_allocated()\n",
    "    reserved = torch.cuda.memory_reserved()\n",
    "    print('GPU {}: {}, Allocated: {}, Reserved: {}'.format(i, torch.cuda.get_device_name(i), allocated, reserved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.482997Z",
     "iopub.status.busy": "2025-01-01T03:28:45.482899Z",
     "iopub.status.idle": "2025-01-01T03:28:45.484946Z",
     "shell.execute_reply": "2025-01-01T03:28:45.484657Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1) #设置当前使用的GPU设备。这里设置为1号GPU设备（第二块显卡）。\n",
    "\n",
    "# CUDA support \n",
    "\n",
    "#设置pytorch的设备，代表了在哪里执行张量积算，设备可以是cpu或者cuda（gpu），并将这个做运算的设备对象存储在变量device中，后续张量计算回在这个设备上执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.486120Z",
     "iopub.status.busy": "2025-01-01T03:28:45.486050Z",
     "iopub.status.idle": "2025-01-01T03:28:45.488983Z",
     "shell.execute_reply": "2025-01-01T03:28:45.488792Z"
    }
   },
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    #第一个方法\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__() #调用父类的__init__方法进行初始化\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1 #定义名为depth的属性，表示神经网络的深度，等于层数-1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh #设置激活函数为tanh\n",
    "         \n",
    "        layer_list = list() #定义一个空列表layer_list\n",
    "        for i in range(self.depth - 1):  #循环depth次\n",
    "            #将每一层（全连接层）添加到layer_list中\n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            #将每一层的激活函数添加到layer_list中\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "\n",
    "        #循环结束后，将最后一层的线性变换添加到layer_list中（因为没有激活函数了）\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        #然后使用OrderedDict将layer_list中的元素转换为有序字典\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers，将layerDict转换为一个神经网络模型，赋值给self.layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    #第二个方法，定义了模型的前向传播过程\n",
    "    def forward(self, x):  #接收输入x\n",
    "        out = self.layers(x) #将输入x传入神经网络模型self.layers中，得到输出out\n",
    "        return out #返回输出out\n",
    "    \n",
    "    # 新增方法，获取最后一个隐藏层的输出\n",
    "    def hidden_output(self, x):\n",
    "        # 遍历每一层，直到最后一个隐藏层\n",
    "        for i in range(self.depth - 1):\n",
    "            # 获取当前层的线性变换\n",
    "            x = self.layers[i*2](x)\n",
    "            # 获取当前层的激活函数\n",
    "            x = self.layers[i*2 + 1](x)\n",
    "        # 返回最后一个隐藏层的输出\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.490355Z",
     "iopub.status.busy": "2025-01-01T03:28:45.490275Z",
     "iopub.status.idle": "2025-01-01T03:28:45.541319Z",
     "shell.execute_reply": "2025-01-01T03:28:45.541079Z"
    }
   },
   "outputs": [],
   "source": [
    "#set the class of PINN\n",
    "\n",
    "#定义了一个名为`PhysicsInformedNN'的类，用于实现基于物理的神经网络。\n",
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub, X_star, u_star, v_star, h_star): #这个类包含的第一个方法__init__，这是一个特殊的方法，也就是这个类的构造函数，用于初始化新创建的对象，接受了几个参数\n",
    "        \n",
    "        \n",
    "        #`numpy.concatenate`是一个用于数组拼接的函数。它可以将多个数组沿指定的轴拼接在一起，形成一个新的数组：numpy.concatenate((a1,a2, ...), axis=0)其中，`a1,a2, ...`是需要拼接的数组（只能接受数组或序列类型的参数，且参数形状必须相同），可以是多个。`axis`参数用于指定拼接的轴向，`axis=0`表示沿着第一个轴（即行）进行拼接，不指定`axis`参数默认值是0。\n",
    "        X0 = np.concatenate((x0,0*x0), 1) # [x0, 0],将x0和0*x0两个数组在第二个维度（即列）上进行了合并。0*x0会生成一个与x0形状相同，但所有元素都为0的数组。因此，X0的结果是一个新的二维数组，其中第一列是x0的值，第二列全为0\n",
    "        X_lb = np.concatenate((0*tb+lb[0],tb), 1) # [lb[0], tb],将0*tb+lb[0]和tb两个数组在第二个维度（即列）上进行了合并。0*tb+lb[0]会生成一个与tb形状相同，但所有元素都为lb[0]的数组。因此，X_lb的结果是一个新的二维数组，其中第一列全为lb[0]的值，第二列是tb的值。\n",
    "        X_ub = np.concatenate((0*tb+ub[0],tb), 1) # [ub[0], tb],同上生成一个与tb形状相同，但所有元素都为ub[0]的数组。因此，X_ub的结果是一个新的二维数组，其中第一列全为ub[0]的值，第二列是tb的值\n",
    "        \n",
    "        #Python使用self关键字来表示类的实例。当在类的方法中定义一个变量时，例如lb和ub，这些变量只在该方法内部可见，也就是说它们的作用域仅限于该方法。当方法执行完毕后，这些变量就会被销毁，无法在其他方法中访问它们。但如果希望在类的其他方法中也能访问这些变量就需要将它们保存为类的实例属性。这就是self.lb和self.ub的作用。\n",
    "            #通过将lb和ub赋值给self.lb和self.ub，就可以在类的其他方法中通过self.lb和self.ub来访问这些值。总的来说，self.lb和self.ub是类的实例属性，它们的作用域是整个类，而不仅仅是定义它们的方法。\n",
    "        self.lb = torch.tensor(lb).float().to(device) #将传入的lb和ub参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.lb和self.ub来访问这些值。\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "\n",
    "\n",
    "        self.x0 = torch.tensor(X0[:,0:1], requires_grad=True).float().to(device) #将X0的第一列赋值给self.x0（:表示取所有行,0：1实际上表示取第一列，因为python是左闭右开的）,将X0的第二列赋值给self.t0。这样可以在类的其他方法中通过self.x0和self.t0来访问这些值。\n",
    "        self.t0 = torch.tensor(X0[:,1:2], requires_grad=True).float().to(device) #将x0的第二列赋值给self.t0\n",
    "\n",
    "        self.x_lb = torch.tensor(X_lb[:,0:1], requires_grad=True).float().to(device) #将X_lb的第一列赋值给self.x_lb\n",
    "        self.t_lb = torch.tensor(X_lb[:,1:2], requires_grad=True).float().to(device) #将X_lb的第二列赋值给self.t_lb\n",
    "\n",
    "        self.x_ub = torch.tensor(X_ub[:,0:1], requires_grad=True).float().to(device) #将X_ub的第一列赋值给self.x_ub\n",
    "        self.t_ub = torch.tensor(X_ub[:,0:1], requires_grad=True).float().to(device) #将X_ub的第二列赋值给self.t_ub\n",
    "        \n",
    "        self.x_f = torch.tensor(X_f[:,0:1], requires_grad=True).float().to(device) #将X_f的第一列赋值给self.x_f\n",
    "        self.t_f = torch.tensor(X_f[:,1:2], requires_grad=True).float().to(device) #将X_f的第二列赋值给self.t_f\n",
    "        \n",
    "        self.u0 = torch.tensor(u0).float().to(device) #将传入的u0和v0参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.u0和self.v0来访问这些值。\n",
    "        self.v0 = torch.tensor(v0).float().to(device)\n",
    "\n",
    "\n",
    "        self.x_star = torch.tensor(X_star[:,0:1], requires_grad=True).float().to(device) #将X_star的第一列赋值给self.x_star\n",
    "        self.t_star = torch.tensor(X_star[:,1:2], requires_grad=True).float().to(device) #将X_star的第二列赋值给self.t_star\n",
    "        self.u_star = torch.tensor(u_star).float().to(device) #将传入的u_star和v_star参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.u_star和self.v_star来访问这些值。\n",
    "        self.v_star = torch.tensor(v_star).float().to(device)\n",
    "        self.h_star = torch.tensor(h_star).float().to(device)\n",
    "        \n",
    "        # Initialize NNs \n",
    "        self.layers = layers #将传入的layers参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.layers来访问这些值。\n",
    "        \n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device) #创建一个DNN类的实例，传入layers参数来实现神经网络的初始化，然后将这个实例移动到指定的设备上\n",
    "\n",
    "\n",
    "\n",
    "        # optimizers: using the same settings，这里是使用pytorch库进行优化的部分\n",
    "        #创建优化器optimizer，使用LBFGS算法，具体每个参数意义见下方\n",
    "        self.optimizer_LBFGS = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), #要优化的参数，这里返回的是一个生成器，包含了self.dnn中的所有参数（神经网络权重、偏置以及两个新加的变量）\n",
    "            lr=1.0,  #学习率设置为1\n",
    "            max_iter=50000,  #最大迭代次数为50000\n",
    "            max_eval=50000,  #最大评估次数为50000\n",
    "            history_size=50, #历史大小为50，即用于计算Hessian矩阵近似的最近几步的信息\n",
    "            tolerance_grad=1e-5,  #优化的第一个停止条件，当梯度的L2范数小于1e-5时停止优化\n",
    "            tolerance_change=1.0 * np.finfo(float).eps, #优化的第二个停止条件，当优化的目标函数值的变化小于1.0 * np.finfo(float).eps时停止优化\n",
    "            line_search_fn=\"strong_wolfe\"       # 制定了用于一维搜索的方法，这里表示用强Wolfe条件\n",
    "        )\n",
    "        #创建第二个优化器，括号内为要优化的参数，使用Adam优化方法\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
    "                \n",
    "\n",
    "        self.iter = 0 #记录迭代次数 \n",
    "\n",
    "        self.loss_value = [] #创建一个空列表，用于存储损失值\n",
    "\n",
    "        self.test_error = [] #创建一个空列表，用于存储测试误差\n",
    "        self.test_error_mse = [] #创建一个空列表，用于存储测试误差\n",
    "        self.test_error_mae = [] #创建一个空列表，用于存储测试误差\n",
    "    \n",
    "    #这个函数与下面的net_uv函数功能相同，只是不计算梯度，因为在记录每个epoch的error时，使用with torch.no_grad()情况下调用net_uv函数会报错，不知道为啥？\n",
    "    def net_uv_error(self, x, t):  \n",
    "        uv = self.dnn(torch.cat([x, t], dim=1))  #（第一个参数将输入的两个参数x和t在第二个维度（列）上进行拼接，形成一个新的张量）调用DNN，根据两个参数权重和偏置，以及新得到的张量，计算神经网络的输出u\n",
    "        #将uv（是一个二维张量）的第一列赋值给u，第二列赋值给v\n",
    "        u=uv[:,0:1]\n",
    "        v=uv[:,1:2]\n",
    "\n",
    "        return u,v #返回神经网络的输出u和v，以及u关于x的梯度u_x和v关于x的梯度v_x\n",
    "\n",
    "\n",
    "    #pytorch中\n",
    "    #定义了一个名为net_u的函数/方法，用于计算神经网络的输出。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回神经网络的输出。     \n",
    "    def net_uv(self, x, t):  \n",
    "        uv = self.dnn(torch.cat([x, t], dim=1))  #（第一个参数将输入的两个参数x和t在第二个维度（列）上进行拼接，形成一个新的张量）调用DNN，根据两个参数权重和偏置，以及新得到的张量，计算神经网络的输出u\n",
    "        #将uv（是一个二维张量）的第一列赋值给u，第二列赋值给v\n",
    "        u=uv[:,0:1]\n",
    "        v=uv[:,1:2]\n",
    "\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v_x = torch.autograd.grad(\n",
    "            v, x, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        return u,v,u_x,v_x #返回神经网络的输出u和v，以及u关于x的梯度u_x和v关于x的梯度v_x\n",
    "\n",
    "\n",
    "    #定义了一个名为net_f的函数/方法，用于计算论文中的f。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回计算得到的f。\n",
    "    def net_f_uv(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "\n",
    "        u,v,u_x,v_x=self.net_uv(x,t) #调用上面的函数/方法，计算神经网络的输出（两个）以及输出关于输入x的梯度（两个）\n",
    "        \n",
    "        #计算u关于t的梯度，也就是u关于t的导数，这里使用了pytorch的自动求导功能\n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t,  #输入的张量，要计算u关于t的导数\n",
    "            grad_outputs=torch.ones_like(u), #生成一个与u形状相同，所有元素均为1的张量，这个参数用于指定向量-雅可比积的像两部分\n",
    "            retain_graph=True, #表示计算完梯度之后保留计算图若需要多次计算梯度，则需要设置改参数为True\n",
    "            create_graph=True #创建梯度的计算图，使我们能够计算高阶导数\n",
    "        )[0] #这个函数的返回值是一个元组，其中包含了每个输入张量的梯度。这里只关心第一个输入张量u的梯度，所以我们使用[0]来获取这个梯度。？？？？又说只有一个梯度\n",
    "        v_t = torch.autograd.grad(\n",
    "            v, t, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v_xx = torch.autograd.grad(\n",
    "            v_x, x, \n",
    "            grad_outputs=torch.ones_like(v_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        f_u=u_t+0.5*v_xx+(u**2+v**2)*v    #计算f_u,定义见论文\n",
    "        f_v=v_t-0.5*u_xx-(u**2+v**2)*u   #计算f_v,定义见论文\n",
    "        return f_u, f_v  #返回计算得到的f_u和f_v\n",
    "\n",
    "\n",
    "    def loss_func(self):\n",
    "        self.optimizer_LBFGS.zero_grad() #清除之前计算的梯度（在PyTorch中，梯度会累积，所以在每次新的优化迭代之前，我们需要清除之前的梯度）\n",
    "\n",
    "        u0_pred, v0_pred, _ , _ = self.net_uv(self.x0, self.t0) #是调用net_uv函数,将self.x0_tf和self.t0_tf作为参数传入,然后将返回的前两个结果赋值给self.u0_pred和self.v0_pred。后两个_是Python惯用法，表示不关心net_uv函数返回的后两个结果。\n",
    "        u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = self.net_uv(self.x_lb, self.t_lb) #同上，不过这里函数返回的后两个结果会赋值给self.u_x_lb_pred和self.v_x_lb_pred。\n",
    "        u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = self.net_uv(self.x_ub, self.t_ub) #同上\n",
    "        f_u_pred, f_v_pred = self.net_f_uv(self.x_f, self.t_f) #调用net_f_uv函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_u_pred和self.f_v_pred。\n",
    "\n",
    "        loss = torch.mean((self.u0 - u0_pred) ** 2)  + \\\n",
    "                    torch.mean((self.v0 - v0_pred) ** 2) + \\\n",
    "                    torch.mean((u_lb_pred - u_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_lb_pred - v_ub_pred) ** 2) + \\\n",
    "                    torch.mean((u_x_lb_pred - u_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_x_lb_pred - v_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean(f_u_pred ** 2) + \\\n",
    "                    torch.mean(f_v_pred ** 2)\n",
    "        loss.backward() #被调用以计算损失函数关于神经网络参数的梯度。这个梯度将被用于优化器来更新神经网络参数\n",
    "        \n",
    "        self.iter += 1 #每调用一次损失函数，迭代次数加1\n",
    "\n",
    "\n",
    "        #record the loss value\n",
    "        self.loss_value.append(loss) #将计算得到的loss值添加到self.loss_value列表中\n",
    "\n",
    "        #record the test error\\\n",
    "        self.dnn.eval()\n",
    "        with torch.no_grad():\n",
    "            u_real_pred, v_real_pred= self.net_uv_error(self.x_star, self.t_star)\n",
    "            h_real_pred = torch.sqrt(u_real_pred**2 + v_real_pred**2)\n",
    "\n",
    "        error_u_test = torch.norm(self.u_star - u_real_pred, 2) / torch.norm(self.u_star, 2)\n",
    "        error_v_test = torch.norm(self.v_star - v_real_pred, 2) / torch.norm(self.v_star, 2)\n",
    "        error_h_test = torch.norm(self.h_star - h_real_pred, 2) / torch.norm(self.h_star, 2)\n",
    "        \n",
    "        self.test_error.append(torch.tensor([error_u_test.item(), error_v_test.item(), error_h_test.item()]))\n",
    "\n",
    "        # 计算 MAE\n",
    "        mae = torch.mean(torch.abs(self.h_star - h_real_pred))\n",
    "        # 计算 MSE\n",
    "        mse = torch.mean((self.h_star - h_real_pred) ** 2)\n",
    "        # 记录 MAE 和 MSE\n",
    "        self.test_error_mae.append(mae)\n",
    "        self.test_error_mse.append(mse)\n",
    "        \n",
    "\n",
    "        return loss #返回loss\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #定义了一个名为train的函数/方法，用于训练神经网络。这个方法接受一个参数nIter，表示训练的迭代次数。\n",
    "    def train(self, nIter, nIterLBFGS):\n",
    "\n",
    "        #先使用Adam优化器优化nIter次\n",
    "        for epoch in range(nIter):\n",
    "            self.dnn.train()#将神经网络设置为训练模式而不是评估模式\n",
    "            u0_pred, v0_pred, _ , _ = self.net_uv(self.x0, self.t0) #是调用net_uv函数,将self.x0_tf和self.t0_tf作为参数传入,然后将返回的前两个结果赋值给self.u0_pred和self.v0_pred。后两个_是Python惯用法，表示不关心net_uv函数返回的后两个结果。\n",
    "            u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = self.net_uv(self.x_lb, self.t_lb) #同上，不过这里函数返回的后两个结果会赋值给self.u_x_lb_pred和self.v_x_lb_pred。\n",
    "            u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = self.net_uv(self.x_ub, self.t_ub) #同上\n",
    "            f_u_pred, f_v_pred = self.net_f_uv(self.x_f, self.t_f) #调用net_f_uv函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_u_pred和self.f_v_pred。\n",
    "\n",
    "            loss = torch.mean((self.u0 - u0_pred) ** 2)  + \\\n",
    "                    torch.mean((self.v0 - v0_pred) ** 2) + \\\n",
    "                    torch.mean((u_lb_pred - u_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_lb_pred - v_ub_pred) ** 2) + \\\n",
    "                    torch.mean((u_x_lb_pred - u_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_x_lb_pred - v_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean(f_u_pred ** 2) + \\\n",
    "                    torch.mean(f_v_pred ** 2)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad() #清除该优化器之前计算的梯度（在PyTorch中，梯度会累积，所以在每次新的优化迭代之前，我们需要清除之前的梯度）\n",
    "            loss.backward() #被调用以计算损失函数关于神经网络参数的梯度。这个梯度将被用于优化器来更新神经网络参数\n",
    "            self.optimizer_Adam.step()  #使用之前的优化器self.optimizer_Adam，调用step方法(执行一步优化算法)，传入损失函数self.loss_func，进行优化\n",
    "            \n",
    "\n",
    "            #record the loss value\n",
    "            self.loss_value.append(loss) #将计算得到的loss值添加到self.loss_value列表中\n",
    "            \n",
    "\n",
    "            #record the test error\n",
    "            self.dnn.eval()\n",
    "            with torch.no_grad():\n",
    "                u_real_pred, v_real_pred = self.net_uv_error(self.x_star, self.t_star)\n",
    "                h_real_pred = torch.sqrt(u_real_pred**2 + v_real_pred**2)\n",
    "\n",
    "            error_u_test = torch.norm(self.u_star - u_real_pred, 2) / torch.norm(self.u_star, 2)\n",
    "            error_v_test = torch.norm(self.v_star - v_real_pred, 2) / torch.norm(self.v_star, 2)\n",
    "            error_h_test = torch.norm(self.h_star - h_real_pred, 2) / torch.norm(self.h_star, 2)\n",
    "            \n",
    "            self.test_error.append(torch.tensor([error_u_test.item(), error_v_test.item(), error_h_test.item()]))\n",
    "\n",
    "            # 计算 MAE和MSE\n",
    "            mae = torch.mean(torch.abs(self.h_star - h_real_pred))\n",
    "            mse = torch.mean((self.h_star - h_real_pred) ** 2)\n",
    "            # 记录 MAE 和 MSE\n",
    "            self.test_error_mae.append(mae)\n",
    "            self.test_error_mse.append(mse)\n",
    "\n",
    "\n",
    "\n",
    "        #Backward the optimize，使用LBFGS优化器进一步，注意这里虽然迭代了500次，但其实使用LBFGS优化器优化的次数不止500次\n",
    "        for i in range(nIterLBFGS):\n",
    "            self.dnn.train() #将神经网络设置为训练模式而不是评估模式\n",
    "            self.optimizer_LBFGS.step(self.loss_func)  #使用之前的优化器self.optimizer，调用step方法(执行一步优化算法)，传入计算损失函数的方法self.loss_func，进行优化   \n",
    "\n",
    "                                    \n",
    "    #定义了一个名为predict的函数/方法，用于预测神经网络的输出。这个方法接受一个参数X_star，表示输入数据。最后返回预测的两个输出和两个输出的梯度。\n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device) #从输入中得到x和t（第一列和第二列），是张量，需要计算梯度，转换为浮点数类型，并将张量移动到指定设备上\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval() #将神经网络切换为评估模式\n",
    "        u, v, _, _ = self.net_uv(x, t) #调用之前定义的函数得到神经网络的输出u,以及f\n",
    "        f_u, f_v = self.net_f_uv(x, t) \n",
    "\n",
    "        u = u.detach().cpu().numpy() #将张量u和v先从计算图中分离出来，然后转换为numpy数组，最后将这个数组移动到cpu上\n",
    "        v = v.detach().cpu().numpy()\n",
    "        f_u = f_u.detach().cpu().numpy()\n",
    "        f_v = f_v.detach().cpu().numpy()\n",
    "        return u, v, f_u, f_v \n",
    "    \n",
    "    #定义函数获得隐藏层的输出\n",
    "    def hidden_predict(self, x,t):\n",
    "        x = torch.tensor(x, requires_grad=True).float().to(device) #从输入中得到x和t（第一列和第二列），是张量，需要计算梯度，转换为浮点数类型，并将张量移动到指定设备上\n",
    "        t = torch.tensor(t, requires_grad=True).float().to(device)\n",
    "        self.dnn.eval() #将神经网络切换为评估模式\n",
    "        hidden_output = self.dnn.hidden_output(torch.cat([x, t], dim=1)) #调用上一个神经网络类中的hidden_output方法，得到最后一个隐藏层的输出\n",
    "        hidden_output_x = hidden_output[:, 0] #将输出的第一列赋值给hidden_output_x\n",
    "        hidden_output_t = hidden_output[:, 1] #将输出的第二列赋值给hidden_output_t\n",
    "        hidden_output_x = hidden_output_x.detach().cpu().numpy() #将张量hidden_output_x和hidden_output_t先从计算图中分离出来，然后转换为numpy数组，最后将这个数组移动到cpu上\n",
    "        hidden_output_t = hidden_output_t.detach().cpu().numpy() #将张量hidden_output_x和hidden_output_t先从计算图中分离出来，然后转换为numpy数组，最后将这个数组移动到cpu上\n",
    "        return hidden_output_x, hidden_output_t #返回隐藏层的输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.542737Z",
     "iopub.status.busy": "2025-01-01T03:28:45.542661Z",
     "iopub.status.idle": "2025-01-01T03:28:45.546616Z",
     "shell.execute_reply": "2025-01-01T03:28:45.546431Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义采样函数，目的是采用sampler方法，生成n_samples个在指定空间内的准随机样本，这里space为二维的，因此每个样本都是一个二维点，即n_samples*2的数据点\n",
    "\n",
    "#共有6种采样器，分别是LHS、Halton、Hammersley、Sobol、Grid和Random，均为均匀采样方法\n",
    "\n",
    "def caiyang(n_samples, sampler): #接受两个参数，n_samples是样本数量，sampler是采样器名称，是一个字符串\n",
    "    space = [(-1.0, 1.0), (0.0, 1.0)] #指定样本生成的空间，一个二维空间，第一个维度是-1到1，第二个维度是0到1\n",
    "\n",
    "    #根据sampler的不同，选择不同的采样器，返回的sampler是一个采样器对象\n",
    "    if sampler == \"LHS\": #如果采样器是LHS（拉丁超采样，每个维度都被均匀划分为等量区间，每个样本都是从每个维度的一个区间中随机选取的）\n",
    "        sampler = skopt.sampler.Lhs(lhs_type=\"centered\", criterion=\"maximin\", iterations=1000) #第一个参数表示如何从每个区间选取样本，这里表示从每个区间的中心选取样本；第二个参数表示如何划分区间，这里表示尽可能使样本之间的最小距离最大；第三个表示通过优化过程得到样本量的迭代次数（即会尝试1000种不同的样本配置，并最终选择质量最好的那个）\n",
    "    elif sampler == \"Halton\": #Halton序列是一种低差异序列，用于在高维空间中生成点\n",
    "        sampler = skopt.sampler.Halton(min_skip=-1, max_skip=-1)  #两个参数用于控制序列的起始点，Halton序列可以通过跳过序列的前几个点来改变序列的七十点。两个参数分别制定了跳过点的最小和最大数量，这里-1表示不跳过任何点\n",
    "    elif sampler == \"Hammersley\": #Hammersley序列是一种低差异序列，用于在高维空间中生成点\n",
    "        sampler = skopt.sampler.Hammersly(min_skip=-1, max_skip=-1) #两个参数用于控制序列的起始点，Hammersley序列可以通过跳过序列的前几个点来改变序列的七十点。两个参数分别制定了跳过点的最小和最大数量，这里-1表示不跳过任何点\n",
    "    elif sampler == \"Sobol\":\n",
    "        # Remove the first point [0, 0, ...] and the second point [0.5, 0.5, ...], which are too special and may cause some error.\n",
    "        # Sobol采样器的实现有一个问题，即生成的前两个样本点通常不是随机的而是固定的，Sobol序列的前两个点（[0, 0, ...]和[0.5, 0.5, ...]）在许多情况下都被认为是“特殊”的点，可能会对某些计算产生不利影响。因此设置跳过前两个点，而且skopt库在0.9版本号取消了max/min_skip参数，所以需要根据skopt的版本号来选择不同的参数\n",
    "        if LooseVersion(skopt.__version__) < LooseVersion(\"0.9\"): #先检查skopt的版本是否大于0.9,若小于\n",
    "            sampler = skopt.sampler.Sobol(min_skip=2, max_skip=2, randomize=False) #则使用Sobol采样器，min_skip和max_skip表示跳过的点的数量，这里表示跳过前两个点，randomize表示是否随机化\n",
    "        else: #若skopt的版本大于0.9\n",
    "            sampler = skopt.sampler.Sobol(skip=0, randomize=False) #则使用Sobol采样器，skip表示跳过的点的数量，这里表示不跳过任何点，randomize表示是否随机化 \n",
    "            return np.array(sampler.generate(space, n_samples + 2)[2:]) #生成n_samples+2个样本，然后返回除了前两个样本之外的所有样本，也就是返回n_samples个样本，每个样本都是一个二维点，且范围在指定的空间space里面\n",
    "    elif sampler == \"Grid\":\n",
    "        x_min, x_max = space[1]\n",
    "        t_min, t_max = space[0]\n",
    "        \n",
    "        # 计算每个维度的网格大小\n",
    "        x_grid_size = (x_max - x_min) / (n_samples // int(np.sqrt(n_samples)) - 1) # x维度上（纵轴），每行有10个点\n",
    "        t_grid_size = (t_max - t_min) / int(np.sqrt(n_samples))  # \n",
    "        \n",
    "        # 生成等距均匀网格采样点\n",
    "        samples = []\n",
    "        for i in range(n_samples // int(np.sqrt(n_samples))):\n",
    "            for j in range(int(np.sqrt(n_samples))):\n",
    "                # 计算每个网格单元的中心点\n",
    "                x = x_min + i * x_grid_size\n",
    "                t = t_min + j * t_grid_size\n",
    "                samples.append([t, x])\n",
    "        \n",
    "        return np.array(samples)\n",
    "    \n",
    "    elif sampler == \"Random\":\n",
    "        # 从space中提取出x_min, x_max, t_min, t_max\n",
    "        x_min, x_max = space[1]\n",
    "        t_min, t_max = space[0]\n",
    "\n",
    "        # 生成x和t的随机数\n",
    "        x = np.random.rand(n_samples, 1) * (x_max - x_min) + x_min\n",
    "        t = np.random.rand(n_samples, 1) * (t_max - t_min) + t_min\n",
    "\n",
    "        # 将x和t合并为一个(n_samples, 2)的数组\n",
    "        samples = np.hstack((t, x))\n",
    "        return samples #生成一个形状为(n_samples, 2)的随机数组\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(sampler.generate(space, n_samples)) #生成n_samples个样本，每个样本都是一个二维点，且范围在指定的空间space里面（n_samples*2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.547942Z",
     "iopub.status.busy": "2025-01-01T03:28:45.547832Z",
     "iopub.status.idle": "2025-01-01T03:28:45.549931Z",
     "shell.execute_reply": "2025-01-01T03:28:45.549749Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义设置随机数种子的函数，第一个参数seed表示种子；第二个参数用来设置CUDA的卷积操作是否确定性，默认为False，表示没有确定性\n",
    "def set_seed(seed):\n",
    "    # torch.manual_seed(seed) #设置pytorch的CPU随机数生成器的种子\n",
    "    # torch.cuda.manual_seed_all(seed) #设置putorch的所有GPU随机数生成器的种子\n",
    "    # np.random.seed(seed) #设置numpy的随机数生成器的种子\n",
    "    # random.seed(seed) #设置python的内置随机数生成器的种子\n",
    "    # torch.backends.cudnn.deterministic = deterministic #True会让CUDA的卷积操作变得确定性，即对于相同的输入，每次运行会得到相同的结果，False则相反\n",
    "    \"\"\"\n",
    "    设置PyTorch的随机种子, 用于生成随机数. 通过设置相同的种子, 可以确保每次运行时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置PyTorch在所有可用的CUDA设备上的随机种子. 如果在使用GPU进行计算, 这个设置可以确保在不同的GPU上生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置PyTorch在当前CUDA设备上的随机种子. 它与上一行代码的作用类似, 但只影响当前设备\n",
    "    \"\"\"\n",
    "    torch.cuda.manual_seed(seed)\n",
    " \n",
    "    \"\"\"\n",
    "    设置NumPy的随机种子, 用于生成随机数. 通过设置相同的种子，可以确保在使用NumPy的随机函数时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    设置Python内置的随机函数的种子. Python的random模块提供了许多随机函数, 包括生成随机数、打乱列表等. 通过设置相同的种子, 可以确保使用这些随机函数时生成的随机数序列相同\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    设置Python的哈希种子 (哈希函数被广泛用于数据结构 (如字典和集合) 的实现，以及一些内部操作 (如查找和比较)). 通过设置相同的种子, 可以确保在不同的运行中生成的哈希结果相同\n",
    "    \"\"\"\n",
    "    # os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    \"\"\"\n",
    "    该设置确保每次运行代码时, cuDNN的计算结果是确定性的, 即相同的输入会产生相同的输出, 这是通过禁用一些非确定性的算法来实现的, 例如在卷积操作中使用的算法. 这样做可以保证模型的训练和推理在相同的硬件和软件环境下是可复现的, 即每次运行代码时的结果都相同. 但是, 这可能会导致一些性能上的损失, 因为禁用了一些优化的非确定性算法\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \"\"\"\n",
    "    该设置禁用了cuDNN的自动优化过程. 当它被设置为False时, PyTorch不会在每次运行时重新寻找最优的算法配置, 而是使用固定的算法配置. 这样做可以确保每次运行代码时的性能是一致的, 但可能会导致一些性能上的损失\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.551117Z",
     "iopub.status.busy": "2025-01-01T03:28:45.551011Z",
     "iopub.status.idle": "2025-01-01T03:28:45.554178Z",
     "shell.execute_reply": "2025-01-01T03:28:45.553998Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义根据模型计算给定输入（点集中的点）的混沌度的函数，这个函数接受三个参数，分别是模型、输入数据和迭代次数\n",
    "def calculate_chaos(model, X, num_iter):\n",
    "    \"\"\"\n",
    "    计算模型混沌情况的函数。\n",
    "    参数:\n",
    "    - model: 用于预测的模型对象，必须有一个名为hidden_predict的方法。模型的hidden_predict为倒数第二层的输出，倒数第二层的维度必须和输入维度相同。\n",
    "    - X: 输入数据，形状为(N_f_new, 2)，其中第一列为x0，第二列为t0。\n",
    "    - num_iter: 计算混沌情况的迭代次数。\n",
    "    返回:\n",
    "    - distances: 每个采样点（与该采样点加上微扰比较）在最后一次迭代后的欧氏距离数组，形状为(N_f_new,)。\n",
    "    \"\"\"\n",
    "    #对于所有的采样点\n",
    "    x0 = X[:, 0:1] #取X_f_train_new的第一列，赋值给x0，(N_f_new,1)形状\n",
    "    t0 = X[:, 1:2] #取X_f_train_new的第二列，赋值给t0\n",
    "    # 利用x0和t0计算x{t}和t{t}，存储在xs中\n",
    "    xs = [] #初始化xs\n",
    "    x,t = model.hidden_predict(x0,t0) #调用predict方法，传入X_f_train_new，得到x和t，这里x和t形状均为(N_f_new,)，因此下一步需要reshape\n",
    "    x = x.reshape(-1,1) #将x的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "    t = t.reshape(-1,1) #将t的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "    \n",
    "    # 迭代预测\n",
    "    for i in range(num_iter): #循环num_iter次\n",
    "        x,t = model.hidden_predict(x,t) #每次计算隐藏层输出，得到的x和t形状均为(N_f_new,)，因此下一步需要reshape\n",
    "        x = x.reshape(-1,1) #将x的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "        t = t.reshape(-1,1) #将t的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "        xs.append([x,t]) #将x的数据添加到xs中\n",
    "    #最后得到的xs是一个列表，列表中的每个元素都是一个列表（num_iter个元素），每个列表中有两个元素，分别代表x和t，长度均为N_f_new，对应原始采样点的迭代结果\n",
    "\n",
    "\n",
    "    # 给所有采样点加上一个很小的扰动\n",
    "    x1 = x0 + np.random.normal(0, 0.0001) #加上一个很小的扰动，(N_f_new,1)形状\n",
    "    t1 = t0 + np.random.normal(0, 0.0001)\n",
    "    # 利用x0{1}和t0{1}计算x{t1}和t{t1}，存储在xs1中\n",
    "    xs1 = [] #初始化xs1\n",
    "    x,t = model.hidden_predict(x1,t1) #调用predict方法，传入X_f_train_new，得到x和t，这里x和t形状均为(N_f_new,)，因此下一步需要reshape\n",
    "    x = x.reshape(-1,1) #将x的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "    t = t.reshape(-1,1) #将t的形状变为(N_f_new,1)（这一步是为了之后能重复输入神经网络）\n",
    "\n",
    "    # 迭代预测（扰动后）\n",
    "    for i in range(num_iter): #循环num_iter次\n",
    "        x,t = model.hidden_predict(x,t) #每次计算隐藏层输出，得到的x和t形状均为(N_f_new,)，因此下一步需要reshape\n",
    "        x = x.reshape(-1,1) #将x的形状变为(N_f_new,1)\n",
    "        t = t.reshape(-1,1) #将t的形状变为(N_f_new,1)\n",
    "        xs1.append([x,t]) #将x的数据添加到xs1中\n",
    "    #最后得到的xs1是一个列表，列表中的每个元素都是一个列表（num_iter个元素），每个列表中有两个元素，分别代表x和t，长度均为N_f_new，对应加了扰动后的采样点的迭代结果\n",
    "\n",
    "    # 计算最后一次迭代的隐藏层输出，即最后一次迭代的x和t\n",
    "    last_iter_xs = np.array(xs[-1]) #转换为数组，便于之后计算距离\n",
    "    last_iter_xs1 = np.array(xs1[-1])\n",
    "    #这两个数组的形状均为(2,N_f_new,1)，第一个代表x和t，第二个代表N_f_new个样本点得到的结果，第三个代表1个数\n",
    "\n",
    "    # 计算这两个点的欧氏距离\n",
    "    distances = np.linalg.norm(last_iter_xs - last_iter_xs1, axis=0)\n",
    "    #得到的是一个形状为（N_f_new,1）的数组，每个元素代表了两个点之间的欧氏距离，这里点在xt平面上\n",
    "\n",
    "    distances = distances.flatten()\n",
    "\n",
    "    #对distances进行归一化\n",
    "    # distances = distances / np.linalg.norm(distances)\n",
    "    # 对distances进行归一化前，检查分母是否接近零\n",
    "    # norm = np.linalg.norm(distances)\n",
    "    # if norm < 1e-10:  # 1e-10是一个非常小的数，用于检测norm是否接近于零\n",
    "    #     distances = np.zeros_like(distances)  # 如果分母接近0，将distances设置为全零数组，因为范数为0时，distances中的值想对于彼此几乎没有差异，意味着所有点都几乎处于同一混沌度水平\n",
    "    # else:\n",
    "    #     distances = distances / norm\n",
    "\n",
    "    # 现在可以安全地根据distances对点进行排序，即使在所有值都相同的情况下\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.555509Z",
     "iopub.status.busy": "2025-01-01T03:28:45.555383Z",
     "iopub.status.idle": "2025-01-01T03:28:45.557815Z",
     "shell.execute_reply": "2025-01-01T03:28:45.557640Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定期采样GPU使用情况的函数\n",
    "def sample_gpu_usage(interval, gpu_usage_list, stop_event, gpu_index): #interval是采样间隔时间\n",
    "    while not stop_event.is_set():\n",
    "        gpu_usage = subprocess.check_output(\n",
    "            ['nvidia-smi', '-i', str(gpu_index), '--query-gpu=utilization.gpu,memory.used', '--format=csv,noheader,nounits']\n",
    "        ).decode('utf-8').strip().split('\\n')[0].split(', ')\n",
    "        gpu_usage = [int(x) for x in gpu_usage]\n",
    "        gpu_usage_list.append(gpu_usage)\n",
    "        time.sleep(interval)\n",
    "\n",
    "# 模拟训练函数\n",
    "def train_model(model, Iter, IterLBFGS, sample_interval=1, gpu_index=1): #这里gpu_index代表GPU的索引，sample_interval代表采样间隔时间几s\n",
    "    # 用于存储GPU使用情况的列表\n",
    "    gpu_usage_list = []\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # 启动一个线程定期采样GPU使用情况\n",
    "    sampling_thread = threading.Thread(target=sample_gpu_usage, args=(sample_interval, gpu_usage_list, stop_event, gpu_index))\n",
    "    sampling_thread.start()\n",
    "\n",
    "    start_time = time.time()  # 获取当前时间\n",
    "    # 训练模型\n",
    "    model.train(Iter, IterLBFGS)\n",
    "    end_time = time.time() # 获取当前时间\n",
    "    training_time = end_time - start_time # 计算训练时间\n",
    "\n",
    "    # 停止采样线程\n",
    "    stop_event.set()\n",
    "    sampling_thread.join()\n",
    "\n",
    "    # 计算平均GPU使用情况\n",
    "    avg_gpu_usage = np.mean(gpu_usage_list, axis=0)\n",
    "\n",
    "    return training_time, avg_gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T03:28:45.559053Z",
     "iopub.status.busy": "2025-01-01T03:28:45.558938Z",
     "iopub.status.idle": "2025-01-01T04:42:03.706841Z",
     "shell.execute_reply": "2025-01-01T04:42:03.705156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 429.13 秒\n",
      "平均GPU使用率: 71.57%\n",
      "平均GPU显存使用量: 766.07MiB\n",
      "当前为第1次循环，种子为0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 440.12 秒\n",
      "平均GPU使用率: 71.60%\n",
      "平均GPU显存使用量: 821.02MiB\n",
      "当前为第2次循环，种子为1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 424.50 秒\n",
      "平均GPU使用率: 71.66%\n",
      "平均GPU显存使用量: 823.00MiB\n",
      "当前为第3次循环，种子为12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 420.53 秒\n",
      "平均GPU使用率: 71.43%\n",
      "平均GPU显存使用量: 823.00MiB\n",
      "当前为第4次循环，种子为21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 436.27 秒\n",
      "平均GPU使用率: 71.74%\n",
      "平均GPU显存使用量: 823.00MiB\n",
      "当前为第5次循环，种子为123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 408.19 秒\n",
      "平均GPU使用率: 72.08%\n",
      "平均GPU显存使用量: 823.00MiB\n",
      "当前为第6次循环，种子为321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 442.79 秒\n",
      "平均GPU使用率: 71.65%\n",
      "平均GPU显存使用量: 823.00MiB\n",
      "当前为第7次循环，种子为1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 432.77 秒\n",
      "平均GPU使用率: 72.00%\n",
      "平均GPU显存使用量: 825.00MiB\n",
      "当前为第8次循环，种子为4321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 475.97 秒\n",
      "平均GPU使用率: 71.22%\n",
      "平均GPU显存使用量: 825.08MiB\n",
      "当前为第9次循环，种子为12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 423.06 秒\n",
      "平均GPU使用率: 72.17%\n",
      "平均GPU显存使用量: 829.00MiB\n",
      "当前为第10次循环，种子为54321\n"
     ]
    }
   ],
   "source": [
    "#basic PINN\n",
    "#RAR-G方法，对1000个点，先选择10个点训练500次，然后每500次迭代重采样100个点，选出其中残差最大的10个点添加到训练点中；最后总共有1000个点，共训练10000次\n",
    "seeds = [0, 1, 12, 21, 123, 321, 1234, 4321, 12345, 54321] #生成10个随机种子\n",
    "# seeds = [1]\n",
    "\n",
    "#设置噪声值为0 \n",
    "noise = 0.0        \n",
    "\n",
    "# Doman bounds，定义两个一维数组lb和ub，问题域是一个二维空间，其中 x 的范围是 -5 到 5，t 的范围是 0 到 π/2(竖着的)\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "#定义三个整数，分别表示初始条件点数量、边界条件点数量和配位点的数量（这些点用于训练神经网络）\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 5000\n",
    "#定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "layers = [2, 100, 100, 100, 100, 2, 2]\n",
    "#读取名为NLS.mat的Matlab文件，文件中的数据存储在data变量中。这里的路径也要随着设备的情况修改    \n",
    "data = scipy.io.loadmat('../data/NLS.mat')\n",
    "#从data字典中取出变量tt和x的值，并转换为一维数组（flatten方法），最后tongg[:,None]将一维数组转换为二维数组\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu'] #从data字典中取出变量uu的值，并赋值给Exact\n",
    "Exact_u = np.real(Exact)  #取Exact的实部，赋值给Exact_u\n",
    "Exact_v = np.imag(Exact)  #取Exact的虚部，赋值给Exact_v\n",
    "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2) #计算复数uu的|uu|\n",
    "#生成一个二位网络，X和T是输出的二维数组\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  #X_star是一个二维数组，其中第一列是X的展平，第二列是T的展平\n",
    "u_star = Exact_u.T.flatten()[:,None] #先对Exact_u进行转置，然后使用flatten方法将其转换为一维数组，最后使用[:,None]将其转换为二维数组\n",
    "v_star = Exact_v.T.flatten()[:,None] #同上，比如Exact_v是m*n二维数组，Exact_v.T是n*m二维数组，Exact_v.T.flatten()是一个长度为n*m的一维数组，Exact_v.T.flatten()[:,None]是一个(n*m)*1的三维数组\n",
    "h_star = Exact_h.T.flatten()[:,None]\n",
    "#上面五行代码的意义见Numpy库的索引的介绍\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "#从0~数组x的行数(256)中随机选择N0个数，replace=False表示不允许重复选择，最后将这N0个数赋值给idx_x\n",
    "#从所有的初值边界值训练基础数据中选取N_u=100个点\n",
    "idx_x = np.linspace(0, x.shape[0] - 1, N0, dtype=int) #生成一个等差数列，从0到X的行数，间隔为1，赋值给idx_x\n",
    "# idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "#从x中选择N0个对应的行(idx_x对应的行)，最后将这N0行赋值给x0\n",
    "x0 = x[idx_x,:]\n",
    "#从Exact_u中选择N0个对应的行(idx_x对应的行)的第一列元素，最后将这N0个元素赋值给u0\n",
    "u0 = Exact_u[idx_x,0:1]\n",
    "v0 = Exact_v[idx_x,0:1]\n",
    "#从0~数组t的行数中随机选择N_b个数，replace=False表示不允许重复选择，最后将这N_b个数赋值给idx_t\n",
    "idx_t = np.linspace(0, t.shape[0] - 1, N_b, dtype=int) #生成一个等差数列，从0到t的行数，间隔为1，赋值给idx_t\n",
    "# idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "#从t中选择N_b个对应的行(idx_t对应的行)，最后将这N_b行赋值给tb\n",
    "tb = t[idx_t,:]\n",
    "\n",
    "PINN_training_time = [] #创建一个空列表，用于存储PINN训练时间\n",
    "GPU_usage = [] #创建一个空列表，用于存储GPU使用率\n",
    "GPU_memory = [] #创建一个空列表，用于存储GPU显存占用\n",
    "\n",
    "i = 0 #初始化i为0\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed) #设置随机数种子\n",
    "\n",
    "\n",
    "\n",
    "    #2.生成配位点并进行训练\n",
    "\n",
    "    nIter = 50000 #设置迭代次数为10000\n",
    "    nIterLBFGS = 1000 #设置LBFGS迭代次数为500\n",
    "    \n",
    "    # 调用quasirandom函数生成配位点\n",
    "    X_f = lb + (ub-lb)*lhs(2, N_f) #lhs函数采用拉丁超采样方法，生成一个近似均匀分布的多维样本点集，返回的是一个形状为（$N_f$，2）的数组，每一行都是一个2维的样本点，所有样本点都在[0,1]范围内，并对该样本集进行缩放，把每个样本从[0,1]区间缩放到[lb,ub]区域内，即得到了指定范围内均匀分布的样本$X_f$。\n",
    "\n",
    "\n",
    "    #开始训练\n",
    "\n",
    "    #创建PINN模型并输入各种参数        \n",
    "    model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub, X_star, u_star, v_star, h_star)\n",
    "\n",
    "    training_time, avg_gpu_usage = train_model(model, Iter = nIter, IterLBFGS = nIterLBFGS, sample_interval=1)\n",
    "    print(f\"训练时间: {training_time:.2f} 秒\")\n",
    "    print(f\"平均GPU使用率: {avg_gpu_usage[0]:.2f}%\")\n",
    "    print(f\"平均GPU显存使用量: {avg_gpu_usage[1]:.2f}MiB\")\n",
    "\n",
    "    PINN_training_time.append(training_time)\n",
    "    GPU_usage.append(avg_gpu_usage[0])\n",
    "    GPU_memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    i+=1 #i加1\n",
    "    print(f'当前为第{i}次循环，种子为{seed}')\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T04:42:03.712772Z",
     "iopub.status.busy": "2025-01-01T04:42:03.712205Z",
     "iopub.status.idle": "2025-01-01T04:42:03.721090Z",
     "shell.execute_reply": "2025-01-01T04:42:03.719847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN训练时间为['429.1301488876343%', '440.11865234375%', '424.49504041671753%', '420.53189992904663%', '436.26982021331787%', '408.1919469833374%', '442.78579592704773%', '432.7674825191498%', '475.973445892334%', '423.0583190917969%']s\n",
      "平均PINN训练时间为433.33s\n",
      "GPU使用率为['71.57281553398059%', '71.60189573459715%', '71.65601965601965%', '71.42679900744417%', '71.73747016706444%', '72.08418367346938%', '71.6470588235294%', '72.00240963855421%', '71.22319474835886%', '72.16748768472907%']\n",
      "平均GPU使用率为71.71%\n",
      "GPU显存使用为['766.0679611650486MiB', '821.0189573459716MiB', '823.0MiB', '823.0MiB', '823.0MiB', '823.0MiB', '823.0MiB', '825.0MiB', '825.0831509846827MiB', '829.0MiB']\n",
      "平均GPU显存使用为818.12MiB\n"
     ]
    }
   ],
   "source": [
    "#普通PINN\n",
    "# 打印PINN训练时间\n",
    "print(f'PINN训练时间为{[f\"{traingtime}%\" for traingtime in PINN_training_time]}s')\n",
    "# 打印平均PINN训练时间\n",
    "print(f'平均PINN训练时间为{np.mean(PINN_training_time):.2f}s')\n",
    "\n",
    "# 打印GPU使用率\n",
    "print(f'GPU使用率为{[f\"{usage}%\" for usage in GPU_usage]}')\n",
    "# 打印平均GPU使用率\n",
    "print(f'平均GPU使用率为{np.mean(GPU_usage):.2f}%')\n",
    "\n",
    "# 打印GPU显存使用率\n",
    "print(f'GPU显存使用为{[f\"{memory}MiB\" for memory in GPU_memory]}')\n",
    "# 打印平均GPU显存使用率\n",
    "print(f'平均GPU显存使用为{np.mean(GPU_memory):.2f}MiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "效率为604.28\n"
     ]
    }
   ],
   "source": [
    "#计算效率=总训练时间/平均GPU使用率\n",
    "efficiency = 433.33 / 0.7171\n",
    "print(f'效率为{efficiency:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总训练次数为 2.55e+08\n"
     ]
    }
   ],
   "source": [
    "#总训练次数\n",
    "N_f = 5000\n",
    "nIter = 50000\n",
    "nIterLBFGS = 1000\n",
    "total_training = N_f * (nIter + nIterLBFGS)\n",
    "\n",
    "# 用科学计数法表示\n",
    "total_training_scientific = f\"{total_training:.2e}\"\n",
    "print(f'总训练次数为 {total_training_scientific}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T04:42:04.005368Z",
     "iopub.status.busy": "2025-01-01T04:42:04.005260Z",
     "iopub.status.idle": "2025-01-01T06:16:23.188582Z",
     "shell.execute_reply": "2025-01-01T06:16:23.186510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 489.16 秒\n",
      "平均GPU使用率: 57.30%\n",
      "平均GPU显存使用: 852.94MiB\n",
      "当前为第1次循环，种子为0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 529.49 秒\n",
      "平均GPU使用率: 45.56%\n",
      "平均GPU显存使用: 874.97MiB\n",
      "当前为第2次循环，种子为1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 502.90 秒\n",
      "平均GPU使用率: 52.33%\n",
      "平均GPU显存使用: 877.99MiB\n",
      "当前为第3次循环，种子为12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 497.57 秒\n",
      "平均GPU使用率: 45.12%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第4次循环，种子为21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 481.37 秒\n",
      "平均GPU使用率: 55.93%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第5次循环，种子为123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 493.26 秒\n",
      "平均GPU使用率: 57.00%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第6次循环，种子为321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 536.37 秒\n",
      "平均GPU使用率: 45.68%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第7次循环，种子为1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 479.21 秒\n",
      "平均GPU使用率: 53.89%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第8次循环，种子为4321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 500.20 秒\n",
      "平均GPU使用率: 57.20%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第9次循环，种子为12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 483.63 秒\n",
      "平均GPU使用率: 59.20%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第10次循环，种子为54321\n"
     ]
    }
   ],
   "source": [
    "#RAR-G方法，对1000个点，先选择10个点训练500次，然后每500次迭代重采样100个点，选出其中残差最大的10个点添加到训练点中；最后总共有1000个点，共训练10000次\n",
    "seeds = [0, 1, 12, 21, 123, 321, 1234, 4321, 12345, 54321] #生成10个随机种子\n",
    "# seeds = [0]\n",
    "\n",
    "#设置噪声值为0 \n",
    "noise = 0.0        \n",
    "\n",
    "# Doman bounds，定义两个一维数组lb和ub，问题域是一个二维空间，其中 x 的范围是 -5 到 5，t 的范围是 0 到 π/2(竖着的)\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "#定义三个整数，分别表示初始条件点数量、边界条件点数量和配位点的数量（这些点用于训练神经网络）\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 5000\n",
    "#定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "layers = [2, 100, 100, 100, 100, 2, 2]\n",
    "#读取名为NLS.mat的Matlab文件，文件中的数据存储在data变量中。这里的路径也要随着设备的情况修改    \n",
    "data = scipy.io.loadmat('../data/NLS.mat')\n",
    "#从data字典中取出变量tt和x的值，并转换为一维数组（flatten方法），最后tongg[:,None]将一维数组转换为二维数组\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu'] #从data字典中取出变量uu的值，并赋值给Exact\n",
    "Exact_u = np.real(Exact)  #取Exact的实部，赋值给Exact_u\n",
    "Exact_v = np.imag(Exact)  #取Exact的虚部，赋值给Exact_v\n",
    "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2) #计算复数uu的|uu|\n",
    "#生成一个二位网络，X和T是输出的二维数组\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  #X_star是一个二维数组，其中第一列是X的展平，第二列是T的展平\n",
    "u_star = Exact_u.T.flatten()[:,None] #先对Exact_u进行转置，然后使用flatten方法将其转换为一维数组，最后使用[:,None]将其转换为二维数组\n",
    "v_star = Exact_v.T.flatten()[:,None] #同上，比如Exact_v是m*n二维数组，Exact_v.T是n*m二维数组，Exact_v.T.flatten()是一个长度为n*m的一维数组，Exact_v.T.flatten()[:,None]是一个(n*m)*1的三维数组\n",
    "h_star = Exact_h.T.flatten()[:,None]\n",
    "#上面五行代码的意义见Numpy库的索引的介绍\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "#从0~数组x的行数(256)中随机选择N0个数，replace=False表示不允许重复选择，最后将这N0个数赋值给idx_x\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "#从x中选择N0个对应的行(idx_x对应的行)，最后将这N0行赋值给x0\n",
    "x0 = x[idx_x,:]\n",
    "#从Exact_u中选择N0个对应的行(idx_x对应的行)的第一列元素，最后将这N0个元素赋值给u0\n",
    "u0 = Exact_u[idx_x,0:1]\n",
    "v0 = Exact_v[idx_x,0:1]\n",
    "#从0~数组t的行数中随机选择N_b个数，replace=False表示不允许重复选择，最后将这N_b个数赋值给idx_t\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "#从t中选择N_b个对应的行(idx_t对应的行)，最后将这N_b行赋值给tb\n",
    "tb = t[idx_t,:]\n",
    "nIter = 50000 #设置迭代次数为10000\n",
    "nIterLBFGS = 1000 #设置LBFGS迭代次数为500\n",
    "\n",
    "\n",
    "PINN_training_time = [] #创建一个空列表，用于存储PINN训练时间\n",
    "GPU_usage = [] #创建一个空列表，用于存储GPU使用率\n",
    "GPU_memory = [] #创建一个空列表，用于存储GPU显存占用\n",
    "\n",
    "\n",
    "i = 0 #初始化i为0\n",
    "\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed) #设置随机数种子\n",
    "\n",
    "    timeused = [] #创建一个空列表，用于存储时间\n",
    "    Usage = [] #创建一个空列表，用于存储使用率\n",
    "    Memory = [] #创建一个空列表，用于存储显存占用\n",
    "\n",
    "\n",
    "    #1.先训练500次\n",
    "\n",
    "    #设置chaos的迭代次数\n",
    "    num_iter = 50\n",
    "    #采样配位点，每次采样总训练点数的1%\n",
    "    N_f_1 = N_f//100\n",
    "    #总的N_f个配位点\n",
    "    X_f = lb + (ub-lb)*lhs(2, N_f_1) #lhs函数采用拉丁超采样方法，生成一个近似均匀分布的多维样本点集，返回的是一个形状为（$N_f$，2）的数组，每一行都是一个2维的样本点，所有样本点都在[0,1]范围内，并对该样本集进行缩放，把每个样本从[0,1]区间缩放到[lb,ub]区域内，即得到了指定范围内均匀分布的样本$X_f$。\n",
    "\n",
    "    #创建PINN模型并输入各种参数        \n",
    "    model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub, X_star, u_star, v_star, h_star)\n",
    "\n",
    "\n",
    "    training_time, avg_gpu_usage = train_model(model, nIter//100, 0)\n",
    "\n",
    "    timeused.append(training_time)\n",
    "    Usage.append(avg_gpu_usage[0])\n",
    "    Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "\n",
    "    #2.训练结束后，每500次迭代重采样一次N_f_new个点（即N_f_1的10倍），并从中选出N_f_1个点加入训练点中（即总训练点数的1%）；最后总共有N_f个点，共训练50000次\n",
    "    for iter in range(nIter//100+1, nIter+1, nIter//100): #每500次迭代\n",
    "\n",
    "        N_f_new = N_f_1 * 10 #重新总训练点数的1%个训练点的10倍\n",
    "        # 生成新的X_f_new数据\n",
    "        X_f_new = lb + (ub-lb)*lhs(2, N_f_new)\n",
    "\n",
    "        #计算混沌情况\n",
    "        distances = calculate_chaos(model, X_f_new, num_iter)\n",
    "\n",
    "        # 计算残差\n",
    "        _, _, residual_u, residual_v = model.predict(X_f_new)\n",
    "        residual = np.sqrt(residual_u**2 + residual_v**2)\n",
    "        # 计算残差的绝对值\n",
    "        abs_residual = np.abs(residual)\n",
    "        #将二维数组转换为一维数组\n",
    "        abs_residual = abs_residual.flatten()\n",
    "\n",
    "        #对abs_residual进行归一化\n",
    "        abs_residual = abs_residual / np.linalg.norm(abs_residual)\n",
    "\n",
    "        # 对distances进行归一化\n",
    "        epsilon = 1e-5\n",
    "        norm_distances = np.linalg.norm(distances)\n",
    "        if norm_distances > epsilon:\n",
    "            distances = distances / norm_distances\n",
    "        else:\n",
    "            distances = np.zeros_like(distances)\n",
    "\n",
    "        # 计算信息量\n",
    "        xinxi = distances + abs_residual\n",
    "\n",
    "        #找出绝对值最大的N_f_1个值的索引\n",
    "        topk_indices = np.argpartition(xinxi, -N_f_1)[-N_f_1:] #该函数会对数组进行排序，使得指定的k个最大值出现在数组的最后k给位置上，并获取最后1000个元素\n",
    "\n",
    "        # 使用这些索引来提取对应的数据，即新加入的训练数据\n",
    "        X_f_topk = X_f_new[topk_indices]\n",
    "\n",
    "        #与之前的训练数据合并\n",
    "        X_f = np.vstack((X_f, X_f_topk))\n",
    "\n",
    "\n",
    "        # 更新模型中的X_f_train数据\n",
    "        model.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
    "        model.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        # 在更新数据后的模型上进行训练500次\n",
    "        # model.train(nIter//100,0)\n",
    "        training_time, avg_gpu_usage = train_model(model, nIter//100, 0)\n",
    "\n",
    "        timeused.append(training_time)\n",
    "        Usage.append(avg_gpu_usage[0])\n",
    "        Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "\n",
    "\n",
    "    # model.train(0,nIterLBFGS) #使用LBFGS训练500次\n",
    "    training_time, avg_gpu_usage = train_model(model, 0, nIterLBFGS)\n",
    "\n",
    "    timeused.append(training_time)\n",
    "    Usage.append(avg_gpu_usage[0])\n",
    "    Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    training_time = sum(timeused)\n",
    "    avg_gpu_usage = [np.mean(Usage), np.mean(Memory)]\n",
    "    print(f\"训练时间: {training_time:.2f} 秒\")\n",
    "    print(f\"平均GPU使用率: {avg_gpu_usage[0]:.2f}%\")\n",
    "    print(f\"平均GPU显存使用: {avg_gpu_usage[1]:.2f}MiB\")\n",
    "\n",
    "    PINN_training_time.append(training_time)\n",
    "    GPU_usage.append(avg_gpu_usage[0])\n",
    "    GPU_memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    \n",
    "\n",
    "    i+=1 #i加1\n",
    "    print(f'当前为第{i}次循环，种子为{seed}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T06:16:23.193877Z",
     "iopub.status.busy": "2025-01-01T06:16:23.193466Z",
     "iopub.status.idle": "2025-01-01T06:16:23.203524Z",
     "shell.execute_reply": "2025-01-01T06:16:23.201895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN训练时间为['489.15600991249084%', '529.4939539432526%', '502.8973126411438%', '497.5713629722595%', '481.371844291687%', '493.2644889354706%', '536.3663499355316%', '479.21054100990295%', '500.19933319091797%', '483.6336691379547%']s\n",
      "平均PINN训练时间为499.32s\n",
      "GPU使用率为['57.298844884488446%', '45.55736287914506%', '52.33469567887021%', '45.120232779580476%', '55.93185429654076%', '57.00499978569287%', '45.67651977632997%', '53.892465998192186%', '57.20008947323303%', '59.202863274933975%']\n",
      "平均GPU使用率为52.92%\n",
      "GPU显存使用为['852.9356435643564MiB', '874.9653465346535MiB', '877.990099009901MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB']\n",
      "平均GPU显存使用为875.89MiB\n"
     ]
    }
   ],
   "source": [
    "#fuhe PINN\n",
    "# 打印PINN训练时间\n",
    "print(f'PINN训练时间为{[f\"{traingtime}%\" for traingtime in PINN_training_time]}s')\n",
    "# 打印平均PINN训练时间\n",
    "print(f'平均PINN训练时间为{np.mean(PINN_training_time):.2f}s')\n",
    "\n",
    "# 打印GPU使用率\n",
    "print(f'GPU使用率为{[f\"{usage}%\" for usage in GPU_usage]}')\n",
    "# 打印平均GPU使用率\n",
    "print(f'平均GPU使用率为{np.mean(GPU_usage):.2f}%')\n",
    "\n",
    "# 打印GPU显存使用率\n",
    "print(f'GPU显存使用为{[f\"{memory}MiB\" for memory in GPU_memory]}')\n",
    "# 打印平均GPU显存使用率\n",
    "print(f'平均GPU显存使用为{np.mean(GPU_memory):.2f}MiB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "效率为943.54\n"
     ]
    }
   ],
   "source": [
    "#计算效率=总训练时间/平均GPU使用率\n",
    "efficiency = 499.32 / 0.5292\n",
    "print(f'效率为{efficiency:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总训练次数为 1.31e+08\n"
     ]
    }
   ],
   "source": [
    "#总训练次数\n",
    "def calculate_total_training(N_f, nIter):\n",
    "    total_training = 0\n",
    "    current_points = N_f // 100\n",
    "    iterations_per_stage = nIter // 100\n",
    "\n",
    "    for stage in range(1, 101):\n",
    "        total_training += current_points * iterations_per_stage\n",
    "        current_points += N_f // 100\n",
    "\n",
    "    return total_training\n",
    "\n",
    "\n",
    "total_training = calculate_total_training(N_f, nIter) + N_f * nIterLBFGS\n",
    "\n",
    "# 用科学计数法表示\n",
    "total_training_scientific = f\"{total_training:.2e}\"\n",
    "print(f'总训练次数为 {total_training_scientific}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T06:16:23.432423Z",
     "iopub.status.busy": "2025-01-01T06:16:23.432318Z",
     "iopub.status.idle": "2025-01-01T07:39:28.763050Z",
     "shell.execute_reply": "2025-01-01T07:39:28.761370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 409.35 秒\n",
      "平均GPU使用率: 58.60%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第1次循环，种子为0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 421.56 秒\n",
      "平均GPU使用率: 59.03%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第2次循环，种子为1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 471.46 秒\n",
      "平均GPU使用率: 45.92%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第3次循环，种子为12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 428.21 秒\n",
      "平均GPU使用率: 50.65%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第4次循环，种子为21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 410.04 秒\n",
      "平均GPU使用率: 58.23%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第5次循环，种子为123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 419.34 秒\n",
      "平均GPU使用率: 56.63%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第6次循环，种子为321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 429.52 秒\n",
      "平均GPU使用率: 53.43%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第7次循环，种子为1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 460.04 秒\n",
      "平均GPU使用率: 45.91%\n",
      "平均GPU显存使用: 879.00MiB\n",
      "当前为第8次循环，种子为4321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 444.15 秒\n",
      "平均GPU使用率: 49.59%\n",
      "平均GPU显存使用: 879.99MiB\n",
      "当前为第9次循环，种子为12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间: 453.60 秒\n",
      "平均GPU使用率: 56.66%\n",
      "平均GPU显存使用: 881.00MiB\n",
      "当前为第10次循环，种子为54321\n"
     ]
    }
   ],
   "source": [
    "#RAR-G方法，对1000个点，先选择10个点训练500次，然后每500次迭代重采样100个点，选出其中残差最大的10个点添加到训练点中；最后总共有1000个点，共训练10000次\n",
    "seeds = [0, 1, 12, 21, 123, 321, 1234, 4321, 12345, 54321] #生成10个随机种子\n",
    "\n",
    "#设置噪声值为0 \n",
    "noise = 0.0        \n",
    "\n",
    "# Doman bounds，定义两个一维数组lb和ub，问题域是一个二维空间，其中 x 的范围是 -5 到 5，t 的范围是 0 到 π/2(竖着的)\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "#定义三个整数，分别表示初始条件点数量、边界条件点数量和配位点的数量（这些点用于训练神经网络）\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 5000\n",
    "#定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "layers = [2, 100, 100, 100, 100, 2, 2]\n",
    "#读取名为NLS.mat的Matlab文件，文件中的数据存储在data变量中。这里的路径也要随着设备的情况修改    \n",
    "data = scipy.io.loadmat('../data/NLS.mat')\n",
    "#从data字典中取出变量tt和x的值，并转换为一维数组（flatten方法），最后tongg[:,None]将一维数组转换为二维数组\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu'] #从data字典中取出变量uu的值，并赋值给Exact\n",
    "Exact_u = np.real(Exact)  #取Exact的实部，赋值给Exact_u\n",
    "Exact_v = np.imag(Exact)  #取Exact的虚部，赋值给Exact_v\n",
    "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2) #计算复数uu的|uu|\n",
    "#生成一个二位网络，X和T是输出的二维数组\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  #X_star是一个二维数组，其中第一列是X的展平，第二列是T的展平\n",
    "u_star = Exact_u.T.flatten()[:,None] #先对Exact_u进行转置，然后使用flatten方法将其转换为一维数组，最后使用[:,None]将其转换为二维数组\n",
    "v_star = Exact_v.T.flatten()[:,None] #同上，比如Exact_v是m*n二维数组，Exact_v.T是n*m二维数组，Exact_v.T.flatten()是一个长度为n*m的一维数组，Exact_v.T.flatten()[:,None]是一个(n*m)*1的三维数组\n",
    "h_star = Exact_h.T.flatten()[:,None]\n",
    "#上面五行代码的意义见Numpy库的索引的介绍\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "#从0~数组x的行数(256)中随机选择N0个数，replace=False表示不允许重复选择，最后将这N0个数赋值给idx_x\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "#从x中选择N0个对应的行(idx_x对应的行)，最后将这N0行赋值给x0\n",
    "x0 = x[idx_x,:]\n",
    "#从Exact_u中选择N0个对应的行(idx_x对应的行)的第一列元素，最后将这N0个元素赋值给u0\n",
    "u0 = Exact_u[idx_x,0:1]\n",
    "v0 = Exact_v[idx_x,0:1]\n",
    "#从0~数组t的行数中随机选择N_b个数，replace=False表示不允许重复选择，最后将这N_b个数赋值给idx_t\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "#从t中选择N_b个对应的行(idx_t对应的行)，最后将这N_b行赋值给tb\n",
    "tb = t[idx_t,:]\n",
    "nIter = 50000 #设置迭代次数为10000\n",
    "nIterLBFGS = 1000 #设置LBFGS迭代次数为500\n",
    "\n",
    "\n",
    "PINN_training_time = [] #创建一个空列表，用于存储PINN训练时间\n",
    "GPU_usage = [] #创建一个空列表，用于存储GPU使用率\n",
    "GPU_memory = [] #创建一个空列表，用于存储GPU显存占用\n",
    "\n",
    "\n",
    "i = 0 #初始化i为0\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed) #设置随机数种子\n",
    "\n",
    "    timeused = [] #创建一个空列表，用于存储时间\n",
    "    Usage = [] #创建一个空列表，用于存储使用率\n",
    "    Memory = [] #创建一个空列表，用于存储显存占用\n",
    "\n",
    "    #1.先训练500次\n",
    "\n",
    "    #采样配位点1000个\n",
    "    N_f_1 = N_f//100\n",
    "    #总的N_f个配位点\n",
    "    X_f_all = lb + (ub-lb)*lhs(2, N_f*2) #lhs函数采用拉丁超采样方法，生成一个近似均匀分布的多维样本点集，返回的是一个形状为（$N_f$，2）的数组，每一行都是一个2维的样本点，所有样本点都在[0,1]范围内，并对该样本集进行缩放，把每个样本从[0,1]区间缩放到[lb,ub]区域内，即得到了指定范围内均匀分布的样本$X_f$。\n",
    "\n",
    "    #获取第一批训练配位点\n",
    "    indices = np.arange(X_f_all.shape[0]) #生成一个0到X_f_train行数-1的数组，赋值给indices\n",
    "\n",
    "    id = np.random.choice(indices, N_f_1, replace=False) #从indices中随机选择10个数，replace=False表示不允许重复选择，最后将这10个数赋值给id，代表训练过的数据索引\n",
    "\n",
    "    X_f = X_f_all[id, :] #从X_f_train中选取id对应的的10行，赋值给X_f_train\n",
    "\n",
    "    #创建PINN模型并输入各种参数        \n",
    "    model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub, X_star, u_star, v_star, h_star)\n",
    "\n",
    "    #开始训练模型            \n",
    "    # model.train(nIter//100,0)\n",
    "    training_time, avg_gpu_usage = train_model(model, nIter//100, 0)\n",
    "\n",
    "    timeused.append(training_time)\n",
    "    Usage.append(avg_gpu_usage[0])\n",
    "    Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    #删除已经训练过的数据的索引\n",
    "    indices = np.setdiff1d(indices, id) #从indices中去除id中的元素，最后将结果赋值给indices\n",
    "\n",
    "\n",
    "    #2.训练结束后，每500次迭代重采样一次N_f_new个点；最后总共有N_f=10000个点，共训练50000次\n",
    "    for iter in range(nIter//100+1, nIter+1, nIter//100): #每500次迭代\n",
    "        N_f_new = N_f_1 #重新采样10个点\n",
    "        # 生成新的X_f_new数据\n",
    "        id = np.random.choice(indices, N_f_new, replace=False) #从indices中随机选择N_f_new个数，replace=False表示不允许重复选择，最后将这N_f_new个数赋值给id，代表训练过的数据索引\n",
    "        X_f_new = X_f_all[id, :] #从X_f_train中选取id对应的的N_f_new行，赋值给X_f_train_new\n",
    "\n",
    "        # 将新生成的数据添加到原数据中\n",
    "        X_f = np.vstack((X_f, X_f_new)) #与之前的训练数据合并\n",
    "\n",
    "        # 更新模型中的X_f_train数据\n",
    "        model.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
    "        model.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        # 在更新数据后的模型上进行训练500次\n",
    "        # model.train(nIter//100,0)\n",
    "        training_time, avg_gpu_usage = train_model(model, nIter//100, 0)\n",
    "\n",
    "        timeused.append(training_time)\n",
    "        Usage.append(avg_gpu_usage[0])\n",
    "        Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "        # 删除已经训练过的数据的索引\n",
    "        indices = np.setdiff1d(indices, id)\n",
    "\n",
    "\n",
    "    # model.train(0,nIterLBFGS) #使用LBFGS训练500次\n",
    "    training_time, avg_gpu_usage = train_model(model, 0, nIterLBFGS)\n",
    "\n",
    "    timeused.append(training_time)\n",
    "    Usage.append(avg_gpu_usage[0])\n",
    "    Memory.append(avg_gpu_usage[1])\n",
    "\n",
    "    training_time = sum(timeused)\n",
    "    avg_gpu_usage = [np.mean(Usage), np.mean(Memory)]\n",
    "    print(f\"训练时间: {training_time:.2f} 秒\")\n",
    "    print(f\"平均GPU使用率: {avg_gpu_usage[0]:.2f}%\")\n",
    "    print(f\"平均GPU显存使用: {avg_gpu_usage[1]:.2f}MiB\")\n",
    "\n",
    "    PINN_training_time.append(training_time)\n",
    "    GPU_usage.append(avg_gpu_usage[0])\n",
    "    GPU_memory.append(avg_gpu_usage[1])\n",
    "\n",
    "\n",
    "    i+=1 #i加1\n",
    "    print(f'当前为第{i}次循环，种子为{seed}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T07:39:28.768913Z",
     "iopub.status.busy": "2025-01-01T07:39:28.767845Z",
     "iopub.status.idle": "2025-01-01T07:39:28.776636Z",
     "shell.execute_reply": "2025-01-01T07:39:28.775357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN训练时间为['409.345819234848%', '421.5586404800415%', '471.4633746147156%', '428.21253275871277%', '410.0355887413025%', '419.34427070617676%', '429.5158007144928%', '460.043181180954%', '444.1521589756012%', '453.5966098308563%']s\n",
      "平均PINN训练时间为434.73s\n",
      "GPU使用率为['58.60074691679694%', '59.031765676567666%', '45.916311819151844%', '50.65425203050497%', '58.22616038199565%', '56.627419680743586%', '53.427057159497465%', '45.90874587458745%', '49.585839724208334%', '56.66114778144482%']\n",
      "平均GPU使用率为53.46%\n",
      "GPU显存使用为['879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.0MiB', '879.990099009901MiB', '881.0MiB']\n",
      "平均GPU显存使用为879.30MiB\n"
     ]
    }
   ],
   "source": [
    "#acitve PINN\n",
    "\n",
    "# 打印PINN训练时间\n",
    "print(f'PINN训练时间为{[f\"{traingtime}%\" for traingtime in PINN_training_time]}s')\n",
    "# 打印平均PINN训练时间\n",
    "print(f'平均PINN训练时间为{np.mean(PINN_training_time):.2f}s')\n",
    "\n",
    "# 打印GPU使用率\n",
    "print(f'GPU使用率为{[f\"{usage}%\" for usage in GPU_usage]}')\n",
    "# 打印平均GPU使用率\n",
    "print(f'平均GPU使用率为{np.mean(GPU_usage):.2f}%')\n",
    "\n",
    "# 打印GPU显存使用率\n",
    "print(f'GPU显存使用为{[f\"{memory}MiB\" for memory in GPU_memory]}')\n",
    "# 打印平均GPU显存使用率\n",
    "print(f'平均GPU显存使用为{np.mean(GPU_memory):.2f}MiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "效率为813.19\n"
     ]
    }
   ],
   "source": [
    "#计算效率=总训练时间/平均GPU使用率\n",
    "efficiency = 434.73 / 0.5346\n",
    "print(f'效率为{efficiency:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总训练次数为 1.31e+08\n"
     ]
    }
   ],
   "source": [
    "#总训练次数\n",
    "def calculate_total_training(N_f, nIter):\n",
    "    total_training = 0\n",
    "    current_points = N_f // 100\n",
    "    iterations_per_stage = nIter // 100\n",
    "\n",
    "    for stage in range(1, 101):\n",
    "        total_training += current_points * iterations_per_stage\n",
    "        current_points += N_f // 100\n",
    "\n",
    "    return total_training\n",
    "\n",
    "\n",
    "total_training = calculate_total_training(N_f, nIter) + N_f * nIterLBFGS\n",
    "\n",
    "# 用科学计数法表示\n",
    "total_training_scientific = f\"{total_training:.2e}\"\n",
    "print(f'总训练次数为 {total_training_scientific}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
