{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49019b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif pde_name == \"reaction_diffusion\": \n",
    "        # unpack data-generation parameters\n",
    "        x_range = data_params[\"x_range\"]\n",
    "        t_range = data_params[\"t_range\"]\n",
    "        x_num = data_params[\"x_num\"]\n",
    "        t_num = data_params[\"t_num\"]\n",
    "        res_idx = data_params[\"res_idx\"]\n",
    "        # generate grid\n",
    "        x = np.linspace(x_range[0], x_range[1], x_num-1, endpoint=False).reshape(-1, 1) # exclude upper boundary\n",
    "        t = np.linspace(t_range[0], t_range[1], t_num).reshape(-1, 1)\n",
    "        x_mesh, t_mesh = np.meshgrid(x, t)\n",
    "        # compute initial solution\n",
    "        u0 = np.exp(-(1/2) * np.square((x - np.pi) / (np.pi / 4))).flatten()\n",
    "        u = np.zeros((x_num, t_num))\n",
    "        u[:-1,0] = u0\n",
    "\n",
    "        IKX_pos = 1j * np.arange(0, (x_num-1) / 2 + 1, 1)\n",
    "        IKX_neg = 1j * np.arange(-(x_num-1) / 2 + 1, 0, 1)\n",
    "        IKX = np.concatenate((IKX_pos, IKX_neg))\n",
    "        IKX2 = IKX * IKX\n",
    "        # perform time-marching\n",
    "        t_step_size = (t_range[1] - t_range[0]) / (t_num - 1)\n",
    "        u_t = u0.copy()\n",
    "        for i in range(t_num - 1): \n",
    "            # reaction component\n",
    "            factor = u_t * np.exp(pde_coefs['rho'] * t_step_size)\n",
    "            u_t = factor / (factor + (1 - u_t))\n",
    "            # diffusion component\n",
    "            factor = np.exp(pde_coefs['nu'] * IKX2 * t_step_size)\n",
    "            u_hat = np.fft.fft(u_t) * factor\n",
    "            u_t = np.real(np.fft.ifft(u_hat))\n",
    "            u[:-1,i+1] = u_t\n",
    "\n",
    "        # add back solution on the upper boundary using the periodic boundary condition\n",
    "        u[-1,:] = u[0,:]\n",
    "        # split the solution\n",
    "        sol_left = u[:,0].reshape(-1,1)\n",
    "        sol_upper = u[-1,:].reshape(-1,1)\n",
    "        sol_lower = u[0,:].reshape(-1,1)\n",
    "        sol_res = u[1:-1, 1:].T.reshape(-1,1)[res_idx]\n",
    "\n",
    "        sol = np.vstack([sol_res, sol_left, sol_upper, sol_lower])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b168030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5087a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        \"\"\"\n",
    "        :param num_features: 特征或通道的数量\n",
    "        :param eps: 添加的一个小的数值用于数值稳定性\n",
    "        :param affine: 如果为True，RevIN将具有可学习的仿射参数(affine parameters)\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:,-1,:].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c02a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__all__ = ['Transpose', 'get_activation_fn', 'moving_avg', 'series_decomp', 'PositionalEncoding', 'SinCosPosEncoding', 'Coord2dPosEncoding', 'Coord1dPosEncoding', 'positional_encoding']\n",
    "__all__ = ['moving_avg', 'series_decomp',  'Flatten_Head']\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "# decomposition\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "# forecast task head\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
    "        super(Flatten_Head, self).__init__()\n",
    "\n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "\n",
    "        if self.individual:\n",
    "            self.linears = nn.ModuleList()\n",
    "            self.dropouts = nn.ModuleList()\n",
    "            self.flattens = nn.ModuleList()\n",
    "            for i in range(self.n_vars):\n",
    "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
    "                self.linears.append(nn.Linear(nf, target_window))\n",
    "                self.dropouts.append(nn.Dropout(head_dropout))\n",
    "        else:\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.linear = nn.Linear(nf, target_window)\n",
    "            self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "        if self.individual:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars):\n",
    "                z = self.flattens[i](x[:, i, :, :])  # z: [bs x d_model * patch_num]\n",
    "                z = self.linears[i](z)  # z: [bs x target_window]\n",
    "                z = self.dropouts[i](z)\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=1)  # x: [bs x nvars x target_window]\n",
    "        else:\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear(x)\n",
    "            x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470060ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.norm = nn.Layernorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, M, D, N = x.shape\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.reshape(B * M, N, D)\n",
    "        x = self.norm(\n",
    "            x)\n",
    "        x = x.reshape(B, M, N, D)\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        return x\n",
    "\n",
    "def get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n",
    "    return nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "\n",
    "def get_bn(channels):\n",
    "    return nn.BatchNorm1d(channels)\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1,bias=False):\n",
    "    if padding is None:\n",
    "        padding = kernel_size // 2\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv', get_conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                         stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))\n",
    "    result.add_module('bn', get_bn(out_channels))\n",
    "    return result\n",
    "\n",
    "def fuse_bn(conv, bn):\n",
    "\n",
    "    kernel = conv.weight\n",
    "    running_mean = bn.running_mean\n",
    "    running_var = bn.running_var\n",
    "    gamma = bn.weight\n",
    "    beta = bn.bias\n",
    "    eps = bn.eps\n",
    "    std = (running_var + eps).sqrt()\n",
    "    t = (gamma / std).reshape(-1, 1, 1)\n",
    "    return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "class ReparamLargeKernelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride, groups,\n",
    "                 small_kernel,\n",
    "                 small_kernel_merged=False, nvars=7):\n",
    "        super(ReparamLargeKernelConv, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.small_kernel = small_kernel\n",
    "        # We assume the conv does not change the feature map size, so padding = k//2. Otherwise, you may configure padding as you wish, and change the padding of small_conv accordingly.\n",
    "        padding = kernel_size // 2\n",
    "        if small_kernel_merged:\n",
    "            self.lkb_reparam = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                         stride=stride, padding=padding, dilation=1, groups=groups, bias=True)\n",
    "        else:\n",
    "            self.lkb_origin = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        stride=stride, padding=padding, dilation=1, groups=groups,bias=False)\n",
    "            if small_kernel is not None:\n",
    "                assert small_kernel <= kernel_size, 'The kernel size for re-param cannot be larger than the large kernel!'\n",
    "                self.small_conv = conv_bn(in_channels=in_channels, out_channels=out_channels,\n",
    "                                            kernel_size=small_kernel,\n",
    "                                            stride=stride, padding=small_kernel // 2, groups=groups, dilation=1,bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if hasattr(self, 'lkb_reparam'):\n",
    "            out = self.lkb_reparam(inputs)\n",
    "        else:\n",
    "            out = self.lkb_origin(inputs)\n",
    "            if hasattr(self, 'small_conv'):\n",
    "                out += self.small_conv(inputs)\n",
    "        return out\n",
    "\n",
    "    def PaddingTwoEdge1d(self,x,pad_length_left,pad_length_right,pad_values=0):\n",
    "\n",
    "        D_out,D_in,ks=x.shape\n",
    "        if pad_values ==0:\n",
    "            pad_left = torch.zeros(D_out,D_in,pad_length_left)\n",
    "            pad_right = torch.zeros(D_out,D_in,pad_length_right)\n",
    "        else:\n",
    "            pad_left = torch.ones(D_out, D_in, pad_length_left) * pad_values\n",
    "            pad_right = torch.ones(D_out, D_in, pad_length_right) * pad_values\n",
    "        x = torch.cat([pad_left,x],dims=-1)\n",
    "        x = torch.cat([x,pad_right],dims=-1)\n",
    "        return x\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        eq_k, eq_b = fuse_bn(self.lkb_origin.conv, self.lkb_origin.bn)\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            small_k, small_b = fuse_bn(self.small_conv.conv, self.small_conv.bn)\n",
    "            eq_b += small_b\n",
    "            eq_k += self.PaddingTwoEdge1d(small_k, (self.kernel_size - self.small_kernel) // 2,\n",
    "                                          (self.kernel_size - self.small_kernel) // 2, 0)\n",
    "        return eq_k, eq_b\n",
    "\n",
    "    def merge_kernel(self):\n",
    "        eq_k, eq_b = self.get_equivalent_kernel_bias()\n",
    "        self.lkb_reparam = nn.Conv1d(in_channels=self.lkb_origin.conv.in_channels,\n",
    "                                     out_channels=self.lkb_origin.conv.out_channels,\n",
    "                                     kernel_size=self.lkb_origin.conv.kernel_size, stride=self.lkb_origin.conv.stride,\n",
    "                                     padding=self.lkb_origin.conv.padding, dilation=self.lkb_origin.conv.dilation,\n",
    "                                     groups=self.lkb_origin.conv.groups, bias=True)\n",
    "        self.lkb_reparam.weight.data = eq_k\n",
    "        self.lkb_reparam.bias.data = eq_b\n",
    "        self.__delattr__('lkb_origin')\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            self.__delattr__('small_conv')\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, large_size, small_size, dmodel, dff, nvars, small_kernel_merged=False, drop=0.1):\n",
    "\n",
    "        super(Block, self).__init__()\n",
    "        self.dw = ReparamLargeKernelConv(in_channels=nvars * dmodel, out_channels=nvars * dmodel,\n",
    "                                         kernel_size=large_size, stride=1, groups=nvars * dmodel,\n",
    "                                         small_kernel=small_size, small_kernel_merged=small_kernel_merged, nvars=nvars)\n",
    "        self.norm = nn.BatchNorm1d(dmodel)\n",
    "\n",
    "        #convffn1\n",
    "        self.ffn1pw1 = nn.Conv1d(in_channels=nvars * dmodel, out_channels=nvars * dff, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=nvars)\n",
    "        self.ffn1act = nn.GELU()\n",
    "        self.ffn1pw2 = nn.Conv1d(in_channels=nvars * dff, out_channels=nvars * dmodel, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=nvars)\n",
    "        self.ffn1drop1 = nn.Dropout(drop)\n",
    "        self.ffn1drop2 = nn.Dropout(drop)\n",
    "\n",
    "        #convffn2\n",
    "        self.ffn2pw1 = nn.Conv1d(in_channels=nvars * dmodel, out_channels=nvars * dff, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=dmodel)\n",
    "        self.ffn2act = nn.GELU()\n",
    "        self.ffn2pw2 = nn.Conv1d(in_channels=nvars * dff, out_channels=nvars * dmodel, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=dmodel)\n",
    "        self.ffn2drop1 = nn.Dropout(drop)\n",
    "        self.ffn2drop2 = nn.Dropout(drop)\n",
    "\n",
    "        self.ffn_ratio = dff//dmodel\n",
    "    def forward(self,x):\n",
    "\n",
    "        input = x\n",
    "        B, M, D, N = x.shape\n",
    "        x = x.reshape(B,M*D,N)\n",
    "        x = self.dw(x)\n",
    "        x = x.reshape(B,M,D,N)\n",
    "        x = x.reshape(B*M,D,N)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, M, D, N)\n",
    "        x = x.reshape(B, M * D, N)\n",
    "\n",
    "        x = self.ffn1drop1(self.ffn1pw1(x))\n",
    "        x = self.ffn1act(x)\n",
    "        x = self.ffn1drop2(self.ffn1pw2(x))\n",
    "        x = x.reshape(B, M, D, N)\n",
    "\n",
    "        x = input + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Stage(nn.Module):\n",
    "    def __init__(self, ffn_ratio, num_blocks, large_size, small_size, dmodel, dw_model, nvars,\n",
    "                 small_kernel_merged=False, drop=0.1):\n",
    "\n",
    "        super(Stage, self).__init__()\n",
    "        d_ffn = dmodel * ffn_ratio\n",
    "        blks = []\n",
    "        for i in range(num_blocks):\n",
    "            blk = Block(large_size=large_size, small_size=small_size, dmodel=dmodel, dff=d_ffn, nvars=nvars, small_kernel_merged=small_kernel_merged, drop=drop)\n",
    "            blks.append(blk)\n",
    "\n",
    "        self.blocks = nn.ModuleList(blks)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModernTCN(nn.Module):\n",
    "    def __init__(self,patch_size,patch_stride, stem_ratio, downsample_ratio, ffn_ratio, num_blocks, large_size, small_size, dims, dw_dims,\n",
    "                 nvars, small_kernel_merged=False, backbone_dropout=0.1, head_dropout=0.1, use_multi_scale=True, revin=True, affine=True,\n",
    "                 subtract_last=False, freq=None, seq_len=512, c_in=7, individual=False, target_window=96):\n",
    "\n",
    "        super(ModernTCN, self).__init__()\n",
    "\n",
    "        # RevIN\n",
    "        self.revin = revin\n",
    "        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
    "\n",
    "        # stem layer & down sampling layers\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv1d(1, dims[0], kernel_size=patch_size, stride=patch_stride),\n",
    "            nn.BatchNorm1d(dims[0])\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        self.num_stage = len(num_blocks)\n",
    "        if self.num_stage > 1:\n",
    "            for i in range(self.num_stage-1):\n",
    "                downsample_layer = nn.Sequential(\n",
    "                    nn.BatchNorm1d(dims[i]),\n",
    "                    nn.Conv1d(dims[i], dims[i + 1], kernel_size=downsample_ratio, stride=downsample_ratio),\n",
    "                )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_stride = patch_stride\n",
    "        self.downsample_ratio = downsample_ratio\n",
    "\n",
    "\n",
    "\n",
    "        # backbone\n",
    "        self.num_stage = len(num_blocks)\n",
    "        self.stages = nn.ModuleList()\n",
    "        for stage_idx in range(self.num_stage):\n",
    "            layer = Stage(ffn_ratio, num_blocks[stage_idx], large_size[stage_idx], small_size[stage_idx], dmodel=dims[stage_idx],\n",
    "                          dw_model=dw_dims[stage_idx], nvars=nvars, small_kernel_merged=small_kernel_merged, drop=backbone_dropout)\n",
    "            self.stages.append(layer)\n",
    "\n",
    "\n",
    "\n",
    "        # head\n",
    "        patch_num = seq_len // patch_stride\n",
    "        self.n_vars = c_in\n",
    "        self.individual = individual\n",
    "        d_model = dims[self.num_stage-1]\n",
    "\n",
    "        if use_multi_scale:\n",
    "            self.head_nf = d_model * patch_num\n",
    "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window,\n",
    "                                     head_dropout=head_dropout)\n",
    "        else:\n",
    "\n",
    "            if patch_num % pow(downsample_ratio,(self.num_stage - 1)) == 0:\n",
    "                self.head_nf = d_model * patch_num // pow(downsample_ratio,(self.num_stage - 1))\n",
    "            else:\n",
    "                self.head_nf = d_model * (patch_num // pow(downsample_ratio, (self.num_stage - 1))+1)\n",
    "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window,\n",
    "                                     head_dropout=head_dropout)\n",
    "\n",
    "\n",
    "    def forward_feature(self, x, te=None):\n",
    "\n",
    "        B,M,L=x.shape\n",
    "        x = x.unsqueeze(-2)\n",
    "\n",
    "        for i in range(self.num_stage):\n",
    "            B, M, D, N = x.shape\n",
    "            x = x.reshape(B * M, D, N)\n",
    "            if i==0:\n",
    "                if self.patch_size != self.patch_stride:\n",
    "                    pad_len = self.patch_size - self.patch_stride\n",
    "                    pad = x[:,:,-1:].repeat(1,1,pad_len)\n",
    "                    x = torch.cat([x,pad],dim=-1)\n",
    "            else:\n",
    "                if N % self.downsample_ratio != 0:\n",
    "                    pad_len = self.downsample_ratio - (N % self.downsample_ratio)\n",
    "                    x = torch.cat([x, x[:, :, -pad_len:]],dim=-1)\n",
    "            x = self.downsample_layers[i](x)\n",
    "            _, D_, N_ = x.shape\n",
    "            x = x.reshape(B, M, D_, N_)\n",
    "            x = self.stages[i](x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, te=None):\n",
    "\n",
    "        # instance norm\n",
    "        if self.revin:\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.revin_layer(x, 'norm')\n",
    "            x = x.permute(0, 2, 1)\n",
    "        x = self.forward_feature(\n",
    "            x,te)\n",
    "        x = self.head(\n",
    "            x)\n",
    "        # de-instance norm\n",
    "        if self.revin:\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.revin_layer(x, 'denorm')\n",
    "            x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "    def structural_reparam(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'merge_kernel'):\n",
    "                m.merge_kernel()\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # hyper param\n",
    "        self.stem_ratio = 6\n",
    "        self.downsample_ratio = 2\n",
    "        self.ffn_ratio = 2\n",
    "        self.num_blocks = [1,1,1,1] #num_blocks in each stage\n",
    "        self.large_size = [31,29,27,13] #big kernel size\n",
    "        self.small_size = [5,5,5,5] #small kernel size for structral reparam\n",
    "        self.dims = [256,256,256,256] #dmodels in each stage\n",
    "        self.dw_dims = [256,256,256,256] #dw dims in dw conv in each stage\n",
    "\n",
    "        self.nvars = 7 #encoder input size\n",
    "        self.small_kernel_merged = False #small_kernel has already merged or not， type=str2bool\n",
    "        self.drop_backbone = 0.05\n",
    "        self.drop_head = 0.0\n",
    "        self.use_multi_scale = True #use_multi_scale fusion, type=str2bool\n",
    "        self.revin = 1 #RevIN; True 1 False 0\n",
    "        self.affine = 0 #RevIN-affine; True 1 False 0\n",
    "        self.subtract_last = 0 #0: subtract mean; 1: subtract last\n",
    "\n",
    "        self.freq = 'h' #freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "        self.seq_len = 96 #input sequence length\n",
    "        self.c_in = self.nvars,\n",
    "        self.individual = 0 #individual head; True 1 False 0\n",
    "        self.target_window = 96 #prediction sequence length\n",
    "\n",
    "        self.kernel_size = 25 #decomposition-kernel\n",
    "        self.patch_size = 16\n",
    "        self.patch_stride = 8\n",
    "\n",
    "\n",
    "        # decomp\n",
    "        self.decomposition = 0 #decomposition; True 1 False 0\n",
    "        if self.decomposition:\n",
    "            self.decomp_module = series_decomp(self.kernel_size)\n",
    "            self.model_res = ModernTCN(patch_size=self.patch_size,patch_stride=self.patch_stride,stem_ratio=self.stem_ratio, downsample_ratio=self.downsample_ratio, ffn_ratio=self.ffn_ratio, num_blocks=self.num_blocks, large_size=self.large_size, small_size=self.small_size, dims=self.dims, dw_dims=self.dw_dims,\n",
    "                 nvars=self.nvars, small_kernel_merged=self.small_kernel_merged, backbone_dropout=self.drop_backbone, head_dropout=self.drop_head, use_multi_scale=self.use_multi_scale, revin=self.revin, affine=self.affine,\n",
    "                 subtract_last=self.subtract_last, freq=self.freq, seq_len=self.seq_len, c_in=self.c_in, individual=self.individual, target_window=self.target_window)\n",
    "            self.model_trend = ModernTCN(patch_size=self.patch_size,patch_stride=self.patch_stride,stem_ratio=self.stem_ratio, downsample_ratio=self.downsample_ratio, ffn_ratio=self.ffn_ratio, num_blocks=self.num_blocks, large_size=self.large_size, small_size=self.small_size, dims=self.dims, dw_dims=self.dw_dims,\n",
    "                 nvars=self.nvars, small_kernel_merged=self.small_kernel_merged, backbone_dropout=self.drop_backbone, head_dropout=self.drop_head, use_multi_scale=self.use_multi_scale, revin=self.revin, affine=self.affine,\n",
    "                 subtract_last=self.subtract_last, freq=self.freq, seq_len=self.seq_len, c_in=self.c_in, individual=self.individual, target_window=self.target_window)\n",
    "        else:\n",
    "            self.model = ModernTCN(patch_size=self.patch_size,patch_stride=self.patch_stride,stem_ratio=self.stem_ratio, downsample_ratio=self.downsample_ratio, ffn_ratio=self.ffn_ratio, num_blocks=self.num_blocks, large_size=self.large_size, small_size=self.small_size, dims=self.dims, dw_dims=self.dw_dims,\n",
    "                 nvars=self.nvars, small_kernel_merged=self.small_kernel_merged, backbone_dropout=self.drop_backbone, head_dropout=self.drop_head, use_multi_scale=self.use_multi_scale, revin=self.revin, affine=self.affine,\n",
    "                 subtract_last=self.subtract_last, freq=self.freq, seq_len=self.seq_len, c_in=self.c_in, individual=self.individual, target_window=self.target_window)\n",
    "\n",
    "    # def forward(self, x,  x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        te= None\n",
    "        if self.decomposition:\n",
    "            res_init, trend_init = self.decomp_module(x)\n",
    "            res_init, trend_init = res_init.permute(0, 2, 1), trend_init.permute(0, 2, 1)\n",
    "            if te is not None:\n",
    "                te = te.permute(0, 2, 1)\n",
    "            res = self.model_res(res_init, te)\n",
    "            trend = self.model_trend(trend_init, te)\n",
    "            x = res + trend\n",
    "            x = x.permute(0, 2, 1)\n",
    "        else:\n",
    "            x = x.permute(0, 2, 1)\n",
    "            if te is not None:\n",
    "                te = te.permute(0, 2, 1)\n",
    "            x = self.model(x, te)\n",
    "            x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3dddf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model): ModernTCN(\n",
      "    (revin_layer): RevIN()\n",
      "    (downsample_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(1, 256, kernel_size=(16,), stride=(8,))\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv1d(256, 256, kernel_size=(2,), stride=(2,))\n",
      "      )\n",
      "    )\n",
      "    (stages): ModuleList(\n",
      "      (0): Stage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            (dw): ReparamLargeKernelConv(\n",
      "              (lkb_origin): Sequential(\n",
      "                (conv): Conv1d(1792, 1792, kernel_size=(31,), stride=(1,), padding=(15,), groups=1792, bias=False)\n",
      "                (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (small_conv): Sequential(\n",
      "                (conv): Conv1d(1792, 1792, kernel_size=(5,), stride=(1,), padding=(2,), groups=1792, bias=False)\n",
      "                (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (ffn1pw1): Conv1d(1792, 3584, kernel_size=(1,), stride=(1,), groups=7)\n",
      "            (ffn1act): GELU(approximate='none')\n",
      "            (ffn1pw2): Conv1d(3584, 1792, kernel_size=(1,), stride=(1,), groups=7)\n",
      "            (ffn1drop1): Dropout(p=0.05, inplace=False)\n",
      "            (ffn1drop2): Dropout(p=0.05, inplace=False)\n",
      "            (ffn2pw1): Conv1d(1792, 3584, kernel_size=(1,), stride=(1,), groups=256)\n",
      "            (ffn2act): GELU(approximate='none')\n",
      "            (ffn2pw2): Conv1d(3584, 1792, kernel_size=(1,), stride=(1,), groups=256)\n",
      "            (ffn2drop1): Dropout(p=0.05, inplace=False)\n",
      "            (ffn2drop2): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Stage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            (dw): ReparamLargeKernelConv(\n",
      "              (lkb_origin): Sequential(\n",
      "                (conv): Conv1d(1792, 1792, kernel_size=(29,), stride=(1,), padding=(14,), groups=1792, bias=False)\n",
      "                (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (small_conv): Sequential(\n",
      "                (conv): Conv1d(1792, 1792, kernel_size=(5,), stride=(1,), padding=(2,), groups=1792, bias=False)\n",
      "                (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (ffn1pw1): Conv1d(1792, 3584, kernel_size=(1,), stride=(1,), groups=7)\n",
      "            (ffn1act): GELU(approximate='none')\n",
      "            (ffn1pw2): Conv1d(3584, 1792, kernel_size=(1,), stride=(1,), groups=7)\n",
      "            (ffn1drop1): Dropout(p=0.05, inplace=False)\n",
      "            (ffn1drop2): Dropout(p=0.05, inplace=False)\n",
      "            (ffn2pw1): Conv1d(1792, 3584, kernel_size=(1,), stride=(1,), groups=256)\n",
      "            (ffn2act): GELU(approximate='none')\n",
      "            (ffn2pw2): Conv1d(3584, 1792, kernel_size=(1,), stride=(1,), groups=256)\n",
      "            (ffn2drop1): Dropout(p=0.05, inplace=False)\n",
      "            (ffn2drop2): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Stage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            (dw): ReparamLargeKernelConv(\n",
      "              (lkb_origin): Sequential(\n",
      "                (conv): Conv1d(1792, 1792, kernel_size=(27,), stride=(1,), padding=(13,), groups=1792, bias=False)\n",
      "                (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (small_conv): Sequential(\n",
      "                (conv): Conv1d(1792, 1792, kernel_size=(5,), stride=(1,), padding=(2,), groups=1792, bias=False)\n",
      "                (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (ffn1pw1): Conv1d(1792, 3584, kernel_size=(1,), stride=(1,), groups=7)\n",
      "            (ffn1act): GELU(approximate='none')\n",
      "            (ffn1pw2): Conv1d(3584, 1792, kernel_size=(1,), stride=(1,), groups=7)\n",
      "            (ffn1drop1): Dropout(p=0.05, inplace=False)\n",
      "            (ffn1drop2): Dropout(p=0.05, inplace=False)\n",
      "            (ffn2pw1): Conv1d(1792, 3584, kernel_size=(1,), stride=(1,), groups=256)\n",
      "            (ffn2act): GELU(approximate='none')\n",
      "            (ffn2pw2): Conv1d(3584, 1792, kernel_size=(1,), stride=(1,), groups=256)\n",
      "            (ffn2drop1): Dropout(p=0.05, inplace=False)\n",
      "            (ffn2drop2): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Stage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            (dw): ReparamLargeKernelConv(\n",
      "              (lkb_origin): Sequential(\n",
      "                (conv): Conv1d(1792, 1792, kernel_size=(13,), stride=(1,), padding=(6,), groups=1792, bias=False)\n",
      "                (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (small_conv): Sequential(\n",
      "                (conv): Conv1d(1792, 1792, kernel_size=(5,), stride=(1,), padding=(2,), groups=1792, bias=False)\n",
      "                (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (ffn1pw1): Conv1d(1792, 3584, kernel_size=(1,), stride=(1,), groups=7)\n",
      "            (ffn1act): GELU(approximate='none')\n",
      "            (ffn1pw2): Conv1d(3584, 1792, kernel_size=(1,), stride=(1,), groups=7)\n",
      "            (ffn1drop1): Dropout(p=0.05, inplace=False)\n",
      "            (ffn1drop2): Dropout(p=0.05, inplace=False)\n",
      "            (ffn2pw1): Conv1d(1792, 3584, kernel_size=(1,), stride=(1,), groups=256)\n",
      "            (ffn2act): GELU(approximate='none')\n",
      "            (ffn2pw2): Conv1d(3584, 1792, kernel_size=(1,), stride=(1,), groups=256)\n",
      "            (ffn2drop1): Dropout(p=0.05, inplace=False)\n",
      "            (ffn2drop2): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (head): Flatten_Head(\n",
      "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
      "      (linear): Linear(in_features=3072, out_features=96, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29066bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8261216"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0 #定义一个变量pp，用于存储参数数量\n",
    "    #遍历模型的所有参数\n",
    "    for p in list(model.parameters()): #model.parameters()返回模型的所有参数，list将其转换为列表，p代表每一个参数\n",
    "        nn=1 #定义一个变量nn，用于存储该参数数量\n",
    "        for s in list(p.size()):  #p.size()是一个元组，包含了参数p在每个维度上的大小，list将其转换为列表，s代表每一个维度的大小\n",
    "            nn = nn*s #将每个维度的大小相乘，得到该参数的数量。当遍历完p的所有维度后，nn即为该参数的数量\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "394e6282",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2560\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m7\u001b[39m)  \u001b[38;5;66;03m# 假设输入的形状为 (batch_size, nvars, seq_len)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lcy\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lcy\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 362\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m te \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         te \u001b[38;5;241m=\u001b[39m te\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 362\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mte\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\lcy\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lcy\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 280\u001b[0m, in \u001b[0;36mModernTCN.forward\u001b[1;34m(self, x, te)\u001b[0m\n\u001b[0;32m    278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevin_layer(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    279\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mte\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\n\u001b[0;32m    283\u001b[0m     x)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# de-instance norm\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 267\u001b[0m, in \u001b[0;36mModernTCN.forward_feature\u001b[1;34m(self, x, te)\u001b[0m\n\u001b[0;32m    265\u001b[0m         pad_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample_ratio \u001b[38;5;241m-\u001b[39m (N \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample_ratio)\n\u001b[0;32m    266\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, x[:, :, \u001b[38;5;241m-\u001b[39mpad_len:]],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 267\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m(x)\n\u001b[0;32m    268\u001b[0m _, D_, N_ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    269\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, M, D_, N_)\n",
      "File \u001b[1;32mc:\\Users\\lcy\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\container.py:295\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32mc:\\Users\\lcy\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\container.py:285\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    283\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    287\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of range"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2560, 8, 7)  # 假设输入的形状为 (batch_size, nvars, seq_len)\n",
    "model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
